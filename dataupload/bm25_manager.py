import os
import jieba
import json
import pickle
import glob
import logging
from rank_bm25 import BM25Okapi

logger = logging.getLogger(__name__)

class BM25Manager:
    def __init__(self, k1=1.5, b=0.75, min_freq=5, max_vocab_size=10000, custom_dict_path=None, user_dict_path=None):
        self.k1 = k1
        self.b = b
        self.min_freq = min_freq
        self.max_vocab_size = max_vocab_size
        self.bm25_model = None
        self.vocabulary = {}
        self.corpus_tokens = []
        self.raw_texts = []  # åŸå§‹æ–‡æœ¬åˆ—è¡¨
        self.is_fitted = False
        if custom_dict_path and os.path.exists(custom_dict_path):
            jieba.load_userdict(custom_dict_path)
        if user_dict_path and os.path.exists(user_dict_path):
            print(f"[DEBUG] åŠ è½½ä¿æŠ¤æ€§userdict: {user_dict_path}")
            with open(user_dict_path, 'r', encoding='utf-8') as f:
                print("[DEBUG] userdictå†…å®¹é¢„è§ˆ:")
                for i, line in enumerate(f):
                    print(line.strip())
                    if i >= 19:
                        print("... (ä»…æ˜¾ç¤ºå‰20è¡Œ)")
                        break
            jieba.load_userdict(user_dict_path)

    def tokenize_chinese(self, text):
        return list(jieba.cut(text))

    def _build_vocabulary(self, corpus_texts):
        from collections import Counter
        all_tokens = []
        for text in corpus_texts:
            all_tokens.extend(self.tokenize_chinese(text))
        counter = Counter(all_tokens)
        # å‰ªæï¼šåªä¿ç•™é«˜é¢‘è¯
        most_common = [w for w, c in counter.items() if c >= self.min_freq]
        most_common = most_common[:self.max_vocab_size]
        self.vocabulary = {w: i for i, w in enumerate(most_common)}

    def fit(self, corpus_texts):
        self.raw_texts = list(corpus_texts)
        self.corpus_tokens = [self.tokenize_chinese(text) for text in self.raw_texts]
        self._build_vocabulary(self.raw_texts)
        self.bm25_model = BM25Okapi(self.corpus_tokens, k1=self.k1, b=self.b)
        self.is_fitted = True

    def incremental_update(self, new_corpus_texts):
        self.raw_texts.extend(new_corpus_texts)
        self.corpus_tokens = [self.tokenize_chinese(text) for text in self.raw_texts]
        self._build_vocabulary(self.raw_texts)
        self.bm25_model = BM25Okapi(self.corpus_tokens, k1=self.k1, b=self.b)
        self.is_fitted = True

    def get_sparse_vector(self, text):
        if not self.is_fitted:
            raise ValueError("BM25Manageræœªè®­ç»ƒï¼Œè¯·å…ˆfitè¯­æ–™åº“")
        tokens = self.tokenize_chinese(text)
        indices, values = [], []
        avgdl = self.bm25_model.avgdl
        k1, b = self.bm25_model.k1, self.bm25_model.b
        dl = len(tokens)
        token_freq = {}
        for token in tokens:
            token_freq[token] = token_freq.get(token, 0) + 1
        for token, freq in token_freq.items():
            if token in self.vocabulary:
                idf = self.bm25_model.idf.get(token, 0)
                tf = freq
                denom = tf + k1 * (1 - b + b * dl / avgdl)
                weight = idf * (tf * (k1 + 1)) / denom if denom > 0 else 0
                if weight > 0:
                    indices.append(self.vocabulary[token])
                    values.append(float(weight))
        return {"indices": indices, "values": values}

    def save_model(self, model_path, vocab_path):
        with open(model_path, 'wb') as f:
            pickle.dump(self.bm25_model, f)
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocabulary, f, ensure_ascii=False)

    def load_model(self, model_path, vocab_path):
        with open(model_path, 'rb') as f:
            self.bm25_model = pickle.load(f)
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.vocabulary = json.load(f)
        self.is_fitted = True

    # ===== é‡æ„ç›®æ ‡ï¼šæ–°å¢æ–¹æ³• =====
    
    def get_latest_dict_file(self, dict_dir: str = "dict") -> str:
        """æŸ¥æ‰¾dictç›®å½•ä¸‹æœ€æ–°çš„bm25_vocab_*.jsonæ–‡ä»¶ï¼ˆæ’é™¤å¸¦freqçš„æ–‡ä»¶ï¼‰"""
        if not os.path.exists(dict_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰bm25_vocab_*.jsonæ–‡ä»¶ï¼Œä½†æ’é™¤å¸¦freqçš„æ–‡ä»¶
        vocab_pattern = os.path.join(dict_dir, "bm25_vocab_*.json")
        vocab_files = glob.glob(vocab_pattern)
        
        # è¿‡æ»¤æ‰åŒ…å«freqçš„æ–‡ä»¶
        vocab_files = [f for f in vocab_files if 'freq' not in os.path.basename(f)]
        
        if not vocab_files:
            return None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        vocab_files.sort(reverse=True)
        return vocab_files[0]
    
    def get_latest_freq_dict_file(self, dict_dir: str = "dict") -> str:
        """æŸ¥æ‰¾dictç›®å½•ä¸‹æœ€æ–°çš„bm25_vocab_freq_*.jsonæ–‡ä»¶"""
        if not os.path.exists(dict_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰bm25_vocab_freq_*.jsonæ–‡ä»¶
        freq_pattern = os.path.join(dict_dir, "bm25_vocab_freq_*.json")
        freq_files = glob.glob(freq_pattern)
        
        if not freq_files:
            return None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        freq_files.sort(reverse=True)
        return freq_files[0]
    
    def load_dictionary(self, dict_file_path: str) -> dict:
        """åŠ è½½è¯å…¸æ–‡ä»¶ - æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ ¼å¼"""
        try:
            with open(dict_file_path, 'r', encoding='utf-8') as f:
                dict_data = json.load(f)
            
            # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ ¼å¼
            if 'freq' in os.path.basename(dict_file_path):
                # å¸¦è¯é¢‘æ ¼å¼ï¼šæå–weightå€¼
                self.vocabulary = {word: data['weight'] for word, data in dict_data.items()}
                logger.info("æ£€æµ‹åˆ°å¸¦è¯é¢‘æ ¼å¼è¯å…¸ï¼Œæå–weightå€¼")
            else:
                # ä¼ ç»Ÿæ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
                if 'vocabulary' in dict_data:
                    self.vocabulary = dict_data['vocabulary']
                else:
                    self.vocabulary = dict_data
                logger.info("æ£€æµ‹åˆ°ä¼ ç»Ÿæ ¼å¼è¯å…¸")
            
            logger.info(f"æˆåŠŸåŠ è½½è¯å…¸ï¼Œè¯æ±‡é‡: {len(self.vocabulary)}")
            return dict_data
        except Exception as e:
            raise Exception(f"åŠ è½½è¯å…¸å¤±è´¥: {e}")
    
    def generate_sparse_vectors(self, texts: list, config=None) -> list:
        """ä¸ºæ–‡æœ¬åˆ—è¡¨ç”Ÿæˆsparseå‘é‡ - æ”¯æŒå‚æ•°é…ç½®"""
        if not self.vocabulary:
            raise Exception("è¯å…¸æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½è¯å…¸")
        
        # å¦‚æœæä¾›äº†é…ç½®ï¼Œåˆ›å»ºæ–°å®ä¾‹ä»¥æ”¯æŒå‚æ•°è°ƒä¼˜ï¼ˆevaltoolçš„æ–¹å¼ï¼‰
        if config:
            logger.info("ğŸ”„ ä½¿ç”¨é…ç½®å‚æ•°åˆ›å»ºæ–°çš„BM25Managerå®ä¾‹")
            # åˆ›å»ºæ–°çš„BM25Managerå®ä¾‹ï¼Œä½¿ç”¨é…ç½®å‚æ•°
            bm25 = BM25Manager(
                min_freq=config.min_freq,
                max_vocab_size=config.max_vocab_size
            )
            bm25.vocabulary = self.vocabulary
            # é‡æ–°è®­ç»ƒä»¥åº”ç”¨æ–°å‚æ•°
            bm25.fit(texts)
            logger.info("âœ… æ–°BM25Managerå®ä¾‹è®­ç»ƒå®Œæˆ")
        else:
            # ä½¿ç”¨å½“å‰å®ä¾‹ï¼ˆupsert.pyçš„æ–¹å¼ï¼‰
            logger.info("ğŸ“ ä½¿ç”¨å½“å‰BM25Managerå®ä¾‹")
            bm25 = self
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œè¿›è¡Œè®­ç»ƒ
            if not bm25.is_fitted:
                logger.info("ğŸ”„ å¼€å§‹è®­ç»ƒBM25æ¨¡å‹...")
                bm25.fit(texts)
                logger.info("âœ… BM25æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # ç”Ÿæˆç¨€ç–å‘é‡
        sparse_vectors = []
        for text in texts:
            try:
                sparse_vector = bm25.get_sparse_vector(text)
                sparse_vectors.append(sparse_vector)
            except Exception as e:
                logger.warning(f"ç”Ÿæˆsparseå‘é‡å¤±è´¥: {e}")
                sparse_vectors.append({'indices': [], 'values': []})
        return sparse_vectors
    
    def analyze_vocabulary_coverage(self, texts: list) -> dict:
        """åˆ†æè¯æ±‡è¡¨è¦†ç›–ç‡ - å‚è€ƒevaltoolçš„å®ç°"""
        if not texts:
            return {
                'total_tokens': 0,
                'covered_tokens': 0,
                'coverage_rate': 0,
                'missing_tokens': []
            }
        
        import jieba
        all_tokens = []
        for text in texts:
            tokens = list(jieba.cut(text))
            all_tokens.extend(tokens)
        
        total_tokens = len(all_tokens)
        unique_tokens = set(all_tokens)
        covered_tokens = [token for token in unique_tokens if token in self.vocabulary]
        missing_tokens = [token for token in unique_tokens if token not in self.vocabulary]
        coverage_rate = len(covered_tokens) / len(unique_tokens) if unique_tokens else 0
        
        return {
            'total_tokens': total_tokens,
            'unique_tokens': len(unique_tokens),
            'covered_tokens': len(covered_tokens),
            'coverage_rate': coverage_rate,
            'missing_tokens': missing_tokens[:100]
        }
    
    def get_vocabulary_stats(self) -> dict:
        """è·å–è¯æ±‡ç»Ÿè®¡ä¿¡æ¯ - ä¸evaltoolä¿æŒä¸€è‡´"""
        if not self.vocabulary:
            return {
                'vocabulary_size': 0,
                'avg_frequency': 0,
                'max_frequency': 0,
                'min_frequency': 0
            }
        
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾vocabularyçš„å€¼æ˜¯é¢‘ç‡ï¼Œå¦‚æœä¸æ˜¯ï¼Œéœ€è¦è°ƒæ•´é€»è¾‘
        # æˆ‘ä¸çŸ¥é“æ‚¨çš„è¯å…¸æ–‡ä»¶ä¸­vocabularyçš„å€¼å…·ä½“æ˜¯ä»€ä¹ˆå«ä¹‰ï¼Œè¯·ç¡®è®¤
        frequencies = list(self.vocabulary.values())
        
        return {
            'vocabulary_size': len(self.vocabulary),
            'avg_frequency': sum(frequencies) / len(frequencies),
            'max_frequency': max(frequencies),
            'min_frequency': min(frequencies)
        }
    
    def generate_dictionary(self, min_freq=None, max_vocab_size=None, input_dir=None) -> str:
        """ç”Ÿæˆè¯å…¸æ–‡ä»¶ - è°ƒç”¨gen_userdict.py"""
        import subprocess
        import sys
        
        # æ„å»ºå‘½ä»¤å‚æ•°
        cmd = [sys.executable, '-m', 'dataupload.gen_userdict']
        
        if min_freq is not None:
            cmd.extend(['--min-freq', str(min_freq)])
        if max_vocab_size is not None:
            cmd.extend(['--max-vocab-size', str(max_vocab_size)])
        if input_dir is not None:
            cmd.extend(['--input-dir', input_dir])
        
        try:
            logger.info(f"æ‰§è¡Œè¯å…¸ç”Ÿæˆå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            logger.info("è¯å…¸ç”ŸæˆæˆåŠŸ")
            
            # è¿”å›ç”Ÿæˆçš„è¯å…¸æ–‡ä»¶è·¯å¾„
            dict_file = self.get_latest_dict_file()
            if dict_file:
                logger.info(f"ç”Ÿæˆçš„è¯å…¸æ–‡ä»¶: {dict_file}")
                return dict_file
            else:
                raise Exception("æ— æ³•æ‰¾åˆ°ç”Ÿæˆçš„è¯å…¸æ–‡ä»¶")
                
        except subprocess.CalledProcessError as e:
            logger.error(f"è¯å…¸ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¾“å‡º: {e.stderr}")
            raise Exception(f"è¯å…¸ç”Ÿæˆå¤±è´¥: {e.stderr}")
        except Exception as e:
            logger.error(f"è¯å…¸ç”Ÿæˆå¼‚å¸¸: {e}")
            raise Exception(f"è¯å…¸ç”Ÿæˆå¼‚å¸¸: {e}") 