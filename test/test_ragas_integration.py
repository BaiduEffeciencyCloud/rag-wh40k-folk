#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGASé›†æˆæµ‹è¯•
éªŒè¯RAGASè¯„ä¼°å™¨èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡å’ŒæŒ‡æ ‡è¯„ä¼°
"""

import os
import sys
import tempfile
import pandas as pd
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaltool.ragas_evaluator import RAGASEvaluator, run_ragas_evaluation
from vector_search import VectorSearch

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    # åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
    queries = [
        "æˆ˜é”¤40Kä¸­ä»€ä¹ˆæ˜¯APå€¼ï¼Ÿ",
        "å¹½å†¥éª‘å£«çš„é˜”æ­¥å·¨å…½æŠ€èƒ½å¦‚ä½•ä½¿ç”¨ï¼Ÿ",
        "åœ°å½¢æ¨¡å‹å¯¹ç§»åŠ¨æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ"
    ]
    
    # åˆ›å»ºæ ‡å‡†ç­”æ¡ˆ
    gold_answers = [
        "APå€¼æ˜¯è£…ç”²ç©¿é€å€¼ï¼Œç”¨äºé™ä½ç›®æ ‡çš„è£…ç”²ä¿æŠ¤",
        "é˜”æ­¥å·¨å…½æŠ€èƒ½å…è®¸å•ä½åœ¨ç§»åŠ¨é˜¶æ®µç§»åŠ¨é¢å¤–çš„è·ç¦»",
        "åœ°å½¢æ¨¡å‹ä¼šå½±å“å•ä½çš„ç§»åŠ¨è·ç¦»å’Œç§»åŠ¨æ–¹å¼"
    ]
    
    return queries, gold_answers

def test_ragas_evaluator_initialization():
    """æµ‹è¯•RAGASè¯„ä¼°å™¨åˆå§‹åŒ–"""
    logger.info("æµ‹è¯•RAGASè¯„ä¼°å™¨åˆå§‹åŒ–...")
    
    try:
        from config import PINECONE_API_KEY, PINECONE_INDEX
        vector_search = VectorSearch(
            pinecone_api_key=PINECONE_API_KEY,
            index_name=PINECONE_INDEX
        )
        
        evaluator = RAGASEvaluator(vector_search)
        
        # éªŒè¯embeddingé…ç½®
        assert hasattr(evaluator, 'ragas_embeddings'), "RAGASè¯„ä¼°å™¨åº”è¯¥æœ‰ragas_embeddingså±æ€§"
        assert hasattr(evaluator, 'embedding_model'), "RAGASè¯„ä¼°å™¨åº”è¯¥æœ‰embedding_modelå±æ€§"
        
        # æµ‹è¯•embeddingåŠŸèƒ½
        test_embedding = evaluator.ragas_embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
        assert len(test_embedding) == 1024, f"embeddingç»´åº¦åº”è¯¥æ˜¯1024ï¼Œå®é™…æ˜¯{len(test_embedding)}"
        
        logger.info("âœ… RAGASè¯„ä¼°å™¨åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
        return evaluator
        
    except Exception as e:
        logger.error(f"RAGASè¯„ä¼°å™¨åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        raise

def test_dataset_preparation(evaluator):
    """æµ‹è¯•æ•°æ®é›†å‡†å¤‡"""
    logger.info("æµ‹è¯•æ•°æ®é›†å‡†å¤‡...")
    
    try:
        queries, gold_answers = create_test_data()
        
        # å‡†å¤‡æ•°æ®é›†
        dataset = evaluator.prepare_dataset(queries, gold_answers, top_k=3)
        
        # éªŒè¯æ•°æ®é›†ç»“æ„
        assert len(dataset) == len(queries), f"æ•°æ®é›†é•¿åº¦åº”è¯¥ä¸æŸ¥è¯¢æ•°é‡ä¸€è‡´"
        assert 'question' in dataset.column_names, "æ•°æ®é›†åº”è¯¥åŒ…å«questionåˆ—"
        assert 'contexts' in dataset.column_names, "æ•°æ®é›†åº”è¯¥åŒ…å«contextsåˆ—"
        assert 'answer' in dataset.column_names, "æ•°æ®é›†åº”è¯¥åŒ…å«answeråˆ—"
        assert 'ground_truth' in dataset.column_names, "æ•°æ®é›†åº”è¯¥åŒ…å«ground_truthåˆ—"
        
        # éªŒè¯æ•°æ®å†…å®¹
        for i in range(len(dataset)):
            assert dataset[i]['question'] == queries[i], f"ç¬¬{i}ä¸ªé—®é¢˜ä¸åŒ¹é…"
            assert dataset[i]['ground_truth'] == gold_answers[i], f"ç¬¬{i}ä¸ªæ ‡å‡†ç­”æ¡ˆä¸åŒ¹é…"
            assert isinstance(dataset[i]['contexts'], list), f"ç¬¬{i}ä¸ªcontextsåº”è¯¥æ˜¯åˆ—è¡¨"
            assert isinstance(dataset[i]['answer'], str), f"ç¬¬{i}ä¸ªansweråº”è¯¥æ˜¯å­—ç¬¦ä¸²"
        
        logger.info("âœ… æ•°æ®é›†å‡†å¤‡æµ‹è¯•é€šè¿‡")
        return dataset
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†å‡†å¤‡æµ‹è¯•å¤±è´¥: {e}")
        raise

def test_metrics_evaluation(evaluator, dataset):
    """æµ‹è¯•æŒ‡æ ‡è¯„ä¼°"""
    logger.info("æµ‹è¯•æŒ‡æ ‡è¯„ä¼°...")
    
    try:
        # æµ‹è¯•æ£€ç´¢æŒ‡æ ‡
        logger.info("æµ‹è¯•æ£€ç´¢æŒ‡æ ‡...")
        retrieval_results = evaluator.evaluate_retrieval_metrics(dataset)
        assert isinstance(retrieval_results, dict), "æ£€ç´¢æŒ‡æ ‡ç»“æœåº”è¯¥æ˜¯å­—å…¸"
        assert 'context_relevance' in retrieval_results, "åº”è¯¥åŒ…å«context_relevanceæŒ‡æ ‡"
        assert 'context_recall' in retrieval_results, "åº”è¯¥åŒ…å«context_recallæŒ‡æ ‡"
        
        # éªŒè¯æŒ‡æ ‡å€¼èŒƒå›´
        for metric, value in retrieval_results.items():
            assert 0 <= value <= 1, f"æŒ‡æ ‡{metric}çš„å€¼åº”è¯¥åœ¨0-1ä¹‹é—´ï¼Œå®é™…æ˜¯{value}"
        
        logger.info(f"æ£€ç´¢æŒ‡æ ‡ç»“æœ: {retrieval_results}")
        
        # æµ‹è¯•ç”ŸæˆæŒ‡æ ‡
        logger.info("æµ‹è¯•ç”ŸæˆæŒ‡æ ‡...")
        generation_results = evaluator.evaluate_generation_metrics(dataset)
        assert isinstance(generation_results, dict), "ç”ŸæˆæŒ‡æ ‡ç»“æœåº”è¯¥æ˜¯å­—å…¸"
        assert 'faithfulness' in generation_results, "åº”è¯¥åŒ…å«faithfulnessæŒ‡æ ‡"
        assert 'answer_relevancy' in generation_results, "åº”è¯¥åŒ…å«answer_relevancyæŒ‡æ ‡"
        assert 'answer_correctness' in generation_results, "åº”è¯¥åŒ…å«answer_correctnessæŒ‡æ ‡"
        
        # éªŒè¯æŒ‡æ ‡å€¼èŒƒå›´
        for metric, value in generation_results.items():
            assert 0 <= value <= 1, f"æŒ‡æ ‡{metric}çš„å€¼åº”è¯¥åœ¨0-1ä¹‹é—´ï¼Œå®é™…æ˜¯{value}"
        
        logger.info(f"ç”ŸæˆæŒ‡æ ‡ç»“æœ: {generation_results}")
        
        # æµ‹è¯•é«˜çº§æŒ‡æ ‡
        logger.info("æµ‹è¯•é«˜çº§æŒ‡æ ‡...")
        advanced_results = evaluator.evaluate_advanced_metrics(dataset)
        assert isinstance(advanced_results, dict), "é«˜çº§æŒ‡æ ‡ç»“æœåº”è¯¥æ˜¯å­—å…¸"
        
        # éªŒè¯æŒ‡æ ‡å€¼èŒƒå›´
        for metric, value in advanced_results.items():
            assert 0 <= value <= 1, f"æŒ‡æ ‡{metric}çš„å€¼åº”è¯¥åœ¨0-1ä¹‹é—´ï¼Œå®é™…æ˜¯{value}"
        
        logger.info(f"é«˜çº§æŒ‡æ ‡ç»“æœ: {advanced_results}")
        
        logger.info("âœ… æŒ‡æ ‡è¯„ä¼°æµ‹è¯•é€šè¿‡")
        return retrieval_results, generation_results, advanced_results
        
    except Exception as e:
        logger.error(f"æŒ‡æ ‡è¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        raise

def test_report_generation(evaluator, dataset, results):
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
    logger.info("æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
    
    try:
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = {}
        all_results.update(results[0])  # retrieval_results
        all_results.update(results[1])  # generation_results
        all_results.update(results[2])  # advanced_results
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # ç”ŸæˆæŠ¥å‘Š
            report_path = evaluator.generate_detailed_report(dataset, all_results, temp_dir)
            
            # éªŒè¯æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨
            assert os.path.exists(report_path), f"æŠ¥å‘Šæ–‡ä»¶åº”è¯¥å­˜åœ¨: {report_path}"
            
            # éªŒè¯æŠ¥å‘Šå†…å®¹
            with open(report_path, 'r', encoding='utf-8') as f:
                report_content = f.read()
                assert "RAGAS Denseå‘é‡æ£€ç´¢è¯„ä¼°æŠ¥å‘Š" in report_content, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ ‡é¢˜"
                assert "è¯„ä¼°æŒ‡æ ‡ç»“æœ" in report_content, "æŠ¥å‘Šåº”è¯¥åŒ…å«æŒ‡æ ‡ç»“æœ"
                assert "è¯¦ç»†åˆ†æ" in report_content, "æŠ¥å‘Šåº”è¯¥åŒ…å«è¯¦ç»†åˆ†æ"
            
            # ä¿å­˜æ•°æ®é›†è¯¦æƒ…
            details_path = evaluator.save_dataset_details(dataset, temp_dir)
            assert os.path.exists(details_path), f"è¯¦æƒ…æ–‡ä»¶åº”è¯¥å­˜åœ¨: {details_path}"
            
            # éªŒè¯è¯¦æƒ…æ–‡ä»¶æ ¼å¼
            import json
            with open(details_path, 'r', encoding='utf-8') as f:
                details = json.load(f)
                assert isinstance(details, list), "è¯¦æƒ…åº”è¯¥æ˜¯åˆ—è¡¨æ ¼å¼"
                assert len(details) == len(dataset), "è¯¦æƒ…é•¿åº¦åº”è¯¥ä¸æ•°æ®é›†ä¸€è‡´"
        
        logger.info("âœ… æŠ¥å‘Šç”Ÿæˆæµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        logger.error(f"æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        raise

def test_full_evaluation_pipeline():
    """æµ‹è¯•å®Œæ•´è¯„ä¼°æµç¨‹"""
    logger.info("æµ‹è¯•å®Œæ•´è¯„ä¼°æµç¨‹...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
        queries, gold_answers = create_test_data()
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºæŸ¥è¯¢æ–‡ä»¶
            query_file = os.path.join(temp_dir, "test_queries.csv")
            queries_df = pd.DataFrame({"query": queries})
            queries_df.to_csv(query_file, index=False)
            
            # åˆ›å»ºæ ‡å‡†ç­”æ¡ˆæ–‡ä»¶
            gold_file = os.path.join(temp_dir, "test_answers.csv")
            gold_df = pd.DataFrame({
                "query": queries,
                "answer": gold_answers
            })
            gold_df.to_csv(gold_file, index=False)
            
            # è¿è¡Œå®Œæ•´è¯„ä¼°
            result = run_ragas_evaluation(
                query_file=query_file,
                gold_file=gold_file,
                top_k=3,
                output_dir=temp_dir
            )
            
            # éªŒè¯ç»“æœç»“æ„
            assert isinstance(result, dict), "è¯„ä¼°ç»“æœåº”è¯¥æ˜¯å­—å…¸"
            assert 'metrics' in result, "ç»“æœåº”è¯¥åŒ…å«metrics"
            assert 'report_path' in result, "ç»“æœåº”è¯¥åŒ…å«report_path"
            assert 'details_path' in result, "ç»“æœåº”è¯¥åŒ…å«details_path"
            assert 'sample_count' in result, "ç»“æœåº”è¯¥åŒ…å«sample_count"
            assert 'evaluation_time' in result, "ç»“æœåº”è¯¥åŒ…å«evaluation_time"
            
            # éªŒè¯æŒ‡æ ‡
            metrics = result['metrics']
            assert isinstance(metrics, dict), "æŒ‡æ ‡åº”è¯¥æ˜¯å­—å…¸"
            assert len(metrics) > 0, "æŒ‡æ ‡ä¸åº”è¯¥ä¸ºç©º"
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            assert os.path.exists(result['report_path']), f"æŠ¥å‘Šæ–‡ä»¶åº”è¯¥å­˜åœ¨: {result['report_path']}"
            assert os.path.exists(result['details_path']), f"è¯¦æƒ…æ–‡ä»¶åº”è¯¥å­˜åœ¨: {result['details_path']}"
            
            logger.info(f"å®Œæ•´è¯„ä¼°ç»“æœ: {result}")
        
        logger.info("âœ… å®Œæ•´è¯„ä¼°æµç¨‹æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        logger.error(f"å®Œæ•´è¯„ä¼°æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("RAGASé›†æˆæµ‹è¯•")
    logger.info("=" * 60)
    
    try:
        # æµ‹è¯•1: RAGASè¯„ä¼°å™¨åˆå§‹åŒ–
        evaluator = test_ragas_evaluator_initialization()
        
        # æµ‹è¯•2: æ•°æ®é›†å‡†å¤‡
        dataset = test_dataset_preparation(evaluator)
        
        # æµ‹è¯•3: æŒ‡æ ‡è¯„ä¼°
        results = test_metrics_evaluation(evaluator, dataset)
        
        # æµ‹è¯•4: æŠ¥å‘Šç”Ÿæˆ
        test_report_generation(evaluator, dataset, results)
        
        # æµ‹è¯•5: å®Œæ•´è¯„ä¼°æµç¨‹
        test_full_evaluation_pipeline()
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        logger.info("âœ… RAGASè¯„ä¼°å™¨é…ç½®æ­£ç¡®")
        logger.info("âœ… æ•°æ®é›†å‡†å¤‡åŠŸèƒ½æ­£å¸¸")
        logger.info("âœ… æŒ‡æ ‡è¯„ä¼°åŠŸèƒ½æ­£å¸¸")
        logger.info("âœ… æŠ¥å‘Šç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        logger.info("âœ… å®Œæ•´è¯„ä¼°æµç¨‹æ­£å¸¸")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        logger.error("=" * 60)
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        sys.exit(1) 