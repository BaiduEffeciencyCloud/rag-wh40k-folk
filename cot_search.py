import logging
import asyncio
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import time

from query_cot import QueryCoTExpander
from vector_search import VectorSearch
from config import (
    OPENAI_API_KEY,
    PINECONE_API_KEY,
    PINECONE_INDEX,
    DEFAULT_TEMPERATURE,
    DEFAULT_TOP_K,
    LOG_FORMAT,
    LOG_LEVEL,
    LLM_MODEL,
    QUERY_TIMEOUT,
    ANSWER_GENERATION_TEMPERATURE,
    ANSWER_MAX_TOKENS,
    DEDUPLICATION_THRESHOLD,
    DEFAULT_MAX_WORKERS,
    MAX_CONTEXT_RESULTS
)
from searchengine.subquery_retriever import SubQueryRetriever

# é…ç½®æ—¥å¿—
logging.basicConfig(level=LOG_LEVEL, format=LOG_FORMAT)
logger = logging.getLogger(__name__)

@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœæ•°æ®ç±»"""
    query: str
    query_type: str
    results: List[Dict[str, Any]]
    score: float
    timestamp: float

@dataclass
class IntegratedResult:
    """æ•´åˆåçš„ç»“æœæ•°æ®ç±»"""
    text: str
    score: float
    source_queries: List[str]
    metadata: Dict[str, Any]

class CoTQueryProcessor:
    """CoTæŸ¥è¯¢å¤„ç†å™¨ - å¤„ç†CoTç”Ÿæˆçš„å¤šä¸ªæŸ¥è¯¢å˜ä½“"""
    
    def __init__(self, cot_expander: QueryCoTExpander):
        self.cot_expander = cot_expander
        logger.info("CoTQueryProcessoråˆå§‹åŒ–å®Œæˆ")
    
    def classify_query(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        # æ¾„æ¸…æŸ¥è¯¢ï¼šåŒ…å«"æ˜¯ä»€ä¹ˆ"ã€"æ˜¯ä»€ä¹ˆï¼Ÿ"ç­‰
        if re.search(r'æ˜¯ä»€ä¹ˆ\??$', query_lower):
            return "clarification"
        
        # æ•ˆæœæŸ¥è¯¢ï¼šåŒ…å«"å¦‚ä½•"ã€"å½±å“"ã€"æ•ˆæœ"ç­‰
        if re.search(r'(å¦‚ä½•|å½±å“|æ•ˆæœ|é™ä½|æå‡)', query_lower):
            return "effect"
        
        # åº”ç”¨æŸ¥è¯¢ï¼šåŒ…å«"ä½¿ç”¨"ã€"åº”ç”¨"ã€"æˆ˜æœ¯"ç­‰
        if re.search(r'(ä½¿ç”¨|åº”ç”¨|æˆ˜æœ¯|ç­–ç•¥)', query_lower):
            return "application"
        
        # é»˜è®¤åˆ†ç±»
        return "general"
    
    def process_query(self, user_query: str) -> Dict[str, List[str]]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆç»“æ„åŒ–çš„æŸ¥è¯¢å˜ä½“
        
        Args:
            user_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æŒ‰ç±»å‹åˆ†ç»„çš„æŸ¥è¯¢å˜ä½“å­—å…¸
        """
        try:
            # ä½¿ç”¨CoTæ‰©å±•å™¨ç”ŸæˆæŸ¥è¯¢å˜ä½“
            expanded_queries = self.cot_expander.expand_query(user_query)
            
            # æŒ‰ç±»å‹åˆ†ç±»æŸ¥è¯¢
            query_groups = {
                "clarification": [],
                "effect": [],
                "application": [],
                "general": []
            }
            
            for query in expanded_queries:
                query_type = self.classify_query(query)
                query_groups[query_type].append(query)
            
            # è®°å½•åˆ†ç±»ç»“æœ
            logger.info(f"æŸ¥è¯¢åˆ†ç±»ç»“æœ: {query_groups}")
            
            return query_groups
            
        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            return {"general": [user_query]}

class MultiQueryRetriever:
    """å¤šæŸ¥è¯¢æ£€ç´¢å™¨ - å¹¶è¡Œå¤„ç†å¤šä¸ªæŸ¥è¯¢å˜ä½“"""
    
    def __init__(self, vector_search: VectorSearch, max_workers: int = DEFAULT_MAX_WORKERS):
        self.vector_search = vector_search
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        logger.info(f"MultiQueryRetrieveråˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
    
    def retrieve_single_query(self, query: str, query_type: str, top_k: int = DEFAULT_TOP_K) -> QueryResult:
        """æ£€ç´¢å•ä¸ªæŸ¥è¯¢"""
        try:
            start_time = time.time()
            # ä½¿ç”¨æ–°çš„ç›¸é‚»åˆ‡ç‰‡æ£€ç´¢æ–¹æ³•
            results = self.vector_search.search_with_adjacent_chunks(query, top_k, adjacent_range=2)
            end_time = time.time()
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            avg_score = sum(r.get('score', 0) for r in results) / len(results) if results else 0
            
            return QueryResult(
                query=query,
                query_type=query_type,
                results=results,
                score=avg_score,
                timestamp=end_time - start_time
            )
            
        except Exception as e:
            logger.error(f"æ£€ç´¢æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")
            return QueryResult(
                query=query,
                query_type=query_type,
                results=[],
                score=0.0,
                timestamp=0.0
            )
    
    def retrieve_for_queries(self, query_groups: Dict[str, List[str]], top_k: int = DEFAULT_TOP_K) -> Dict[str, List[QueryResult]]:
        """
        ä¸ºæ¯ç»„æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢
        
        Args:
            query_groups: æŒ‰ç±»å‹åˆ†ç»„çš„æŸ¥è¯¢åˆ—è¡¨
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            æ¯ç»„æŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
        """
        try:
            all_results = {}
            
            # ä¸ºæ¯ä¸ªæŸ¥è¯¢ç±»å‹å¹¶è¡Œæ‰§è¡Œæ£€ç´¢
            for query_type, queries in query_groups.items():
                if not queries:
                    all_results[query_type] = []
                    continue
                
                logger.info(f"å¼€å§‹æ£€ç´¢ {query_type} ç±»å‹çš„ {len(queries)} ä¸ªæŸ¥è¯¢")
                
                # å¹¶è¡Œæ‰§è¡Œæ£€ç´¢
                futures = []
                for query in queries:
                    future = self.executor.submit(self.retrieve_single_query, query, query_type, top_k)
                    futures.append(future)
                
                # æ”¶é›†ç»“æœ
                results = []
                for future in futures:
                    try:
                        result = future.result(timeout=QUERY_TIMEOUT)  # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
                        results.append(result)
                    except Exception as e:
                        logger.error(f"è·å–æ£€ç´¢ç»“æœæ—¶å‡ºé”™: {e}")
                
                all_results[query_type] = results
                logger.info(f"{query_type} ç±»å‹æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            
            return all_results
            
        except Exception as e:
            logger.error(f"å¤šæŸ¥è¯¢æ£€ç´¢æ—¶å‡ºé”™: {e}")
            return {}

class ResultIntegrator:
    """ç»“æœæ•´åˆå™¨ - æ™ºèƒ½æ•´åˆå¤šä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœ"""
    
    def __init__(self, deduplication_threshold: float = DEDUPLICATION_THRESHOLD):
        self.deduplication_threshold = deduplication_threshold
        logger.info(f"ResultIntegratoråˆå§‹åŒ–å®Œæˆï¼Œå»é‡é˜ˆå€¼: {deduplication_threshold}")
    
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„Jaccardç›¸ä¼¼åº¦ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡å¤„ç†"""
        if not results:
            return results
        
        deduplicated = []
        seen_texts = set()
        
        for result in results:
            text = result.get('text', '').strip()
            if not text:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰ç»“æœé‡å¤
            is_duplicate = False
            for seen_text in seen_texts:
                similarity = self.calculate_text_similarity(text, seen_text)
                if similarity > self.deduplication_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                deduplicated.append(result)
                seen_texts.add(text)
        
        logger.info(f"å»é‡å¤„ç†: {len(results)} -> {len(deduplicated)}")
        return deduplicated
    
    def rank_results(self, results: List[Dict]) -> List[Dict]:
        """æŒ‰ç›¸å…³æ€§æ’åº"""
        # æŒ‰åˆ†æ•°é™åºæ’åº
        sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
        return sorted_results
    
    def integrate_results(self, query_results: Dict[str, List[QueryResult]]) -> List[IntegratedResult]:
        """
        æ•´åˆå¤šä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
        
        Args:
            query_results: æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
            
        Returns:
            æ•´åˆåçš„ç»“æœåˆ—è¡¨
        """
        try:
            all_results = []
            
            # æ”¶é›†æ‰€æœ‰æ£€ç´¢ç»“æœ
            for query_type, query_result_list in query_results.items():
                for query_result in query_result_list:
                    for result in query_result.results:
                        # æ·»åŠ æŸ¥è¯¢ä¿¡æ¯åˆ°ç»“æœä¸­
                        result['source_query'] = query_result.query
                        result['query_type'] = query_result.query_type
                        result['query_score'] = query_result.score
                        all_results.append(result)
            
            if not all_results:
                return []
            
            # å»é‡å¤„ç†
            deduplicated_results = self.deduplicate_results(all_results)
            
            # æ’åºå¤„ç†
            ranked_results = self.rank_results(deduplicated_results)
            
            # è½¬æ¢ä¸ºIntegratedResultæ ¼å¼
            integrated_results = []
            for result in ranked_results:
                integrated_result = IntegratedResult(
                    text=result.get('text', ''),
                    score=result.get('score', 0.0),
                    source_queries=[result.get('source_query', '')],
                    metadata={
                        'query_type': result.get('query_type', ''),
                        'query_score': result.get('query_score', 0.0),
                        'original_metadata': result.get('metadata', {})
                    }
                )
                integrated_results.append(integrated_result)
            
            logger.info(f"ç»“æœæ•´åˆå®Œæˆ: {len(integrated_results)} ä¸ªæ•´åˆç»“æœ")
            return integrated_results
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}")
            return []

class HierarchicalAnswerGenerator:
    """åˆ†å±‚ç­”æ¡ˆç”Ÿæˆå™¨ - åŸºäºä¸åŒå±‚æ¬¡çš„æŸ¥è¯¢ç”Ÿæˆç»“æ„åŒ–ç­”æ¡ˆ"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        logger.info("HierarchicalAnswerGeneratoråˆå§‹åŒ–å®Œæˆ")
    
    def generate_answer(self, user_query: str, integrated_results: List[IntegratedResult]) -> str:
        """
        ç”Ÿæˆåˆ†å±‚çš„æœ€ç»ˆç­”æ¡ˆ
        
        Args:
            user_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            integrated_results: æ•´åˆåçš„æ£€ç´¢ç»“æœ
            
        Returns:
            ç»“æ„åŒ–çš„æœ€ç»ˆç­”æ¡ˆ
        """
        try:
            if not integrated_results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            # æå–æ£€ç´¢åˆ°çš„æ–‡æœ¬å†…å®¹
            retrieved_texts = [result.text for result in integrated_results[:MAX_CONTEXT_RESULTS]]  # é™åˆ¶ä¸Šä¸‹æ–‡ç»“æœæ•°
            
            # æ„å»ºåˆ†å±‚ç­”æ¡ˆç”Ÿæˆçš„æç¤º
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æˆ˜é”¤40Kè§„åˆ™ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å…¨é¢ä¸”ç»“æ„åŒ–çš„ç­”æ¡ˆã€‚

## ç”¨æˆ·é—®é¢˜
{user_query}

## ä¸Šä¸‹æ–‡ä¿¡æ¯
{chr(10).join([f"ã€{i+1}ã€‘{text}" for i, text in enumerate(retrieved_texts)])}

## å›ç­”è¦æ±‚
1. **ç»“æ„åŒ–ç»„ç»‡**ï¼šæŒ‰ç…§"å®šä¹‰â†’æ•ˆæœâ†’åº”ç”¨"çš„ç»“æ„ç»„ç»‡ç­”æ¡ˆ
2. **ä¿¡æ¯æ•´åˆ**ï¼šæ™ºèƒ½æ•´åˆæ¥è‡ªä¸åŒæŸ¥è¯¢çš„ä¿¡æ¯ï¼Œé¿å…é‡å¤
3. **å‡†ç¡®æ€§**ï¼šä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸æ·»åŠ æ¨æµ‹å†…å®¹
4. **å®Œæ•´æ€§**ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜
5. **æ¸…æ™°æ€§**ï¼šä½¿ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„è¯­è¨€è¡¨è¾¾

## å›ç­”ç»“æ„
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»„ç»‡ç­”æ¡ˆï¼š

### æ ¸å¿ƒå®šä¹‰
- è§£é‡Šç›¸å…³å•ä½ã€æŠ€èƒ½æˆ–è§„åˆ™çš„åŸºæœ¬æ¦‚å¿µ

### å…·ä½“æ•ˆæœ
- è¯¦ç»†è¯´æ˜ç›¸å…³æœºåˆ¶å’Œæ•ˆæœ
- åŒ…æ‹¬è§¦å‘æ¡ä»¶ã€å½±å“èŒƒå›´ç­‰

### å®é™…åº”ç”¨
- è¯´æ˜å¦‚ä½•åœ¨å®é™…æ¸¸æˆä¸­ä½¿ç”¨
- æä¾›æˆ˜æœ¯å»ºè®®å’Œæ³¨æ„äº‹é¡¹

è¯·ç›´æ¥è¾“å‡ºç»“æ„åŒ–çš„ç­”æ¡ˆï¼Œæ— éœ€æ·»åŠ "é—®é¢˜ï¼š"ã€"å›ç­”ï¼š"ç­‰æ ‡ç­¾ã€‚"""

            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            response = self.llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=ANSWER_GENERATION_TEMPERATURE,
                max_tokens=ANSWER_MAX_TOKENS
            )
            
            answer = response.choices[0].message.content.strip()
            logger.info("åˆ†å±‚ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            return answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"

class CoTSearch:
    """CoTæœç´¢ä¸»ç±» - æ•´åˆæ‰€æœ‰ç»„ä»¶"""
    
    def __init__(self, 
                 pinecone_api_key: str = PINECONE_API_KEY,
                 index_name: str = PINECONE_INDEX,
                 openai_api_key: str = OPENAI_API_KEY,
                 temperature: float = DEFAULT_TEMPERATURE,
                 max_workers: int = DEFAULT_MAX_WORKERS):
        """
        åˆå§‹åŒ–CoTæœç´¢ç³»ç»Ÿ
        
        Args:
            pinecone_api_key: Pinecone APIå¯†é’¥
            index_name: Pineconeç´¢å¼•åç§°
            openai_api_key: OpenAI APIå¯†é’¥
            temperature: ç”Ÿæˆæ¸©åº¦
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
        """
        # åˆå§‹åŒ–ç»„ä»¶
        self.cot_expander = QueryCoTExpander(openai_api_key, temperature)
        self.vector_search = VectorSearch(pinecone_api_key, index_name, openai_api_key)
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.query_processor = CoTQueryProcessor(self.cot_expander)
        self.retriever = MultiQueryRetriever(self.vector_search, max_workers)
        self.integrator = ResultIntegrator()
        self.answer_generator = HierarchicalAnswerGenerator(self.vector_search.client)
        self.subquery_retriever = SubQueryRetriever(pinecone_api_key, index_name, openai_api_key)
        
        logger.info("CoTSearchç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def extract_entity(self, query: str) -> str:
        """
        ç®€å•è§„åˆ™ï¼šå–ç¬¬ä¸€ä¸ªè¿ç»­çš„ä¸­æ–‡/è‹±æ–‡è¯ç»„ä½œä¸ºå®ä½“ï¼ˆå¯æ›¿æ¢ä¸ºæ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
        """
        import re
        m = re.search(r'([\u4e00-\u9fa5A-Za-z0-9_]+)', query)
        return m.group(1) if m else query[:4]

    def search(self, user_query: str, top_k: int = DEFAULT_TOP_K, use_new_retriever: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡ŒCoTæœç´¢
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
            use_new_retriever: æ˜¯å¦ä½¿ç”¨æ–°çš„æ£€ç´¢é€»è¾‘
            
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹CoTæœç´¢: {user_query}")
            
            # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æŸ¥è¯¢ï¼Œç”ŸæˆæŸ¥è¯¢å˜ä½“
            query_groups = self.query_processor.process_query(user_query)
            all_queries = [q for group in query_groups.values() for q in group]
            # æ–°æ£€ç´¢é€»è¾‘ï¼šå®ä½“è¯†åˆ«+filterå¬å›
            if use_new_retriever:
                logger.info("ä½¿ç”¨SubQueryRetrieverè¿›è¡Œæ£€ç´¢...")
                entities = [self.extract_entity(q) for q in all_queries]
                chunk_results = self.subquery_retriever.retrieve_for_queries(all_queries, entities, top_k)
                # è½¬ä¸ºåŸæœ‰æ ¼å¼ï¼ˆæ¯ä¸ªqueryä¸€ä¸ªQueryResultï¼‰
                query_results = {}
                idx = 0
                for group, queries in query_groups.items():
                    group_results = []
                    for q in queries:
                        results = chunk_results[idx]
                        avg_score = sum(r.get('score', 0) for r in results) / len(results) if results else 0
                        group_results.append(QueryResult(
                            query=q,
                            query_type=group,
                            results=results,
                            score=avg_score,
                            timestamp=0.0
                        ))
                        idx += 1
                    query_results[group] = group_results
            else:
                # åŸæœ‰æ£€ç´¢é€»è¾‘
                query_results = self.retriever.retrieve_for_queries(query_groups, top_k)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ•´åˆç»“æœ
            integrated_results = self.integrator.integrate_results(query_results)
            
            # ç¬¬å››æ­¥ï¼šç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_answer = self.answer_generator.generate_answer(user_query, integrated_results)
            
            end_time = time.time()
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'user_query': user_query,
                'final_answer': final_answer,
                'query_groups': query_groups,
                'query_results': query_results,
                'integrated_results': integrated_results,
                'processing_time': end_time - start_time,
                'total_queries': sum(len(queries) for queries in query_groups.values()),
                'total_results': len(integrated_results)
            }
            
            logger.info(f"CoTæœç´¢å®Œæˆï¼Œè€—æ—¶: {result['processing_time']:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"CoTæœç´¢æ—¶å‡ºé”™: {e}")
            return {
                'user_query': user_query,
                'final_answer': f"æŠ±æ­‰ï¼Œæœç´¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                'error': str(e)
            }
    
    def close(self):
        """å…³é—­èµ„æº"""
        try:
            self.retriever.executor.shutdown(wait=True)
            logger.info("CoTSearchèµ„æºå·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­èµ„æºæ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°ï¼šç”¨äºæµ‹è¯•CoTSearch"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CoTæœç´¢ç³»ç»Ÿæµ‹è¯•')
    parser.add_argument('query', type=str, help='è¦æœç´¢çš„æŸ¥è¯¢')
    parser.add_argument('--top-k', type=int, default=DEFAULT_TOP_K, help='æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡')
    parser.add_argument('--max-workers', type=int, default=DEFAULT_MAX_WORKERS, help='æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--use-new', action='store_true', help='ä½¿ç”¨æ–°æ£€ç´¢é€»è¾‘ï¼ˆSubQueryRetrieverï¼‰')
    args = parser.parse_args()
    
    print("ğŸ¤– CoTæœç´¢ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    print(f"ğŸ“ æŸ¥è¯¢: {args.query}")
    print(f"ğŸ“Š æ¯ä¸ªæŸ¥è¯¢ç»“æœæ•°: {args.top_k}")
    print(f"ğŸ”§ æœ€å¤§å¹¶å‘æ•°: {args.max_workers}")
    print(f"ğŸ†• ä½¿ç”¨æ–°æ£€ç´¢é€»è¾‘: {args.use_new}")
    print("-" * 60)
    
    try:
        # åˆ›å»ºCoTæœç´¢å®ä¾‹
        cot_search = CoTSearch(max_workers=args.max_workers)
        
        # æ‰§è¡Œæœç´¢
        result = cot_search.search(args.query, args.top_k, use_new_retriever=args.use_new)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“‹ æœç´¢ç»“æœ:")
        print("=" * 60)
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}ç§’")
        print(f"ğŸ” æ€»æŸ¥è¯¢æ•°: {result.get('total_queries', 0)}")
        print(f"ğŸ“„ æ€»ç»“æœæ•°: {result.get('total_results', 0)}")
        print("-" * 60)
        
        print("\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:")
        print(result.get('final_answer', 'æ— ç­”æ¡ˆ'))
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if args.verbose:
            print("\nğŸ” è¯¦ç»†ä¿¡æ¯:")
            print(f"æŸ¥è¯¢åˆ†ç»„: {result.get('query_groups', {})}")
            print(f"æ•´åˆç»“æœæ•°é‡: {len(result.get('integrated_results', []))}")
        
        print("=" * 60)
        print("âœ… æœç´¢å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        try:
            cot_search.close()
        except:
            pass


if __name__ == "__main__":
    main() 