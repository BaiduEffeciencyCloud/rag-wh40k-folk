import os
import sys
import json
import argparse
import logging
from typing import List, Dict, Any
import glob
from datetime import datetime
from dataupload.bm25_manager import BM25Manager
from dataupload.bm25_config import BM25Config
from dataupload.vector_builder import build_vector_data, build_vector_via_db, batch_upsert_vectors

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataupload.data_vector_v3 import SemanticDocumentChunker, save_chunks_to_text, save_chunks_to_json

from dataupload.knowledge_graph_manager import KnowledgeGraphManager
from storage.storage_factory import StorageFactory
from conn.conn_factory import ConnectionFactory
from embedding.em_factory import EmbeddingFactory

from config import OPENSEARCH_INDEX


# å¯¼å…¥configä¸­çš„æ—¥å¿—é…ç½®å‡½æ•°
from config import setup_logging

class UpsertManager:
    """
    æ•°æ®å…¥åº“æµç¨‹ç®¡ç†å™¨ (Refactored)
    
    èŒè´£ï¼š
    1. è°ƒç”¨æŒ‡å®šç‰ˆæœ¬çš„åˆ‡ç‰‡å™¨è¿›è¡Œæ–‡æ¡£åˆ‡ç‰‡ã€‚
    2. è°ƒç”¨ knowledge_graph_manager å°†å®ä½“å’Œå…³ç³»å†™å…¥Neo4jã€‚
    3. è°ƒç”¨ embedding_model è¿›è¡Œå‘é‡åŒ–ã€‚
    4. å°†å‘é‡åŒ–åçš„æ•°æ®ä¸Šä¼ åˆ° Pineconeã€‚
    5. ç»Ÿä¸€è°ƒåº¦ã€é…ç½®å’Œæ—¥å¿—è®°å½•ã€‚
    """
    def __init__(self,embedding_model: str = "qwen", environment: str = "production", chunker_version: str = "v3", log_file_path: str = None):
        """
        åˆå§‹åŒ–å…¥åº“ç®¡ç†å™¨
        
        Args:
            environment: è¿è¡Œç¯å¢ƒ ("production", "test", "development")
            chunker_version: åˆ‡ç‰‡å™¨ç‰ˆæœ¬ ("v2", "v3")
            log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜åˆ°æ–‡ä»¶
        """
        self.environment = environment
        self.chunker_version = chunker_version
        self.log_file_path = log_file_path
        self.chunker = None
        self.kg_manager = None
        self.pinecone_index = None
        self.embedding_model = EmbeddingFactory.create_embedding_model(embedding_model)
        self.db_conn = None
        self.bm25_manager = None  # æ–°å¢ï¼šBM25Managerå®ä¾‹
        self.bm25_config = BM25Config()  # æ–°å¢ï¼šBM25Configå®ä¾‹

        # é…ç½®æ—¥å¿—
        if log_file_path:
            setup_logging(log_file_path)
        
        logging.info(f"ğŸš€ UpsertManager åˆå§‹åŒ–ï¼Œç¯å¢ƒ: {self.environment}, åˆ‡ç‰‡å™¨ç‰ˆæœ¬: {self.chunker_version}")

    def _initialize_conn_and_kg(self, enable_kg: bool,db_type:str,faction:str):
        '''
        åˆå§‹åŒ–chunkerçš„å®ä¾‹,çŸ¥è¯†å›¾è°±å’Œæ•°æ®åº“è¿æ¥
        Args:
            enable_kg: æ˜¯å¦å¯ç”¨çŸ¥è¯†å›¾è°±å†™å…¥
            db_type: æ•°æ®åº“ç±»å‹
        '''
        if self.chunker is None:
            if self.chunker_version == 'v3':
                self.chunker = SemanticDocumentChunker(faction_name=faction)
                logging.info("âœ… SemanticDocumentChunker (v3) åˆå§‹åŒ–å®Œæˆ")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆ‡ç‰‡å™¨ç‰ˆæœ¬: {self.chunker_version}")
        if enable_kg and self.kg_manager is None:
            self.kg_manager = KnowledgeGraphManager(environment=self.environment)
            logging.info("âœ… KnowledgeGraphManager åˆå§‹åŒ–å®Œæˆ")
        try:
            self.db_conn = ConnectionFactory.create_conn(connection_type=db_type)
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise e
            

    def _get_latest_userdict_path(self):
        """è·å–dictæ–‡ä»¶å¤¹ä¸­æœ€æ–°çš„ç”¨æˆ·è¯å…¸æ–‡ä»¶"""
        dict_dir = "dict"
        if not os.path.exists(dict_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰ç”¨æˆ·è¯å…¸æ–‡ä»¶ï¼ˆ.txtæ ¼å¼ï¼‰
        dict_pattern = os.path.join(dict_dir, "*.txt")
        dict_files = glob.glob(dict_pattern)
        
        if not dict_files:
            return None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        dict_files.sort(reverse=True)
        return dict_files[0]
    
    def _get_latest_vocab_path(self):
        """è·å–dictæ–‡ä»¶å¤¹ä¸­æœ€æ–°çš„BM25è¯æ±‡è¡¨æ–‡ä»¶"""
        dict_dir = "dict"
        if not os.path.exists(dict_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰bm25_vocab_*.jsonæ–‡ä»¶
        vocab_pattern = os.path.join(dict_dir, "bm25_vocab_*.json")
        vocab_files = glob.glob(vocab_pattern)
        
        if not vocab_files:
            return None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        vocab_files.sort(reverse=True)
        return vocab_files[0]

    def _initialize_bm25_manager(self, userdict_path: str):
        """åˆå§‹åŒ–BM25Managerï¼Œæ”¯æŒæ¨¡å‹åŠ è½½/ä¿å­˜"""
        if self.bm25_manager is None:
            # ä½¿ç”¨BM25Configçš„å‚æ•°å®ä¾‹åŒ–BM25Manager
            self.bm25_manager = BM25Manager(
                k1=self.bm25_config.k1,
                b=self.bm25_config.b,
                min_freq=self.bm25_config.min_freq,
                max_vocab_size=self.bm25_config.max_vocab_size,
                user_dict_path=userdict_path
            )
            logging.info("âœ… BM25Manager åˆå§‹åŒ–å®Œæˆ")
        
        # è·å–æœ€æ–°çš„BM25è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
        vocab_path = self.bm25_manager.get_latest_freq_dict_file()
        
        # ä½¿ç”¨BM25Configç”Ÿæˆæ¨¡å‹è·¯å¾„
        model_path = self.bm25_config.get_model_path()
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(model_path) and vocab_path and os.path.exists(vocab_path):
            # æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼Œå°è¯•åŠ è½½
            try:
                self.bm25_manager.load_model(model_path, vocab_path)
                logging.info(f"âœ… BM25æ¨¡å‹å·²ä»æ–‡ä»¶åŠ è½½: {model_path}, {vocab_path}")
            except Exception as e:
                logging.warning(f"âš ï¸ åŠ è½½BM25æ¨¡å‹å¤±è´¥ï¼Œå°†é‡æ–°è®­ç»ƒ: {e}")
                self.bm25_manager.is_fitted = False
        else:
            logging.info("ğŸ“ BM25æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è¿›è¡Œè®­ç»ƒ")
            # ç¡®ä¿çŠ¶æ€ä¸ºæœªè®­ç»ƒ
            self.bm25_manager.is_fitted = False

    def _save_bm25_model(self):
        """ä¿å­˜BM25æ¨¡å‹åˆ°æ–‡ä»¶"""
        if self.bm25_manager and self.bm25_manager.is_fitted:
            # ä½¿ç”¨BM25Configç”Ÿæˆæ¨¡å‹è·¯å¾„
            model_path = self.bm25_config.get_model_path()
            # ç¡®ä¿modelsç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            # è·å–æœ€æ–°çš„BM25è¯æ±‡è¡¨è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆæ–°çš„
            vocab_path = self._get_latest_vocab_path()
            if not vocab_path:
                # å¦‚æœæ²¡æœ‰ç°æœ‰è¯æ±‡è¡¨ï¼Œç”Ÿæˆæ–°çš„
                vocab_path = self.bm25_config.get_vocab_path()
            
            # ç¡®ä¿dictç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(vocab_path), exist_ok=True)
            
            try:
                self.bm25_manager.save_model(model_path, vocab_path)
                logging.info(f"âœ… BM25æ¨¡å‹å·²ä¿å­˜åˆ°æ–‡ä»¶: {model_path}, {vocab_path}")
            except Exception as e:
                logging.warning(f"âš ï¸ ä¿å­˜BM25æ¨¡å‹å¤±è´¥: {e}")

    
    def process_and_upsert_document(self, file_path: str, enable_kg: bool = True, extra_metadata: dict = None, storage_type: str = 'local',db_type: str = 'opensearch',faction:str = None):
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶å°†å…¶å…¥åº“
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            enable_kg: æ˜¯å¦å¯ç”¨çŸ¥è¯†å›¾è°±å†™å…¥
            extra_metadata: é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚factionï¼‰ï¼Œä¼˜å…ˆçº§æœ€é«˜
        """
        if not os.path.exists(file_path):
            logging.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        logging.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
        self._initialize_conn_and_kg(enable_kg, db_type,faction)

        try:
            # 1. è¯»å–å’Œåˆ‡ç‰‡æ–‡æ¡£
            logging.info("Step 1: æ­£åœ¨è¿›è¡Œæ–‡æ¡£åˆ‡ç‰‡...")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç»„è£…extra_metadata
            base_metadata = {'source_file': os.path.basename(file_path)}
            if extra_metadata:
                base_metadata.update(extra_metadata)

            chunks = self.chunker.chunk_text(content, base_metadata)
            logging.info(f"âœ… æ–‡æ¡£åˆ‡ç‰‡å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ‡ç‰‡ã€‚")
            
            # ä¿å­˜åˆ‡ç‰‡åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œç”¨äºè§‚å¯Ÿåˆ‡ç‰‡æ•ˆæœ
            try:
                save_chunks_to_json(chunks, "dataupload/chunks")
                logging.info("âœ… åˆ‡ç‰‡å·²ä¿å­˜åˆ° dataupload/chunks ç›®å½•")
            except Exception as e:
                logging.warning(f"âš ï¸ ä¿å­˜åˆ‡ç‰‡æ–‡ä»¶æ—¶å‡ºç°è­¦å‘Š: {e}")

            # === ä¿®æ”¹ï¼šåªä½¿ç”¨ç°æœ‰è¯å…¸ï¼Œä¸é‡æ–°ç”Ÿæˆ ===
            # è·å–dictæ–‡ä»¶å¤¹ä¸‹æœ€æ–°çš„ç”¨æˆ·è¯å…¸æ–‡ä»¶
            userdict_path = self._get_latest_userdict_path()
            if userdict_path:
                logging.info(f"âœ… ä½¿ç”¨ç°æœ‰è¯å…¸: {userdict_path}")
            else:
                logging.warning("âš ï¸ æœªæ‰¾åˆ°è¯å…¸æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆ†è¯")
                userdict_path = None

            # === ä¿®æ”¹ï¼šåˆå§‹åŒ–BM25Managerå¹¶æ”¯æŒæ¨¡å‹åŠ è½½/ä¿å­˜ ===
            self._initialize_bm25_manager(userdict_path)

            # ç”Ÿæˆstorageå¯¹è±¡çš„ç›®çš„æ˜¯ä¸ºäº†æ”¯æŒBM25æ¨¡å‹çš„å¢é‡æ›´æ–°ï¼ˆincremental_updateï¼‰ï¼Œ
            # è¿™æ ·å¯ä»¥å°†å†å²åˆ‡ç‰‡æŒä¹…åŒ–å­˜å‚¨ï¼Œé¿å…æ¯æ¬¡éƒ½å…¨é‡é‡å»ºæ¨¡å‹ã€‚
            # å½“å‰å®ç°ä¸ºæœ¬åœ°å­˜å‚¨ï¼Œåç»­å¦‚éœ€æ”¯æŒäº‘ç«¯å­˜å‚¨ï¼ˆå¦‚S3ã€MongoDBç­‰ï¼‰ï¼Œåªéœ€æ‰©å±•StorageFactoryå³å¯æ— ç¼åˆ‡æ¢ã€‚
            storage = StorageFactory.create_storage(storage_type)

            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œåˆ™è¿›è¡Œè®­ç»ƒ
            if not self.bm25_manager.is_fitted:
                self.bm25_manager.fit([chunk['text'] for chunk in chunks])
                logging.info("âœ… BM25Managerå·²è®­ç»ƒå®Œæˆ")
                self._save_bm25_model()
            else:
                # æ¨¡å‹å·²å­˜åœ¨ï¼Œè¿›è¡Œå¢é‡æ›´æ–°
                self.bm25_manager.incremental_update([chunk['text'] for chunk in chunks], storage=storage)
                logging.info("âœ… BM25Managerå·²è¿›è¡Œå¢é‡æ›´æ–°")
                self._save_bm25_model()

            # 2. çŸ¥è¯†å›¾è°±å†™å…¥
            if enable_kg and self.kg_manager:
                logging.info("Step 2: æ­£åœ¨å†™å…¥çŸ¥è¯†å›¾è°±...")
                total_entities = 0
                total_relationships = 0
                for i, chunk in enumerate(chunks):
                    entities = self.kg_manager.extract_entities(chunk['text'], chunk.get('section_heading'), chunk.get('hierarchy'))
                    relationships = self.kg_manager.extract_relationships(chunk['text'], entities, chunk.get('content_type'))
                    
                    if entities or relationships:
                        self.kg_manager.update_knowledge_graph(entities, relationships, chunk)
                        total_entities += len(entities)
                        total_relationships += len(relationships)
                        logging.debug(f"  - Chunk {i+1}: å†™å…¥ {len(entities)} å®ä½“, {len(relationships)} å…³ç³»")
                
                logging.info(f"âœ… çŸ¥è¯†å›¾è°±å†™å…¥å®Œæˆï¼Œå…±è®¡å†™å…¥ {total_entities} ä¸ªå®ä½“, {total_relationships} ä¸ªå…³ç³»ã€‚")

            # 3. å‘é‡ä¸Šä¼ 
            if self.db_conn and self.embedding_model:
                logging.info("Step 3: æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶ä¸Šä¼ åˆ° Pinecone...")
                
                # æ„å»ºå¾…ä¸Šä¼ çš„æ•°æ®å’Œä¸Šä¼ è®°å½•
                vectors_to_upsert = []
                upload_records = []
                chunk_id_mapping = {}
                
                for i, chunk in enumerate(chunks):
                    # ä½¿ç”¨factionä½œä¸ºIDå‰ç¼€ï¼Œæ ¼å¼ï¼šfaction_number
                    faction = base_metadata.get('faction', 'unknown')
                    chunk_id = f"{faction}_{i}"
                    chunk_id_mapping[i] = chunk_id
                    try:
                        embedding = self.embedding_model.get_embedding(chunk['text'])
                        logging.info(f"[embedding] {chunk_id}çš„embeddingå‘é‡ç”Ÿæˆå®Œæ¯•.")
                        # ç”¨build_vector_via_dbæ ¹æ®æ•°æ®åº“ç±»å‹æ„é€ vector
                        vector_data = build_vector_via_db(
                            chunk=chunk,
                            embedding=embedding,
                            bm25_manager=self.bm25_manager,  # ä½¿ç”¨å®ä¾‹å˜é‡
                            chunk_id=chunk_id,
                            base_metadata=base_metadata,
                            db_str=db_type  # ä¼ é€’æ•°æ®åº“ç±»å‹
                        )
                        logging.info("[vector_Data] vector_dataåˆ›å»ºå®Œæ¯•")
                        vectors_to_upsert.append(vector_data)
                        # å…ˆæ ‡è®°ä¸ºpendingï¼Œç­‰æ‰¹é‡ä¸Šä¼ å®Œæˆåå†æ›´æ–°çŠ¶æ€
                        # ä»vector_dataä¸­æå–éœ€è¦çš„å…ƒæ•°æ®å­—æ®µ
                        chunk_metadata = {
                            'chunk_id': chunk_id,
                            'chunk_type': vector_data.get('chunk_type', ''),
                            'content_type': vector_data.get('content_type', ''),
                            'faction': vector_data.get('faction', ''),
                            'section_heading': vector_data.get('section_heading', ''),
                            'source_file': vector_data.get('source_file', ''),
                            'chunk_index': vector_data.get('chunk_index', 0)
                        }
                        upload_records.append({
                            'chunk_id': chunk_id,
                            'status': 'pending',
                            'metadata': chunk_metadata
                        })
                        
                        # è®°å½•ç¨€ç–å‘é‡ç”Ÿæˆæƒ…å†µ
                        if 'sparse' in vector_data and vector_data['sparse']:
                            logging.info(f"[SPARSE_STATS] {chunk_id} - ç¨€ç–å‘é‡ç”ŸæˆæˆåŠŸ")
                        else:
                            logging.warning(f"[SPARSE_STATS] {chunk_id} - ç¨€ç–å‘é‡ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©º")
                    except Exception as e:
                        logging.error(f"[ERROR] Chunk {i} embedding/upload failed: {e}")
                        upload_records.append({
                            'chunk_id': chunk_id,
                            'status': 'fail',
                            'error': str(e)
                        })

                # æ‰§è¡Œæ‰¹é‡ä¸Šä¼ 
                try:
                    batch_results = batch_upsert_vectors(self.db_conn, vectors_to_upsert, batch_size=100, index_name=OPENSEARCH_INDEX)
                    logging.info(f"[DEBUG] batch_results: {batch_results}")
                    logging.info(f"[DEBUG] batch_resultsç±»å‹: {type(batch_results)}")
                    logging.info(f"[DEBUG] batch_resultsé•¿åº¦: {len(batch_results)}")
                    
                    # æ ¹æ®æ‰¹æ¬¡ç»“æœæ›´æ–°æ¯ä¸ªchunkçš„çŠ¶æ€
                    batch_size = 100
                    for batch_idx, batch_success in enumerate(batch_results):
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(upload_records))
                        
                        logging.info(f"[DEBUG] æ‰¹æ¬¡ {batch_idx}: batch_success={batch_success}, start_idx={start_idx}, end_idx={end_idx}")
                        
                        if batch_success:
                            # æ‰¹æ¬¡æˆåŠŸï¼Œæ›´æ–°æ‰€æœ‰pendingçš„è®°å½•ä¸ºsuccess
                            for record_idx in range(start_idx, end_idx):
                                if upload_records[record_idx]['status'] == 'pending':
                                    upload_records[record_idx]['status'] = 'success'
                            logging.info(f"[DEBUG] æ‰¹æ¬¡ {batch_idx} æˆåŠŸ: æ›´æ–°äº† {end_idx - start_idx} ä¸ªè®°å½•ä¸ºsuccess")
                        else:
                            # æ‰¹æ¬¡å¤±è´¥ï¼Œæ›´æ–°æ‰€æœ‰pendingçš„è®°å½•ä¸ºfail
                            for record_idx in range(start_idx, end_idx):
                                if upload_records[record_idx]['status'] == 'pending':
                                    upload_records[record_idx]['status'] = 'fail'
                                    upload_records[record_idx]['error'] = f"æ‰¹æ¬¡{batch_idx+1}ä¸Šä¼ å¤±è´¥"
                            logging.info(f"[DEBUG] æ‰¹æ¬¡ {batch_idx} å¤±è´¥: æ›´æ–°äº† {end_idx - start_idx} ä¸ªè®°å½•ä¸ºfail")
                    
                    logging.info("âœ… æ‰¹é‡ä¸Šä¼ å®Œæˆï¼ŒçŠ¶æ€å·²æ›´æ–°")
                except Exception as e:
                    logging.error(f"âŒ æ‰¹é‡ä¸Šä¼ å¤±è´¥: {e}")
                    # æ‰¹é‡ä¸Šä¼ å¤±è´¥ï¼Œæ›´æ–°æ‰€æœ‰pendingçš„è®°å½•ä¸ºfail
                    for record in upload_records:
                        if record['status'] == 'pending':
                            record['status'] = 'fail'
                            record['error'] = f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}"
                
                
                upload_log_file = f"dataupload/upload_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                os.makedirs(os.path.dirname(upload_log_file), exist_ok=True)
                
                # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„æ•°é‡
                successful_uploads = sum(1 for record in upload_records if record['status'] == 'success')
                failed_uploads = sum(1 for record in upload_records if record['status'] == 'fail')
                # ç»Ÿè®¡ç¨€ç–å‘é‡ç”Ÿæˆæƒ…å†µ
                sparse_vectors_count = sum(1 for vector in vectors_to_upsert if 'sparse' in vector and vector['sparse'])
                dense_only_count = len(vectors_to_upsert) - sparse_vectors_count
                
                logging.info(f"âœ… Pinecone å‘é‡ä¸Šä¼ å®Œæˆ:")
                logging.info(f"  - æ€»åˆ‡ç‰‡æ•°: {len(chunks)}")
                logging.info(f"  - æˆåŠŸä¸Šä¼ : {successful_uploads}")
                logging.info(f"  - ä¸Šä¼ å¤±è´¥: {failed_uploads}")
                logging.info(f"  - åŒ…å«ç¨€ç–å‘é‡: {sparse_vectors_count}")
                logging.info(f"  - ä»…ç¨ å¯†å‘é‡: {dense_only_count}")
                logging.info(f"  - ä¸Šä¼ è®°å½•å·²ä¿å­˜åˆ°: {upload_log_file}")
                
                # å¦‚æœç¨€ç–å‘é‡ç”Ÿæˆç‡è¿‡ä½ï¼Œç»™å‡ºè­¦å‘Š
                if sparse_vectors_count == 0:
                    logging.error(f"âŒ è­¦å‘Š: æ‰€æœ‰ {len(vectors_to_upsert)} ä¸ªå‘é‡éƒ½æ²¡æœ‰ç”Ÿæˆç¨€ç–å‘é‡ï¼")
                elif sparse_vectors_count < len(vectors_to_upsert) * 0.5:
                    logging.warning(f"âš ï¸ ç¨€ç–å‘é‡ç”Ÿæˆç‡è¾ƒä½: {sparse_vectors_count}/{len(vectors_to_upsert)} ({sparse_vectors_count/len(vectors_to_upsert)*100:.1f}%)")
                
                # æ£€æŸ¥ç‰¹å®šåˆ‡ç‰‡æ˜¯å¦ä¸Šä¼ æˆåŠŸ
                if failed_uploads > 0:
                    logging.warning("âš ï¸ éƒ¨åˆ†åˆ‡ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ è®°å½•æ–‡ä»¶")
                    for record in upload_records:
                        if record['status'] == 'fail':
                            logging.warning(f"  - åˆ‡ç‰‡ {record['chunk_id']} ä¸Šä¼ å¤±è´¥: {record.get('error', 'æœªçŸ¥é”™è¯¯')}")

            logging.info(f"ğŸ‰ æ–‡æ¡£ {file_path} å¤„ç†å®Œæˆï¼")

        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ–‡æ¡£ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        
        finally:
            if self.kg_manager:
                self.kg_manager.close()
                logging.info("Neo4j è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºå‘½ä»¤è¡Œè°ƒç”¨"""
    parser = argparse.ArgumentParser(description="æ–‡æ¡£å…¥åº“æµç¨‹ç®¡ç†å™¨")
    parser.add_argument("file_path", type=str, help="è¦å¤„ç†çš„æ–‡æ¡£è·¯å¾„")
    parser.add_argument("--env", type=str, default="production", choices=["production", "test", "development"], help="è¿è¡Œç¯å¢ƒ")
    parser.add_argument("-cv", "--chunker-version", type=str, default="v3", choices=["v2", "v3"], help="é€‰æ‹©ä½¿ç”¨çš„åˆ‡ç‰‡å™¨ç‰ˆæœ¬ (v2/v3)")
    parser.add_argument("--no-kg", action="store_true", help="ç¦ç”¨çŸ¥è¯†å›¾è°±å†™å…¥")
    parser.add_argument("--db", type=str, default="opensearch", choices=["pinecone", "opensearch"], help="é€‰æ‹©ä½¿ç”¨çš„æ•°æ®åº“")
    parser.add_argument("--faction", type=str, help="æŒ‡å®šfactionï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰")
    parser.add_argument("--storage", type=str, default="local", choices=["local", "cloud"], help="å­˜å‚¨ç±»å‹")
    parser.add_argument("--embedding_model", type=str, default="qwen", choices=["qwen", "openai"], help="é€‰æ‹©ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹")
    args = parser.parse_args()
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„
    input_filename = os.path.splitext(os.path.basename(args.file_path))[0]
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_filename = f"{input_filename}_{timestamp}.log"
    log_file_path = os.path.join("dataupload", "log", log_filename)

    
    manager = UpsertManager(embedding_model=args.embedding_model,environment=args.env, chunker_version=args.chunker_version, log_file_path=log_file_path)
    # ç»„è£…extra_metadataï¼Œä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    extra_metadata = {'source_file': os.path.basename(args.file_path)}
    if args.faction:
        extra_metadata['faction'] = args.faction
    manager.process_and_upsert_document(
        file_path=args.file_path,
        enable_kg=not args.no_kg,
        extra_metadata=extra_metadata,
        storage_type=args.storage,
        db_type=args.db,
        faction=args.faction
        )


if __name__ == "__main__":
    main()
