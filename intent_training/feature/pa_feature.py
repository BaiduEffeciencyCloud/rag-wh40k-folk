#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½ç½®æ„ŸçŸ¥ç‰¹å¾æå–å™¨ (Position Aware Feature Extractor)
å®ç°ä½ç½®æƒé‡åŒ¹é…ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…ã€åˆ†å±‚å…³é”®è¯ä½“ç³»ç­‰ä¼˜åŒ–åŠŸèƒ½
æ ¸å¿ƒé€»è¾‘ï¼šé€šè¿‡ä½ç½®æƒé‡å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¥åŒºåˆ†Query/List/Compareæ„å›¾
"""

import logging
import numpy as np
import joblib
import os
import sys
import threading
from typing import Dict, List, Tuple, Any
from sklearn.feature_extraction.text import TfidfVectorizer
from difflib import SequenceMatcher
import jieba
from .base import BaseFeatureExtractor
from .complexity import ComplexityCalculator
from sentence_transformers import SentenceTransformer

# å¯¼å…¥ ModelManager ç”¨äºé¢„åŠ è½½æ¨¡å‹æ”¯æŒ
try:
    import sys
    import os
    # æ·»åŠ  rag-api-local ç›®å½•åˆ° Python è·¯å¾„
    rag_api_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../rag-api-local'))
    if rag_api_path not in sys.path:
        sys.path.insert(0, rag_api_path)
    
    # ç›´æ¥å¯¼å…¥ ModelManagerï¼Œä¿æŒå•ä¾‹æ¨¡å¼
    from app.services.model_manager import ModelManager  # type: ignore
except Exception:
    ModelManager = None

logger = logging.getLogger(__name__)


class EnhancedFeatureExtractor(BaseFeatureExtractor):
    """å¢å¼ºç‰¹å¾æå–å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¢å¼ºç‰¹å¾æå–å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        super().__init__(config)
        self.config = config
        self.tfidf_vectorizer = None
        self.primary_keywords = self._init_primary_keywords()
        self.secondary_keywords = self._init_secondary_keywords()
        self.negative_keywords = self._init_negative_keywords()

        # åˆå§‹åŒ–å¤æ‚åº¦è®¡ç®—å™¨
        self.complexity_calculator = ComplexityCalculator(config)

        # ä»é…ç½®æ–‡ä»¶è¯»å–evilgood_featureé…ç½®
        self._init_evilgood_feature_from_config()

        # çº¿ç¨‹é”ï¼Œç¡®ä¿å¥å‘é‡æ¨¡å‹åªè¢«åŠ è½½ä¸€æ¬¡
        self._model_lock = threading.Lock()
        self._model_loaded = False
        
    def _init_primary_keywords(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–ä¸»è¦å…³é”®è¯ - ä»config.yamlè¯»å–intent_keywords"""
        try:
            # ä»config.yamlè¯»å–intent_keywords
            intent_keywords = self.config.get('advanced_feature', {}).get('intent_keywords', {})
            
            # éªŒè¯é…ç½®æ˜¯å¦å­˜åœ¨
            if not intent_keywords:
                raise ValueError("é…ç½®ä¸­ç¼ºå°‘intent_keywords")
            
            # éªŒè¯é…ç½®æ ¼å¼æ˜¯å¦æ­£ç¡®
            if not isinstance(intent_keywords, dict):
                raise ValueError("intent_keywordså¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            
            # éªŒè¯å¿…éœ€çš„æ„å›¾é”®æ˜¯å¦å­˜åœ¨
            required_intents = ['query', 'list', 'compare', 'rule']
            for intent in required_intents:
                if intent not in intent_keywords:
                    raise ValueError(f"é…ç½®ä¸­ç¼ºå°‘å¿…éœ€çš„æ„å›¾é”®: {intent}")
                if not isinstance(intent_keywords[intent], list):
                    raise ValueError(f"æ„å›¾é”® {intent} çš„å€¼å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼")
                if len(intent_keywords[intent]) == 0:
                    raise ValueError(f"æ„å›¾é”® {intent} çš„å…³é”®è¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            
            logger.info(f"æˆåŠŸä»config.yamlè¯»å–intent_keywords: {list(intent_keywords.keys())}")
            return intent_keywords
            
        except Exception as e:
            logger.error(f"è¯»å–intent_keywordså¤±è´¥: {e}")
            raise
        
    def _init_secondary_keywords(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ¬¡è¦å…³é”®è¯ï¼ˆç§»é™¤è¾…åŠ©è¯æ±‡ï¼‰"""
        return {
            'Query': ['æŸ¥è¯¢', 'äº†è§£', 'çŸ¥é“'],  # ç§»é™¤è¾…åŠ©è¯æ±‡
            'List': ['æŸ¥çœ‹', 'æµè§ˆ', 'å±•ç¤º', 'æ˜¾ç¤º', 'åˆ—å‡º', 'æä¾›', 'ç»™å‡º'],  # ç§»é™¤è¾…åŠ©è¯æ±‡
            'Compare': ['å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«', 'ä¸åŒ', 'ç›¸ä¼¼', 'ç›¸åŒ'],
            'Rule': ['è§„å®š', 'çº¦æŸ', 'é™åˆ¶', 'å…è®¸', 'ç¦æ­¢', 'å¯ä»¥', 'ä¸èƒ½']
        }
        
    def _init_negative_keywords(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–å¦å®šå…³é”®è¯"""
        return {
            'Query': ['ä¸æ˜¯', 'æ²¡æœ‰', 'ä¸åŒ…å«', 'ä¸åŒ…å«'],
            'List': ['ä¸æ˜¾ç¤º', 'éšè—', 'æ’é™¤', 'é™¤äº†'],
            'Compare': ['ä¸æ¯”', 'ä¸å¦‚', 'ä¸é«˜äº', 'ä¸ä½äº'],
            'Rule': ['ä¸è¦æ±‚', 'ä¸é™åˆ¶', 'ä¸å¿…é¡»', 'ä¸éœ€è¦']
        }
        
    def _init_malicious_patterns(self) -> List[str]:
        """åˆå§‹åŒ–æ¶æ„è¯æ±‡æ¨¡å¼"""
        return [
            'æˆ‘ä¸æƒ³çŸ¥é“', 'ä¸è¦å‘Šè¯‰æˆ‘', 'åˆ«è¯´å‡ºæ¥', 'ä¸æƒ³çŸ¥é“',
            'åˆ«è¯´äº†', 'é—­å˜´', 'ä¸è¦å›ç­”', 'åˆ«å›ç­”',
            'æˆ‘ä¸æƒ³å¬', 'ä¸è¦è§£é‡Š', 'åˆ«è§£é‡Š', 'ä¸æƒ³äº†è§£',
            'ä¸è¦è¯´æ˜', 'åˆ«è¯´æ˜', 'ä¸æƒ³æŸ¥è¯¢', 'ä¸è¦æŸ¥è¯¢'
        ]
        
    def _init_helper_patterns(self) -> List[str]:
        """åˆå§‹åŒ–è¾…åŠ©è¯æ±‡æ¨¡å¼"""
        return [
            'æˆ‘æƒ³çŸ¥é“', 'å‘Šè¯‰æˆ‘', 'å›ç­”æˆ‘', 'è¯·å›ç­”',
            'å¸®æˆ‘æŸ¥ä¸€ä¸‹', 'èƒ½å¦è¯´æ˜', 'è¯·è§£é‡Š', 'å¸®æˆ‘çœ‹çœ‹',
            'èƒ½å¦å‘Šè¯‰æˆ‘', 'è¯·å‘Šè¯‰æˆ‘', 'å¸®æˆ‘æŸ¥è¯¢', 'èƒ½å¦æŸ¥è¯¢'
        ]
        
    def _contains_malicious_patterns(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«æ¶æ„è¯æ±‡"""
        malicious_patterns = getattr(self, 'malicious_patterns', None)
        if malicious_patterns is None:
            malicious_patterns = self._init_malicious_patterns()
        return any(pattern in text for pattern in malicious_patterns)
        
    def _contains_helper_patterns(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«è¾…åŠ©è¯æ±‡"""
        helper_patterns = getattr(self, 'helper_patterns', None)
        if helper_patterns is None:
            helper_patterns = self._init_helper_patterns()
        return any(pattern in text for pattern in helper_patterns)
        
    def _get_malicious_weight(self, text: str) -> float:
        """è®¡ç®—æ¶æ„è¯æ±‡æƒé‡ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        coefficient = self._get_malicious_adjustment_coefficient(text)
        # å¦‚æœç³»æ•°æ˜¯1.0ï¼Œè¯´æ˜æ²¡æœ‰æ¶æ„è¯æ±‡ï¼Œè¿”å›0.0
        # å¦‚æœç³»æ•°ä¸æ˜¯1.0ï¼Œè¯´æ˜æœ‰æ¶æ„è¯æ±‡ï¼Œè¿”å›ç³»æ•°å€¼
        return 0.0 if coefficient == 1.0 else coefficient
        
    def _get_helper_context_weight(self, text: str, keyword: str, pos: int) -> float:
        """è®¡ç®—è¾…åŠ©è¯æ±‡ä¸Šä¸‹æ–‡æƒé‡ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        return self._get_helper_adjustment_coefficient(text, keyword, pos)
        
    def position_weighted_matching(self, query: str, keywords: List[str]) -> float:
        """ä½ç½®æƒé‡åŒ¹é…"""
        matches = []
        for keyword in keywords:
            if keyword in query:
                pos = query.find(keyword)
                
                # æ£€æŸ¥æ˜¯å¦è¢«è¾…åŠ©è¯æ±‡åŒ…å›´
                helper_weight = self._get_helper_context_weight(query, keyword, pos)
                
                # å¦‚æœè¢«è¾…åŠ©è¯æ±‡åŒ…å›´ï¼Œé™ä½æƒé‡
                if helper_weight < 1.0:
                    weight = helper_weight
                else:
                    # æ­£å¸¸ä½ç½®æƒé‡é€»è¾‘
                    if pos < len(query) * 0.2:
                        weight = 0.8
                    elif pos > len(query) * 0.8:
                        weight = 0.9
                    else:
                        weight = 1.2
                
                matches.append(weight)
        return sum(matches) if matches else 0.0
        
    def context_aware_matching(self, query: str, keywords: List[str]) -> float:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥åŒ¹é…"""
        matches = []
        for keyword in keywords:
            if keyword in query:
                # æ£€æŸ¥å…³é”®è¯å‰åçš„è¯
                pos = query.find(keyword)
                before = query[max(0, pos-4):pos]
                after = query[pos+len(keyword):pos+len(keyword)+4]
                
                # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´æƒé‡
                context_weight = 1.0
                if any(word in before for word in ['å“ªä¸ª', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ']):
                    context_weight *= 1.3  # ç–‘é—®è¯å¢å¼ºæƒé‡
                if any(word in after for word in ['çš„', 'å€¼', 'å±æ€§', 'èƒ½åŠ›', 'æŠ€èƒ½','æ­¦å™¨','å…³é”®å­—','æŠ€èƒ½']):
                    context_weight *= 1.2  # å±æ€§è¯å¢å¼ºæƒé‡
                matches.append(context_weight)
        return sum(matches) if matches else 0.0
        
    def fuzzy_keyword_matching(self, query: str, keywords: List[str]) -> float:
        """æ¨¡ç³ŠåŒ¹é…"""
        matches = []
        for keyword in keywords:
            # è®¡ç®—ç›¸ä¼¼åº¦
            max_similarity = 0.0
            for i in range(len(query) - len(keyword) + 1):
                similarity = SequenceMatcher(None, query[i:i+len(keyword)], keyword).ratio()
                max_similarity = max(max_similarity, similarity)
            if max_similarity > 0.8:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                matches.append(max_similarity)
        return sum(matches) if matches else 0.0
        
    def _match_cross_char_pattern(self, query: str, pattern: str) -> float:
        """åŒ¹é…è·¨å­—ç¬¦æ¨¡å¼ï¼Œå¦‚"éƒ½...å“ªäº›"ã€"å½“...æ—¶"ç­‰"""
        if "..." not in pattern:
            return 0.0
        
        # åˆ†å‰²æ¨¡å¼
        parts = pattern.split("...")
        if len(parts) != 2:
            return 0.0
        
        start_part, end_part = parts
        
        # æŸ¥æ‰¾å¼€å§‹éƒ¨åˆ†
        start_idx = query.find(start_part)
        if start_idx == -1:
            return 0.0
        
        # æŸ¥æ‰¾ç»“æŸéƒ¨åˆ†ï¼ˆåœ¨å¼€å§‹éƒ¨åˆ†ä¹‹åï¼‰
        end_idx = query.find(end_part, start_idx + len(start_part))
        if end_idx == -1:
            return 0.0
        
        # è®¡ç®—åŒ¹é…åº¦
        total_length = len(start_part) + len(end_part)
        pattern_length = len(pattern)
        match_ratio = total_length / pattern_length
        
        # ç¡®ä¿åŒ¹é…åº¦åœ¨åˆç†èŒƒå›´å†…
        return min(match_ratio, 1.0)

    def cross_char_pattern_matching(self, query: str, keywords: List[str]) -> float:
        """è·¨å­—ç¬¦æ¨¡å¼åŒ¹é… - é€‰æ‹©æœ€é«˜åŒ¹é…åº¦"""
        max_similarity = 0.0
        for keyword in keywords:
            if "..." in keyword:
                similarity = self._match_cross_char_pattern(query, keyword)
                max_similarity = max(max_similarity, similarity)
        
        return max_similarity
        
    def extract_structural_features(self, text: str) -> np.ndarray:
        """æå–ç»“æ„ç‰¹å¾"""
        features = []
        
        # æ¶æ„è¯æ±‡æ£€æµ‹ - ä½¿ç”¨é…ç½®é©±åŠ¨çš„è°ƒæ•´ç³»æ•°
        malicious_coefficient = self._get_malicious_adjustment_coefficient(text)
        
        # ç–‘é—®è¯ç‰¹å¾
        question_words = ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'æ€ä¹ˆ', 'å“ªäº›']
        question_count = sum(1 for word in question_words if word in text)
        # å¦‚æœæ¶æ„è¯æ±‡å­˜åœ¨ï¼Œé™ä½ç–‘é—®è¯æƒé‡
        if malicious_coefficient < 1.0:
            question_count *= malicious_coefficient
        features.append(question_count)
        
        # æ•°é‡è¯ç‰¹å¾
        quantity_words = ['å“ªäº›', 'å¤šå°‘ä¸ª', 'å¤šå°‘ç§', 'å‡ ä¸ª', 'å¤šå°‘']
        quantity_count = sum(1 for word in quantity_words if word in text)
        # å¦‚æœæ¶æ„è¯æ±‡å­˜åœ¨ï¼Œé™ä½æ•°é‡è¯æƒé‡
        if malicious_coefficient < 1.0:
            quantity_count *= malicious_coefficient
        features.append(quantity_count)
        
        # æ¯”è¾ƒè¯ç‰¹å¾
        comparison_words = ['å“ªä¸ª', 'ç›¸æ¯”', 'æ¯”è¾ƒ', 'æ›´', 'æœ€é«˜', 'æœ€ä½', 'æœ€è¿œ', 'æœ€è¿‘']
        comparison_count = sum(1 for word in comparison_words if word in text)
        # å¦‚æœæ¶æ„è¯æ±‡å­˜åœ¨ï¼Œé™ä½æ¯”è¾ƒè¯æƒé‡
        if malicious_coefficient < 1.0:
            comparison_count *= malicious_coefficient
        features.append(comparison_count)
        
        # å¥å­é•¿åº¦ç‰¹å¾
        sentence_length = len(text)
        # å¦‚æœæ¶æ„è¯æ±‡å­˜åœ¨ï¼Œé™ä½å¥å­é•¿åº¦æƒé‡
        if malicious_coefficient < 1.0:
            sentence_length *= malicious_coefficient
        features.append(sentence_length)
        
        # è¯æ•°ç‰¹å¾
        word_count = len(list(jieba.cut(text)))
        # å¦‚æœæ¶æ„è¯æ±‡å­˜åœ¨ï¼Œé™ä½è¯æ•°æƒé‡
        if malicious_coefficient < 1.0:
            word_count *= malicious_coefficient
        features.append(word_count)
        
        # æ ‡ç‚¹ç¬¦å·ç‰¹å¾
        punctuation_count = sum(1 for char in text if char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘')
        # å¦‚æœæ¶æ„è¯æ±‡å­˜åœ¨ï¼Œé™ä½æ ‡ç‚¹ç¬¦å·æƒé‡
        if malicious_coefficient < 1.0:
            punctuation_count *= malicious_coefficient
        features.append(punctuation_count)
        
        return np.array(features)
        
    def extract_interaction_features(self, char_features: np.ndarray, 
                                   semantic_features: np.ndarray, 
                                   statistical_features: np.ndarray) -> np.ndarray:
        """æå–ç‰¹å¾äº¤äº’"""
        # è·å–æœ€å°ç»´åº¦è¿›è¡Œäº¤äº’
        min_dim = min(char_features.shape[1], semantic_features.shape[1], statistical_features.shape[1])
        
        # æˆªå–åˆ°ç›¸åŒç»´åº¦è¿›è¡Œäº¤äº’
        char_subset = char_features[:, :min_dim]
        semantic_subset = semantic_features[:, :min_dim]
        statistical_subset = statistical_features[:, :min_dim]
        
        # ç‰¹å¾äº¤äº’
        char_semantic_interaction = char_subset * semantic_subset  # å­—ç¬¦-è¯­ä¹‰äº¤äº’
        char_statistical_interaction = char_subset * statistical_subset  # å­—ç¬¦-ç»Ÿè®¡äº¤äº’
        semantic_statistical_interaction = semantic_subset * statistical_subset  # è¯­ä¹‰-ç»Ÿè®¡äº¤äº’
        
        # æ‹¼æ¥äº¤äº’ç‰¹å¾
        interaction_features = np.concatenate([
            char_semantic_interaction,
            char_statistical_interaction,
            semantic_statistical_interaction
        ], axis=1)
        
        return interaction_features
        
    def hierarchical_fusion(self, char_features: np.ndarray, 
                           semantic_features: np.ndarray, 
                           statistical_features: np.ndarray,
                           complexity_features: np.ndarray = None,
                           original_texts: List[str] = None) -> np.ndarray:
        """åˆ†å±‚ç‰¹å¾èåˆ"""
        # æ£€æŸ¥é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨å¥å‘é‡
        enhanced_config = self.config.get('advanced_feature', {}).get('enhanced', {})
        use_sentence_embedding = enhanced_config.get('use_sentence_embedding', False)
        
        if use_sentence_embedding and original_texts:
            # æ‰¹é‡å¥å‘é‡èåˆè·¯å¾„
            try:
                # è·å–æ‰¹é‡å¥å‘é‡
                sentence_embeddings = self.get_sentence_embedding(original_texts)
                
                # è°ƒç”¨æ‰¹é‡å¥å‘é‡èåˆæ–¹æ³• - ä¼ é€’å¤æ‚åº¦ç‰¹å¾
                return self._sentence_embedding_fusion(
                    char_features, semantic_features, statistical_features, sentence_embeddings, complexity_features, original_texts
                )
            except Exception as e:
                logger.warning(f"æ‰¹é‡å¥å‘é‡èåˆå¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰é€»è¾‘: {e}")
                # å›é€€åˆ°åŸæœ‰é€»è¾‘ - ä¼ é€’å¤æ‚åº¦ç‰¹å¾
                return self._original_hierarchical_fusion(
                    char_features, semantic_features, statistical_features, complexity_features, original_texts
                )
        else:
            # åŸæœ‰èåˆè·¯å¾„ - ä¿æŒç°æœ‰é€»è¾‘ä¸å˜
            return self._original_hierarchical_fusion(
                char_features, semantic_features, statistical_features, complexity_features, original_texts
            )
    
    def _original_hierarchical_fusion(self, char_features: np.ndarray, 
                                     semantic_features: np.ndarray, 
                                     statistical_features: np.ndarray,
                                     complexity_features: np.ndarray = None,
                                     original_texts: List[str] = None) -> np.ndarray:
        """åŸæœ‰çš„åˆ†å±‚èåˆé€»è¾‘"""
        # è·å–pa_feature_weightsé…ç½®
        pa_weights = self.config.get('advanced_feature', {}).get('pa_feature_weights', {})
        char_weight = pa_weights.get('char_weight', 1.0)
        semantic_weight = pa_weights.get('semantic_weight', 1.0)
        statistical_weight = pa_weights.get('statistical_weight', 1.0)
        complexity_weight = pa_weights.get('complexity_weight', 1.0)
        
        # åº”ç”¨æ¶æ„è¯æƒé‡ï¼ˆå¦‚æœæä¾›äº†åŸå§‹æ–‡æœ¬ï¼‰
        malicious_coefficient = 1.0
        if original_texts and len(original_texts) > 0:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬ä½œä¸ºä»£è¡¨ï¼ˆæ‰¹é‡å¤„ç†æ—¶é€šå¸¸æ‰€æœ‰æ–‡æœ¬éƒ½æ˜¯åŒä¸€ç±»å‹ï¼‰
            malicious_coefficient = self._get_malicious_adjustment_coefficient(original_texts[0])
            if malicious_coefficient < 1.0:
                logger.info(f"æ£€æµ‹åˆ°æ¶æ„è¯ï¼Œåº”ç”¨æ¶æ„è¯æƒé‡: {malicious_coefficient}")
        
        # è®°å½•æƒé‡åº”ç”¨
        logger.info(f"åº”ç”¨pa_featureæƒé‡: char={char_weight}, semantic={semantic_weight}, statistical={statistical_weight}, complexity={complexity_weight}, malicious={malicious_coefficient}")
        
        # åº”ç”¨æƒé‡ï¼ˆåŒ…æ‹¬æ¶æ„è¯æƒé‡ï¼‰
        weighted_char_features = char_features * char_weight * malicious_coefficient
        weighted_semantic_features = semantic_features * semantic_weight * malicious_coefficient
        weighted_statistical_features = statistical_features * statistical_weight * malicious_coefficient
        
        # ç¬¬ä¸€å±‚ï¼šè¯­ä¹‰ç‰¹å¾ä¸ç»Ÿè®¡ç‰¹å¾èåˆ
        semantic_statistical = np.concatenate([weighted_semantic_features, weighted_statistical_features], axis=1)
        
        # ç¬¬äºŒå±‚ï¼šä¸å­—ç¬¦ç‰¹å¾èåˆ
        final_features = np.concatenate([weighted_char_features, semantic_statistical], axis=1)
        
        # ç¬¬ä¸‰å±‚ï¼šä¸å¥å­å¤æ‚åº¦ç‰¹å¾èåˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if complexity_features is not None and complexity_features.shape[1] > 0:
            weighted_complexity_features = complexity_features * complexity_weight * malicious_coefficient
            final_features = np.concatenate([final_features, weighted_complexity_features], axis=1)
        
        return final_features
        
    def _extract_semantic_features(self, texts: List[str]) -> np.ndarray:
        """æå–è¯­ä¹‰ç‰¹å¾"""
        features = []
        for text in texts:
            text_features = []
            
            # ä¸»è¦å…³é”®è¯åŒ¹é…
            for intent, keywords in self.primary_keywords.items():
                primary_score = self.position_weighted_matching(text, keywords)
                text_features.append(primary_score)
            
            # æ¬¡è¦å…³é”®è¯åŒ¹é…
            for intent, keywords in self.secondary_keywords.items():
                secondary_score = self.context_aware_matching(text, keywords)
                text_features.append(secondary_score)
            
            # å¦å®šå…³é”®è¯åŒ¹é…
            for intent, keywords in self.negative_keywords.items():
                negative_score = self.fuzzy_keyword_matching(text, keywords)
                text_features.append(negative_score)
            
            # è·¨å­—ç¬¦æ¨¡å¼åŒ¹é…
            for intent, keywords in self.primary_keywords.items():
                cross_char_score = self.cross_char_pattern_matching(text, keywords)
                text_features.append(cross_char_score)
            
            features.append(text_features)
        
        return np.array(features)
        
    def _extract_statistical_features(self, texts: List[str]) -> np.ndarray:
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        features = []
        for text in texts:
            text_features = self.extract_structural_features(text)
            features.append(text_features)
        return np.array(features)
        
    def _extract_complexity_features(self, texts: List[str]) -> np.ndarray:
        """
        æå–å¥å­å¤æ‚åº¦ç‰¹å¾ï¼ˆç‹¬ç«‹ç‰¹å¾å±‚ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å¥å­å¤æ‚åº¦ç‰¹å¾çŸ©é˜µ
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¥å­å¤æ‚åº¦ç‰¹å¾
        complexity_config = self.config.get('advanced_feature', {}).get('complexity_feature', {})
        enabled = complexity_config.get('enabled', True)  # é»˜è®¤å¯ç”¨
        
        if not enabled:
            return np.array([]).reshape(len(texts), 0)
        
        features = []
        for text in texts:
            complexity = self.complexity_calculator.calculate(text)
            features.append([complexity])
        
        return np.array(features)

    def fit(self, texts: List[str], labels: List[str] = None):
        """è®­ç»ƒç‰¹å¾æå–å™¨"""
        # åˆå§‹åŒ–TF-IDFå‘é‡å™¨
        tfidf_config = self.config.get('advanced_feature', {}).get('tfidf', {})
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=tfidf_config.get('max_features', 1000),
            ngram_range=tuple(tfidf_config.get('ngram_range', [1, 2])),
            min_df=tfidf_config.get('min_df', 1),
            max_df=tfidf_config.get('max_df', 0.8),
            sublinear_tf=True,
            norm='l2'
        )
        
        # è®­ç»ƒTF-IDF
        self.tfidf_vectorizer.fit(texts)
        logger.info("å¢å¼ºç‰¹å¾æå–å™¨è®­ç»ƒå®Œæˆ")
        
    def transform(self, texts: List[str]) -> np.ndarray:
        """è½¬æ¢æ–‡æœ¬ä¸ºç‰¹å¾å‘é‡"""
        if self.tfidf_vectorizer is None:
            raise ValueError("ç‰¹å¾æå–å™¨å°šæœªè®­ç»ƒ")
        
        # æå–TF-IDFç‰¹å¾
        char_features = self.tfidf_vectorizer.transform(texts).toarray()
        
        # æå–è¯­ä¹‰ç‰¹å¾
        semantic_features = self._extract_semantic_features(texts)
        
        # æå–ç»Ÿè®¡ç‰¹å¾
        statistical_features = self._extract_statistical_features(texts)
        
        # æå–å¥å­å¤æ‚åº¦ç‰¹å¾
        complexity_features = self._extract_complexity_features(texts)
        
        # ç‰¹å¾èåˆ - æ€»æ˜¯ä¼ é€’åŸå§‹æ–‡æœ¬ä»¥ç¡®ä¿æ¶æ„è¯æƒé‡èƒ½å¤Ÿåº”ç”¨
        enhanced_config = self.config.get('advanced_feature', {}).get('enhanced', {})
        use_sentence_embedding = enhanced_config.get('use_sentence_embedding', False)
        
        if use_sentence_embedding and texts:
            fused_features = self.hierarchical_fusion(
                char_features, semantic_features, statistical_features, complexity_features,
                original_texts=texts
            )
        else:
            # å³ä½¿æ²¡æœ‰å¯ç”¨å¥å‘é‡ï¼Œä¹Ÿè¦ä¼ é€’original_textsä»¥ç¡®ä¿æ¶æ„è¯æƒé‡èƒ½å¤Ÿåº”ç”¨
            fused_features = self.hierarchical_fusion(
                char_features, semantic_features, statistical_features, complexity_features,
                original_texts=texts
            )
        
        return fused_features
    
    def get_sentence_embedding(self, sentences: List[str]) -> np.ndarray:
        """è·å–æ‰¹é‡å¥å‘é‡ç‰¹å¾ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½æ¨¡å‹ï¼‰"""
        logger.info(f"ğŸš€ pa_feature: get_sentence_embedding è¢«è°ƒç”¨ï¼Œè¾“å…¥å¥å­æ•°: {len(sentences)}")
        
        # å¤„ç†ç©ºåˆ—è¡¨
        if len(sentences) == 0:
            logger.info("ğŸ“ pa_feature: è¾“å…¥å¥å­åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºçŸ©é˜µ")
            return np.zeros((0, 768))
        
        # è·å–æ¨¡å‹ï¼ˆé¢„åŠ è½½ä¼˜å…ˆï¼Œæ‡’åŠ è½½å…œåº•ï¼‰
        logger.info("ğŸ” pa_feature: å¼€å§‹è·å–æ¨¡å‹...")
        model = self._get_model()
        if model is None:
            logger.warning("âš ï¸ pa_feature: æ— æ³•è·å–æ¨¡å‹ï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros((len(sentences), 768))
        
        # ç»Ÿä¸€æå–å¥å‘é‡
        logger.info("ğŸ” pa_feature: å¼€å§‹æå–å¥å‘é‡...")
        return self._extract_embeddings(model, sentences)
    
    def _get_model(self):
        """ç»Ÿä¸€è·å– SentenceTransformer æ¨¡å‹"""
        # 1. å°è¯•é¢„åŠ è½½æ¨¡å‹
        try:
            if ModelManager is not None:
                logger.info("ğŸ” pa_feature: å¼€å§‹æ£€æŸ¥é¢„åŠ è½½æ¨¡å‹...")
                model_manager = ModelManager()
                logger.info(f"ğŸ” pa_feature: ModelManagerå®ä¾‹ID: {id(model_manager)}")
                
                # æ£€æŸ¥å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
                loaded_models = model_manager.get_loaded_models()
                logger.info(f"ğŸ” pa_feature: ModelManagerå·²åŠ è½½æ¨¡å‹: {loaded_models}")
                
                preloaded_model = model_manager.get_sentence_transformer()
                if preloaded_model is not None:
                    logger.info("âœ… pa_feature: æˆåŠŸä»å†…å­˜è·å–é¢„åŠ è½½çš„ SentenceTransformer æ¨¡å‹")
                    logger.info(f"ğŸ” pa_feature: é¢„åŠ è½½æ¨¡å‹å¯¹è±¡ID: {id(preloaded_model)}")
                    return preloaded_model
                else:
                    logger.warning("âš ï¸ pa_feature: ModelManagerä¸­æ²¡æœ‰é¢„åŠ è½½çš„æ¨¡å‹")
            else:
                logger.warning("âš ï¸ pa_feature: ModelManagerä¸ºNoneï¼Œæ— æ³•ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹")
        except Exception as e:
            logger.warning(f"âŒ pa_feature: ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ‡’åŠ è½½æ¨¡å¼")
        
        # 2. å›é€€åˆ°æ‡’åŠ è½½æ¨¡å‹
        if hasattr(self, 'sentence_model') and self.sentence_model is not None:
            logger.info("ğŸ”„ pa_feature: ä½¿ç”¨å·²ç¼“å­˜çš„æ‡’åŠ è½½æ¨¡å‹")
            return self.sentence_model
        
        # 3. æ‡’åŠ è½½æ¨¡å‹ï¼ˆä¿æŒåŸæœ‰çš„å¤æ‚é€»è¾‘ä½œä¸ºå…œåº•ï¼‰
        if not self._model_loaded:
            logger.info("ğŸŒ pa_feature: å¼€å§‹ä»ç½‘ç»œæ‡’åŠ è½½æ¨¡å‹...")
            with self._model_lock:
                # åŒé‡æ£€æŸ¥
                if not self._model_loaded:
                    try:
                        # å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå†å¯¼å…¥SentenceTransformer
                        enhanced_config = self.config.get('advanced_feature', {}).get('enhanced', {})
                        sentence_model = enhanced_config.get('sentence_model', 'shibing624/text2vec-base-chinese')
                        
                        # æ£€æŸ¥sentence_modelæ˜¯å¦ä¸ºNone
                        if sentence_model is None:
                            logger.error("sentence_model is None, using default")
                            sentence_model = 'shibing624/text2vec-base-chinese'

                        logger.info(f"ğŸŒ pa_feature: å‡†å¤‡ä»ç½‘ç»œåŠ è½½æ¨¡å‹: {sentence_model}")

                        # ä»é…ç½®ä¸­è·å–ç½‘ç»œè®¾ç½®
                        network_timeout = enhanced_config.get('network_timeout', 30)
                        allow_offline = enhanced_config.get('allow_offline', True)
                        fallback_to_zero = enhanced_config.get('fallback_to_zero', True)
                        force_offline = enhanced_config.get('force_offline', False)

                        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨å¯¼å…¥SentenceTransformerä¹‹å‰ï¼‰
                        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = str(network_timeout)
                        # ä½¿ç”¨ä¸­å›½ HuggingFace é•œåƒç«™ç‚¹è§£å†³ SSL é—®é¢˜
                        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
                        
                        if force_offline or allow_offline:
                            # è®¾ç½®æ‰€æœ‰ç›¸å…³çš„ç¦»çº¿ç¯å¢ƒå˜é‡
                            os.environ['HF_HUB_OFFLINE'] = '1'
                            os.environ['TRANSFORMERS_OFFLINE'] = '1'
                            os.environ['HF_DATASETS_OFFLINE'] = '1'
                            os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
                            os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
                            os.environ['HF_HUB_DISABLE_EXPERIMENTAL_WARNING'] = '1'
                            logger.info("ğŸŒ pa_feature: è®¾ç½®ç¦»çº¿æ¨¡å¼: HF_HUB_OFFLINE=1, TRANSFORMERS_OFFLINE=1, HF_DATASETS_OFFLINE=1")
                        else:
                            os.environ['HF_HUB_OFFLINE'] = '0'
                            logger.info("ğŸŒ pa_feature: è®¾ç½®åœ¨çº¿æ¨¡å¼: HF_HUB_OFFLINE=0, ä½¿ç”¨ä¸­å›½é•œåƒç«™ç‚¹: https://dl.aifasthub.com")
                        
                        # å°è¯•åŠ è½½æ¨¡å‹
                        try:
                            logger.info("ğŸŒ pa_feature: å¼€å§‹ä»ç½‘ç»œä¸‹è½½å¹¶åŠ è½½æ¨¡å‹...")
                            # ç›´æ¥ä½¿ç”¨æ¨¡å‹ååŠ è½½ï¼ˆç°åœ¨ä½¿ç”¨é•œåƒç«™ç‚¹ï¼‰
                            self.sentence_model = SentenceTransformer(sentence_model)
                            self._model_loaded = True
                            logger.info("ğŸŒ pa_feature: ç½‘ç»œæ‡’åŠ è½½æ¨¡å¼ï¼šå¥å‘é‡æ¨¡å‹åŠ è½½æˆåŠŸ")
                            logger.info(f"ğŸ” pa_feature: æ‡’åŠ è½½æ¨¡å‹å¯¹è±¡ID: {id(self.sentence_model)}")
                            return self.sentence_model
                        except Exception as model_error:
                            logger.error(f"ğŸŒ pa_feature: ç½‘ç»œæ‡’åŠ è½½æ¨¡å¼ï¼šSentenceTransformeråŠ è½½å¤±è´¥: {model_error}")
                            if fallback_to_zero:
                                logger.warning("å°†ä½¿ç”¨é›¶å‘é‡ä»£æ›¿å¥å‘é‡ç‰¹å¾")
                                self.sentence_model = None
                                self._model_loaded = True
                                return None
                            else:
                                raise model_error

                    except ImportError:
                        logger.warning("sentence-transformersæœªå®‰è£…ï¼Œä½¿ç”¨é›¶å‘é‡ä»£æ›¿")
                        self.sentence_model = None
                        self._model_loaded = True  # æ ‡è®°ä¸ºå·²å¤„ç†
                        return None
                    except Exception as e:
                        logger.error(f"ğŸŒ pa_feature: ç½‘ç»œæ‡’åŠ è½½æ¨¡å¼ï¼šå¥å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        if fallback_to_zero:
                            logger.warning("å°†ä½¿ç”¨é›¶å‘é‡ä»£æ›¿å¥å‘é‡ç‰¹å¾")
                            self.sentence_model = None
                            self._model_loaded = True  # æ ‡è®°ä¸ºå·²å¤„ç†
                            return None
                        else:
                            # å¦‚æœä¸å…è®¸å›é€€ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                            raise e
        
        return self.sentence_model
    
    def _extract_embeddings(self, model, sentences: List[str]) -> np.ndarray:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹æå–å¥å‘é‡"""
        try:
            logger.info(f"ğŸ” pa_feature: å¼€å§‹æå–æ‰¹é‡å¥å‘é‡ç‰¹å¾ï¼Œæ ·æœ¬æ•°: {len(sentences)}")
            logger.info(f"ğŸ” pa_feature: ä½¿ç”¨æ¨¡å‹å¯¹è±¡ID: {id(model)}")
            embeddings = model.encode(sentences, batch_size=64, show_progress_bar=True)
            logger.info(f"âœ… pa_feature: å¥å‘é‡æå–æˆåŠŸï¼Œç»´åº¦: {embeddings.shape}")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ pa_feature: å¥å‘é‡æå–å¤±è´¥: {e}")
            return np.zeros((len(sentences), 768))
    
    def get_enhanced_intent_features(self, sentence: str) -> np.ndarray:
        """è·å–å¢å¼ºçš„æ„å›¾ç‰¹å¾ï¼ˆåŒ…å«å¥å‘é‡ï¼‰"""
        try:
            # æå–TF-IDFç‰¹å¾
            char_features = self.tfidf_vectorizer.transform([sentence]).toarray()
            
            # æå–è¯­ä¹‰ç‰¹å¾
            semantic_features = self._extract_semantic_features([sentence])
            
            # æå–ç»Ÿè®¡ç‰¹å¾
            statistical_features = self._extract_statistical_features([sentence])
            
            # æå–å¥å­å¤æ‚åº¦ç‰¹å¾
            complexity_features = self._extract_complexity_features([sentence])
            
            # ç‰¹å¾èåˆ - ä¼ é€’åŸå§‹æ–‡æœ¬å’Œå¤æ‚åº¦ç‰¹å¾
            fused_features = self.hierarchical_fusion(
                char_features, semantic_features, statistical_features, complexity_features, original_texts=[sentence]
            )
            
            return fused_features
        except Exception as e:
            logger.error(f"å¢å¼ºæ„å›¾ç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€ç‰¹å¾ä½œä¸ºå…œåº•
            return self.tfidf_vectorizer.transform([sentence]).toarray()
    
    def _sentence_embedding_fusion(self, char_features: np.ndarray,
                                  semantic_features: np.ndarray,
                                  statistical_features: np.ndarray,
                                  sentence_embeddings: np.ndarray,
                                  complexity_features: np.ndarray = None,
                                  original_texts: List[str] = None) -> np.ndarray:
        """æ‰¹é‡å¥å‘é‡èåˆæ–¹æ³•"""
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å¥å‘é‡èåˆï¼Œç‰¹å¾ç»´åº¦: char={char_features.shape}, semantic={semantic_features.shape}, statistical={statistical_features.shape}, complexity={complexity_features.shape if complexity_features is not None else 'None'}")
            logger.info(f"æ‰¹é‡å¥å‘é‡ç»´åº¦: {sentence_embeddings.shape}")
            
            # è·å–pa_feature_weightsé…ç½®
            pa_weights = self.config.get('advanced_feature', {}).get('pa_feature_weights', {})
            sentence_weight = pa_weights.get('sentence_weight', 0.3)
            
            logger.info(f"pa_featureå¥å‘é‡èåˆæƒé‡: sentence_weight={sentence_weight}")
            
            # åŸæœ‰ç‰¹å¾èåˆ - ä¼ é€’original_textsä»¥ç¡®ä¿æ¶æ„è¯æƒé‡èƒ½å¤Ÿåº”ç”¨
            original_features = self._original_hierarchical_fusion(
                char_features, semantic_features, statistical_features, complexity_features, original_texts
            )
            logger.info(f"åŸæœ‰ç‰¹å¾èåˆå®Œæˆï¼Œç»´åº¦: {original_features.shape}")
            
            # å¯¹å¥å‘é‡åº”ç”¨æƒé‡
            weighted_sentence = sentence_embeddings * sentence_weight
            
            logger.info(f"åŠ æƒèåˆ: original={original_features.shape}, sentence={weighted_sentence.shape}")
            
            # æ‹¼æ¥åŠ æƒç‰¹å¾
            combined_features = np.concatenate([original_features, weighted_sentence], axis=1)
            logger.info(f"æ‰¹é‡å¥å‘é‡èåˆæˆåŠŸï¼Œæœ€ç»ˆç»´åº¦: {combined_features.shape}")
            
            return combined_features
        except Exception as e:
            logger.error(f"æ‰¹é‡å¥å‘é‡èåˆå¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰èåˆ - ä¼ é€’original_textsä»¥ç¡®ä¿æ¶æ„è¯æƒé‡èƒ½å¤Ÿåº”ç”¨
            return self._original_hierarchical_fusion(
                char_features, semantic_features, statistical_features, complexity_features, original_texts
            )
        
    def save(self, filepath: str):
        """ä¿å­˜ç‰¹å¾æå–å™¨"""
        save_data = {
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'primary_keywords': self.primary_keywords,
            'secondary_keywords': self.secondary_keywords,
            'negative_keywords': self.negative_keywords,
            'config': self.config
        }
        joblib.dump(save_data, filepath)
        logger.info(f"å¢å¼ºç‰¹å¾æå–å™¨å·²ä¿å­˜åˆ°: {filepath}")
        
    def load(self, filepath: str):
        """åŠ è½½ç‰¹å¾æå–å™¨"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        save_data = joblib.load(filepath)
        self.tfidf_vectorizer = save_data['tfidf_vectorizer']
        self.primary_keywords = save_data['primary_keywords']
        self.secondary_keywords = save_data['secondary_keywords']
        self.negative_keywords = save_data['negative_keywords']
        self.config = save_data['config']
        logger.info(f"å¢å¼ºç‰¹å¾æå–å™¨å·²ä» {filepath} åŠ è½½")
    
    def save_enhanced_feature_extractor(self, path: str):
        """ä¿å­˜å¢å¼ºçš„ç‰¹å¾æå–å™¨"""
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            
            # ä¿å­˜æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼ŒåŒ…æ‹¬æ¶æ„è¯ç›¸å…³çš„æ–¹æ³•
            save_data = {
                'tfidf_vectorizer': self.tfidf_vectorizer,
                'primary_keywords': self.primary_keywords,
                'secondary_keywords': self.secondary_keywords,
                'negative_keywords': self.negative_keywords,
                'config': self.config,
                # æ·»åŠ æ¶æ„è¯ç›¸å…³çš„æ–¹æ³•
                'malicious_patterns': self._init_malicious_patterns(),
                'helper_patterns': self._init_helper_patterns()
            }
            
            joblib.dump(save_data, path)
            logger.info(f"å¢å¼ºç‰¹å¾æå–å™¨å·²ä¿å­˜åˆ°: {path}")
            
        except Exception as e:
            logger.error(f"å¢å¼ºç‰¹å¾æå–å™¨ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def load_enhanced_feature_extractor(self, path: str):
        """åŠ è½½å¢å¼ºçš„ç‰¹å¾æå–å™¨"""
        try:
            if not os.path.exists(path):
                raise FileNotFoundError(f"å¢å¼ºç‰¹å¾æå–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            
            # åŠ è½½æ‰€æœ‰ç»„ä»¶
            save_data = joblib.load(path)
            self.tfidf_vectorizer = save_data['tfidf_vectorizer']
            self.primary_keywords = save_data['primary_keywords']
            self.secondary_keywords = save_data['secondary_keywords']
            self.negative_keywords = save_data['negative_keywords']
            self.config = save_data['config']
            
            # åŠ è½½æ¶æ„è¯ç›¸å…³çš„æ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'malicious_patterns' in save_data:
                self.malicious_patterns = save_data['malicious_patterns']
            else:
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œé‡æ–°åˆå§‹åŒ–æ¶æ„è¯æ¨¡å¼
                self.malicious_patterns = self._init_malicious_patterns()
                
            if 'helper_patterns' in save_data:
                self.helper_patterns = save_data['helper_patterns']
            else:
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œé‡æ–°åˆå§‹åŒ–è¾…åŠ©è¯æ¨¡å¼
                self.helper_patterns = self._init_helper_patterns()
            
            logger.info(f"å¢å¼ºç‰¹å¾æå–å™¨å·²ä» {path} åŠ è½½")
            
        except Exception as e:
            logger.error(f"å¢å¼ºç‰¹å¾æå–å™¨åŠ è½½å¤±è´¥: {e}")
            raise
    
    def fit_transform(self, queries: List[str]) -> np.ndarray:
        """
        è®­ç»ƒå¹¶æå–ç‰¹å¾ï¼ˆåŒ…è£…ç°æœ‰æ–¹æ³•ï¼‰
        
        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            ç‰¹å¾çŸ©é˜µ
        """
        self.fit(queries)
        return self.transform(queries)
    
    def get_feature_dim(self) -> int:
        """
        è·å–ç‰¹å¾ç»´åº¦ï¼ˆåŒ…è£…ç°æœ‰æ–¹æ³•ï¼‰
        
        Returns:
            ç‰¹å¾ç»´åº¦
        """
        if self.tfidf_vectorizer is None:
            raise ValueError("ç‰¹å¾æå–å™¨å°šæœªè®­ç»ƒ")
        
        # ä½¿ç”¨æµ‹è¯•æ•°æ®è®¡ç®—ç‰¹å¾ç»´åº¦
        test_query = ["æµ‹è¯•æŸ¥è¯¢"]
        features = self.transform(test_query)
        return features.shape[1]


    def _init_evilgood_feature_from_config(self):
        """ä»é…ç½®æ–‡ä»¶è¯»å–evilgood_featureé…ç½®"""
        try:
            evilgood_config = self.config.get('advanced_feature', {}).get('evilgood_feature', {})
            
            # è¯»å–è°ƒæ•´ç³»æ•°
            self.malicious_adjustment_coefficient = evilgood_config.get('malicious_weight', 0.1)
            self.helper_adjustment_coefficient = evilgood_config.get('helper_weight', 0.3)
            
            # è¯»å–æ¨¡å¼åˆ—è¡¨
            self.malicious_patterns = evilgood_config.get('malicious_patterns', self._init_malicious_patterns())
            self.helper_patterns = evilgood_config.get('helper_patterns', self._init_helper_patterns())
            
            logger.info(f"æˆåŠŸè¯»å–evilgood_featureé…ç½®: malicious_weight={self.malicious_adjustment_coefficient}, helper_weight={self.helper_adjustment_coefficient}")
            logger.info(f"æ¶æ„è¯æ¨¡å¼æ•°é‡: {len(self.malicious_patterns)}, è¾…åŠ©è¯æ¨¡å¼æ•°é‡: {len(self.helper_patterns)}")
            
        except Exception as e:
            logger.warning(f"è¯»å–evilgood_featureé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼
            self.malicious_adjustment_coefficient = 0.1
            self.helper_adjustment_coefficient = 0.3
            self.malicious_patterns = self._init_malicious_patterns()
            self.helper_patterns = self._init_helper_patterns()
        
    def _get_malicious_adjustment_coefficient(self, text: str) -> float:
        """ä»é…ç½®æ–‡ä»¶è·å–æ¶æ„è¯è°ƒæ•´ç³»æ•°"""
        # ç¡®ä¿å±æ€§å·²åˆå§‹åŒ–
        if not hasattr(self, 'malicious_adjustment_coefficient'):
            self._init_evilgood_feature_from_config()
        
        if hasattr(self, 'malicious_patterns') and self.malicious_patterns:
            for pattern in self.malicious_patterns:
                if pattern in text:
                    logger.info(f"æ£€æµ‹åˆ°æ¶æ„è¯æ¨¡å¼: {pattern}, åº”ç”¨æƒé‡: {self.malicious_adjustment_coefficient}")
                    return self.malicious_adjustment_coefficient
        return 1.0
        
    def _get_helper_adjustment_coefficient(self, text: str, keyword: str, pos: int) -> float:
        """ä»é…ç½®æ–‡ä»¶è·å–è¾…åŠ©è¯è°ƒæ•´ç³»æ•°"""
        # ç¡®ä¿å±æ€§å·²åˆå§‹åŒ–
        if not hasattr(self, 'helper_adjustment_coefficient'):
            self._init_evilgood_feature_from_config()
        
        if hasattr(self, 'helper_patterns') and self.helper_patterns:
            # æ£€æŸ¥å…³é”®è¯å‰åçš„ä¸Šä¸‹æ–‡
            before = text[max(0, pos-8):pos]
            after = text[pos+len(keyword):pos+len(keyword)+8]
            
            # å¦‚æœè¢«è¾…åŠ©è¯æ±‡åŒ…å›´ï¼Œé™ä½æƒé‡
            is_helper_context = any(pattern in before for pattern in self.helper_patterns)
            
            if is_helper_context:
                logger.info(f"æ£€æµ‹åˆ°è¾…åŠ©è¯ä¸Šä¸‹æ–‡ï¼Œåº”ç”¨æƒé‡: {self.helper_adjustment_coefficient}")
                return self.helper_adjustment_coefficient
        return 1.0




class FeatureInteractionFusion:
    """ç‰¹å¾äº¤äº’èåˆå™¨"""
    
    def __init__(self, fusion_method: str = 'interaction'):
        """
        åˆå§‹åŒ–ç‰¹å¾èåˆå™¨
        
        Args:
            fusion_method: èåˆæ–¹æ³• ('interaction', 'attention', 'hierarchical')
        """
        self.fusion_method = fusion_method
        
    def feature_interaction_fusion(self, char_features: np.ndarray, 
                                 semantic_features: np.ndarray, 
                                 statistical_features: np.ndarray) -> np.ndarray:
        """ç‰¹å¾äº¤äº’èåˆ"""
        # åŸºç¡€æ‹¼æ¥
        basic_fusion = np.concatenate([char_features, semantic_features, statistical_features], axis=1)
        
        # è·å–æœ€å°ç»´åº¦è¿›è¡Œäº¤äº’
        min_dim = min(char_features.shape[1], semantic_features.shape[1], statistical_features.shape[1])
        
        # æˆªå–åˆ°ç›¸åŒç»´åº¦è¿›è¡Œäº¤äº’
        char_subset = char_features[:, :min_dim]
        semantic_subset = semantic_features[:, :min_dim]
        statistical_subset = statistical_features[:, :min_dim]
        
        # ç‰¹å¾äº¤äº’
        char_semantic_interaction = char_subset * semantic_subset  # å­—ç¬¦-è¯­ä¹‰äº¤äº’
        char_statistical_interaction = char_subset * statistical_subset  # å­—ç¬¦-ç»Ÿè®¡äº¤äº’
        semantic_statistical_interaction = semantic_subset * statistical_subset  # è¯­ä¹‰-ç»Ÿè®¡äº¤äº’
        
        # æœ€ç»ˆèåˆ
        final_features = np.concatenate([
            basic_fusion,
            char_semantic_interaction,
            char_statistical_interaction,
            semantic_statistical_interaction
        ], axis=1)
        
        return final_features
        
    def attention_fusion(self, char_features: np.ndarray, 
                        semantic_features: np.ndarray, 
                        statistical_features: np.ndarray) -> np.ndarray:
        """æ³¨æ„åŠ›æœºåˆ¶èåˆ"""
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        char_attention = np.mean(char_features, axis=1, keepdims=True)
        semantic_attention = np.mean(semantic_features, axis=1, keepdims=True)
        statistical_attention = np.mean(statistical_features, axis=1, keepdims=True)
        
        # å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡
        total_attention = char_attention + semantic_attention + statistical_attention
        char_weight = char_attention / (total_attention + 1e-8)
        semantic_weight = semantic_attention / (total_attention + 1e-8)
        statistical_weight = statistical_attention / (total_attention + 1e-8)
        
        # è·å–æœ€å°ç»´åº¦è¿›è¡ŒåŠ æƒèåˆ
        min_dim = min(char_features.shape[1], semantic_features.shape[1], statistical_features.shape[1])
        
        # æˆªå–åˆ°ç›¸åŒç»´åº¦è¿›è¡ŒåŠ æƒèåˆ
        char_subset = char_features[:, :min_dim]
        semantic_subset = semantic_features[:, :min_dim]
        statistical_subset = statistical_features[:, :min_dim]
        
        # åŠ æƒèåˆ
        fused_features = (char_subset * char_weight + 
                         semantic_subset * semantic_weight + 
                         statistical_subset * statistical_weight)
        
        return fused_features
        
    def hierarchical_fusion(self, char_features: np.ndarray, 
                           semantic_features: np.ndarray, 
                           statistical_features: np.ndarray) -> np.ndarray:
        """åˆ†å±‚ç‰¹å¾èåˆ"""
        # ç¬¬ä¸€å±‚ï¼šè¯­ä¹‰ç‰¹å¾ä¸ç»Ÿè®¡ç‰¹å¾èåˆ
        semantic_statistical = np.concatenate([semantic_features, statistical_features], axis=1)
        
        # ç¬¬äºŒå±‚ï¼šä¸å­—ç¬¦ç‰¹å¾èåˆ
        final_features = np.concatenate([char_features, semantic_statistical], axis=1)
        
        return final_features


class StructuralFeatureExtractor:
    """ç»“æ„ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»“æ„ç‰¹å¾æå–å™¨"""
        self.question_words = ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'æ€ä¹ˆ', 'å“ªäº›']
        self.quantity_words = ['å“ªäº›', 'å¤šå°‘ä¸ª', 'å¤šå°‘ç§', 'å‡ ä¸ª', 'å¤šå°‘', 'å…¨éƒ¨', 'æ‰€æœ‰']
        self.comparison_words = ['å“ªä¸ª', 'ç›¸æ¯”', 'æ¯”è¾ƒ', 'æ›´', 'æœ€é«˜', 'æœ€ä½', 'æœ€è¿œ', 'æœ€è¿‘', 'æœ€é•¿', 'æœ€çŸ­']
        
    def extract_question_word_features(self, text: str) -> np.ndarray:
        """æå–ç–‘é—®è¯ç‰¹å¾"""
        features = []
        for word in self.question_words:
            count = text.count(word)
            features.append(count)
        return np.array(features)
        
    def extract_quantity_word_features(self, text: str) -> np.ndarray:
        """æå–æ•°é‡è¯ç‰¹å¾"""
        features = []
        for word in self.quantity_words:
            count = text.count(word)
            features.append(count)
        return np.array(features)
        
    def extract_comparison_word_features(self, text: str) -> np.ndarray:
        """æå–æ¯”è¾ƒè¯ç‰¹å¾"""
        features = []
        for word in self.comparison_words:
            count = text.count(word)
            features.append(count)
        return np.array(features)
        
    def extract_sentence_structure_features(self, text: str) -> np.ndarray:
        """æå–å¥å­ç»“æ„ç‰¹å¾"""
        features = []
        
        # å¥å­é•¿åº¦
        features.append(len(text))
        
        # è¯æ•°
        word_count = len(list(jieba.cut(text)))
        features.append(word_count)
        
        # æ ‡ç‚¹ç¬¦å·æ•°
        punctuation_count = sum(1 for char in text if char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘')
        features.append(punctuation_count)
        
        # ç–‘é—®è¯ä½ç½®ï¼ˆå¥é¦–/å¥ä¸­/å¥å°¾ï¼‰
        question_positions = []
        for word in self.question_words:
            if word in text:
                pos = text.find(word)
                if pos < len(text) * 0.3:
                    question_positions.append(1)  # å¥é¦–
                elif pos > len(text) * 0.7:
                    question_positions.append(3)  # å¥å°¾
                else:
                    question_positions.append(2)  # å¥ä¸­
            else:
                question_positions.append(0)  # æ²¡æœ‰
        
        features.extend(question_positions)
        
        return np.array(features) 