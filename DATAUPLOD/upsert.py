import os
import sys
import argparse
import logging
from typing import List, Dict, Any
import subprocess
from DATAUPLOD.bm25_manager import BM25Manager

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data_vector_v3 import SemanticDocumentChunker, save_chunks_to_text, save_chunks_to_json
from data_vector_v2 import DocumentChunker
from knowledge_graph_manager import KnowledgeGraphManager
from config import get_pinecone_index, get_embedding_model

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class UpsertManager:
    """
    æ•°æ®å…¥åº“æµç¨‹ç®¡ç†å™¨ (Refactored)
    
    èŒè´£ï¼š
    1. è°ƒç”¨æŒ‡å®šç‰ˆæœ¬çš„åˆ‡ç‰‡å™¨è¿›è¡Œæ–‡æ¡£åˆ‡ç‰‡ã€‚
    2. è°ƒç”¨ knowledge_graph_manager å°†å®ä½“å’Œå…³ç³»å†™å…¥Neo4jã€‚
    3. è°ƒç”¨ embedding_model è¿›è¡Œå‘é‡åŒ–ã€‚
    4. å°†å‘é‡åŒ–åçš„æ•°æ®ä¸Šä¼ åˆ° Pineconeã€‚
    5. ç»Ÿä¸€è°ƒåº¦ã€é…ç½®å’Œæ—¥å¿—è®°å½•ã€‚
    """
    def __init__(self, environment: str = "production", chunker_version: str = "v3"):
        """
        åˆå§‹åŒ–å…¥åº“ç®¡ç†å™¨
        
        Args:
            environment: è¿è¡Œç¯å¢ƒ ("production", "test", "development")
            chunker_version: åˆ‡ç‰‡å™¨ç‰ˆæœ¬ ("v2", "v3")
        """
        self.environment = environment
        self.chunker_version = chunker_version
        self.chunker = None
        self.kg_manager = None
        self.pinecone_index = None
        self.embedding_model = None

        logging.info(f"ğŸš€ UpsertManager åˆå§‹åŒ–ï¼Œç¯å¢ƒ: {self.environment}, åˆ‡ç‰‡å™¨ç‰ˆæœ¬: {self.chunker_version}")

    def _initialize_services(self, enable_kg: bool, enable_pinecone: bool):
        """å»¶è¿Ÿåˆå§‹åŒ–æ‰€éœ€æœåŠ¡"""
        if self.chunker is None:
            if self.chunker_version == 'v3':
                self.chunker = SemanticDocumentChunker()
                logging.info("âœ… SemanticDocumentChunker (v3) åˆå§‹åŒ–å®Œæˆ")
            elif self.chunker_version == 'v2':
                self.chunker = DocumentChunker()
                logging.info("âœ… DocumentChunker (v2) åˆå§‹åŒ–å®Œæˆ")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆ‡ç‰‡å™¨ç‰ˆæœ¬: {self.chunker_version}")
            
        if enable_kg and self.kg_manager is None:
            self.kg_manager = KnowledgeGraphManager(environment=self.environment)
            logging.info("âœ… KnowledgeGraphManager åˆå§‹åŒ–å®Œæˆ")
            
        if enable_pinecone and self.pinecone_index is None:
            self.pinecone_index = get_pinecone_index()
            self.embedding_model = get_embedding_model()
            logging.info("âœ… Pinecone Index å’Œ Embedding Model åˆå§‹åŒ–å®Œæˆ")

    def process_and_upsert_document(self, file_path: str, enable_kg: bool = True, enable_pinecone: bool = True, extra_metadata: dict = None):
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶å°†å…¶å…¥åº“
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            enable_kg: æ˜¯å¦å¯ç”¨çŸ¥è¯†å›¾è°±å†™å…¥
            enable_pinecone: æ˜¯å¦å¯ç”¨Pineconeå‘é‡ä¸Šä¼ 
            extra_metadata: é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚factionï¼‰ï¼Œä¼˜å…ˆçº§æœ€é«˜
        """
        if not os.path.exists(file_path):
            logging.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        logging.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
        self._initialize_services(enable_kg, enable_pinecone)

        try:
            # 1. è¯»å–å’Œåˆ‡ç‰‡æ–‡æ¡£
            logging.info("Step 1: æ­£åœ¨è¿›è¡Œæ–‡æ¡£åˆ‡ç‰‡...")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç»„è£…extra_metadata
            base_metadata = {'source_file': os.path.basename(file_path)}
            if extra_metadata:
                base_metadata.update(extra_metadata)
            # å°è¯•ä»æ–‡ä»¶åæ¨æ–­factionï¼ˆå¦‚æœªæŒ‡å®šï¼‰
            if 'faction' not in base_metadata:
                filename = os.path.basename(file_path)
                if 'aeldari' in filename.lower():
                    base_metadata['faction'] = 'é˜¿è‹ç„‰å°¼'
                elif 'corerule' in filename.lower():
                    base_metadata['faction'] = 'æ ¸å¿ƒè§„åˆ™'
                elif 'corefaq' in filename.lower():
                    base_metadata['faction'] = 'æ ¸å¿ƒFAQ'
            chunks = self.chunker.chunk_text(content, base_metadata)
            logging.info(f"âœ… æ–‡æ¡£åˆ‡ç‰‡å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ‡ç‰‡ã€‚")
            
            # ä¿å­˜åˆ‡ç‰‡åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œç”¨äºè§‚å¯Ÿåˆ‡ç‰‡æ•ˆæœ
            try:
                save_chunks_to_json(chunks, "DATAUPLOD/chunks")
                logging.info("âœ… åˆ‡ç‰‡å·²ä¿å­˜åˆ° DATAUPLOD/chunks ç›®å½•")
            except Exception as e:
                logging.warning(f"âš ï¸ ä¿å­˜åˆ‡ç‰‡æ–‡ä»¶æ—¶å‡ºç°è­¦å‘Š: {e}")

            # === æ–°å¢ï¼šç”Ÿæˆ/åˆ·æ–°å…¨åº“ä¿æŠ¤æ€§è¯å…¸ ===
            userdict_path = 'all_docs_dict.txt'
            # å‡è®¾æ‰€æœ‰æ–‡æ¡£è·¯å¾„å¯é€šè¿‡æŸç§æ–¹å¼è·å¾—ï¼Œè¿™é‡Œä»…ç”¨å½“å‰file_pathåšæ¼”ç¤º
            # å®é™…å·¥ç¨‹ä¸­å»ºè®®ä¼ å…¥æ‰€æœ‰æ–‡æ¡£è·¯å¾„
            subprocess.run(f"python3 DATAUPLOD/gen_userdict.py --f {file_path}", shell=True, check=True)
            logging.info(f"âœ… ä¿æŠ¤æ€§è¯å…¸å·²ç”Ÿæˆ/åˆ·æ–°: {userdict_path}")

            # === æ–°å¢ï¼šåˆå§‹åŒ–BM25Managerå¹¶fitæ‰€æœ‰chunkæ–‡æœ¬ ===
            bm25 = BM25Manager(user_dict_path=userdict_path)
            bm25.fit([chunk['text'] for chunk in chunks])
            logging.info(f"âœ… BM25Managerå·²åˆå§‹åŒ–å¹¶fitæ‰€æœ‰chunkæ–‡æœ¬")

            # 2. çŸ¥è¯†å›¾è°±å†™å…¥
            if enable_kg and self.kg_manager:
                logging.info("Step 2: æ­£åœ¨å†™å…¥çŸ¥è¯†å›¾è°±...")
                total_entities = 0
                total_relationships = 0
                for i, chunk in enumerate(chunks):
                    entities = self.kg_manager.extract_entities(chunk['text'], chunk.get('section_heading'), chunk.get('hierarchy'))
                    relationships = self.kg_manager.extract_relationships(chunk['text'], entities, chunk.get('content_type'))
                    
                    if entities or relationships:
                        self.kg_manager.update_knowledge_graph(entities, relationships, chunk)
                        total_entities += len(entities)
                        total_relationships += len(relationships)
                        logging.debug(f"  - Chunk {i+1}: å†™å…¥ {len(entities)} å®ä½“, {len(relationships)} å…³ç³»")
                
                logging.info(f"âœ… çŸ¥è¯†å›¾è°±å†™å…¥å®Œæˆï¼Œå…±è®¡å†™å…¥ {total_entities} ä¸ªå®ä½“, {total_relationships} ä¸ªå…³ç³»ã€‚")

            # 3. Pinecone å‘é‡ä¸Šä¼ 
            if enable_pinecone and self.pinecone_index and self.embedding_model:
                logging.info("Step 3: æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶ä¸Šä¼ åˆ° Pinecone...")
                
                # æ„å»ºå¾…ä¸Šä¼ çš„æ•°æ®å’Œä¸Šä¼ è®°å½•
                vectors_to_upsert = []
                upload_records = []
                chunk_id_mapping = {}
                
                for i, chunk in enumerate(chunks):
                    chunk_id = f"{os.path.basename(file_path)}-{i}"
                    chunk_id_mapping[i] = chunk_id
                    try:
                        embedding = self.embedding_model.embed_query(chunk['text'])
                        # æ–°å¢ï¼šç”Ÿæˆsparse embedding
                        sparse_embedding = bm25.get_sparse_vector(chunk['text'])
                        pinecone_metadata = {
                            'text': chunk['text'],
                            'chunk_type': chunk.get('chunk_type', ''),
                            'content_type': chunk.get('content_type', ''),
                            'faction': chunk.get('faction', base_metadata.get('faction', 'æœªçŸ¥')),
                            'section_heading': chunk.get('section_heading', ''),
                        }
                        # åŠ å…¥å¤šçº§æ ‡é¢˜å­—æ®µ
                        for j in range(1, 7):
                            pinecone_metadata[f'h{j}'] = chunk.get(f'h{j}', '')
                        pinecone_metadata['chunk_id'] = str(i)
                        pinecone_metadata['source_file'] = chunk.get('source_file', '')
                        vector_data = {
                            'id': chunk_id,
                            'values': embedding,
                            'sparse_values': sparse_embedding,
                            'metadata': pinecone_metadata
                        }
                        vectors_to_upsert.append(vector_data)
                        upload_records.append({
                            'chunk_id': chunk_id,
                            'status': 'success',
                            'metadata': pinecone_metadata
                        })
                    except Exception as e:
                        print(f"[ERROR] Chunk {i} embedding/upload failed: {e}")
                        upload_records.append({
                            'chunk_id': chunk_id,
                            'status': 'fail',
                            'error': str(e)
                        })

                # åˆ†æ‰¹ä¸Šä¼ 
    batch_size = 100
                successful_uploads = 0
                failed_uploads = 0
                
                for i in range(0, len(vectors_to_upsert), batch_size):
                    batch = vectors_to_upsert[i:i+batch_size]
                    batch_start = i
                    batch_end = min(i + batch_size, len(vectors_to_upsert))
                    
                    try:
                        self.pinecone_index.upsert(vectors=batch)
                        successful_uploads += len(batch)
                        
                        # æ›´æ–°æ‰¹æ¬¡ä¸­æ‰€æœ‰åˆ‡ç‰‡çš„çŠ¶æ€
                        for j in range(batch_start, batch_end):
                            if j < len(upload_records):
                                upload_records[j]['status'] = 'uploaded'
                        
                        logging.info(f"  - ä¸Šä¼ æ‰¹æ¬¡ {i//batch_size + 1}/{(len(vectors_to_upsert)-1)//batch_size + 1}: {len(batch)} ä¸ªåˆ‡ç‰‡")
                        
                    except Exception as e:
                        failed_uploads += len(batch)
                        logging.error(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} ä¸Šä¼ å¤±è´¥: {e}")
                        
                        # æ›´æ–°æ‰¹æ¬¡ä¸­æ‰€æœ‰åˆ‡ç‰‡çš„çŠ¶æ€
                        for j in range(batch_start, batch_end):
                            if j < len(upload_records):
                                upload_records[j]['status'] = 'failed'
                                upload_records[j]['error'] = str(e)
                
                # ä¿å­˜ä¸Šä¼ è®°å½•åˆ°æ–‡ä»¶
                import json
                from datetime import datetime
                
                upload_log_file = f"DATAUPLOD/upload_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                os.makedirs(os.path.dirname(upload_log_file), exist_ok=True)
                
                upload_summary = {
                    'file_path': file_path,
                    'upload_time': datetime.now().isoformat(),
                    'total_chunks': len(chunks),
                    'successful_uploads': successful_uploads,
                    'failed_uploads': failed_uploads,
                    'upload_records': upload_records
                }
                
                with open(upload_log_file, 'w', encoding='utf-8') as f:
                    json.dump(upload_summary, f, ensure_ascii=False, indent=2)
                
                logging.info(f"âœ… Pinecone å‘é‡ä¸Šä¼ å®Œæˆ:")
                logging.info(f"  - æ€»åˆ‡ç‰‡æ•°: {len(chunks)}")
                logging.info(f"  - æˆåŠŸä¸Šä¼ : {successful_uploads}")
                logging.info(f"  - ä¸Šä¼ å¤±è´¥: {failed_uploads}")
                logging.info(f"  - ä¸Šä¼ è®°å½•å·²ä¿å­˜åˆ°: {upload_log_file}")
                
                # æ£€æŸ¥ç‰¹å®šåˆ‡ç‰‡æ˜¯å¦ä¸Šä¼ æˆåŠŸ
                if failed_uploads > 0:
                    logging.warning("âš ï¸ éƒ¨åˆ†åˆ‡ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ è®°å½•æ–‡ä»¶")
                    for record in upload_records:
                        if record['status'] == 'failed':
                            logging.warning(f"  - åˆ‡ç‰‡ {record['chunk_id']} ä¸Šä¼ å¤±è´¥: {record['error']}")

            logging.info(f"ğŸ‰ æ–‡æ¡£ {file_path} å¤„ç†å®Œæˆï¼")

        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ–‡æ¡£ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        
        finally:
            if self.kg_manager:
                self.kg_manager.close()
                logging.info("Neo4j è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºå‘½ä»¤è¡Œè°ƒç”¨"""
    parser = argparse.ArgumentParser(description="æ–‡æ¡£å…¥åº“æµç¨‹ç®¡ç†å™¨")
    parser.add_argument("file_path", type=str, help="è¦å¤„ç†çš„æ–‡æ¡£è·¯å¾„")
    parser.add_argument("--env", type=str, default="production", choices=["production", "test", "development"], help="è¿è¡Œç¯å¢ƒ")
    parser.add_argument("-cv", "--chunker-version", type=str, default="v3", choices=["v2", "v3"], help="é€‰æ‹©ä½¿ç”¨çš„åˆ‡ç‰‡å™¨ç‰ˆæœ¬ (v2/v3)")
    parser.add_argument("--no-kg", action="store_true", help="ç¦ç”¨çŸ¥è¯†å›¾è°±å†™å…¥")
    parser.add_argument("--no-pinecone", action="store_true", help="ç¦ç”¨Pineconeå‘é‡ä¸Šä¼ ")
    parser.add_argument("--faction", type=str, help="æŒ‡å®šfactionï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰")
    args = parser.parse_args()
    
    manager = UpsertManager(environment=args.env, chunker_version=args.chunker_version)
    # ç»„è£…extra_metadataï¼Œä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    extra_metadata = {'source_file': os.path.basename(args.file_path)}
    if args.faction:
        extra_metadata['faction'] = args.faction
    manager.process_and_upsert_document(
        file_path=args.file_path,
        enable_kg=not args.no_kg,
        enable_pinecone=not args.no_pinecone,
        extra_metadata=extra_metadata
    )

if __name__ == "__main__":
    main()
