#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ‡ç‰‡é€»è¾‘åˆ†ææµ‹è¯•è„šæœ¬
ç”¨äºåˆ†æ"æ­»äº¡ä¹‹èˆ"ç­‰å…³é”®å†…å®¹æ˜¯å¦è¢«æ­£ç¡®åˆ‡ç‰‡
"""

import os
import sys
import re
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataupload.data_vector_v3 import SemanticDocumentChunker, save_chunks_to_text, save_chunks_to_json

def find_skill_content_in_text(text: str, skill_name: str) -> List[Dict[str, Any]]:
    """
    åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾æŒ‡å®šæŠ€èƒ½çš„å†…å®¹
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        skill_name: æŠ€èƒ½åç§°
        
    Returns:
        åŒ…å«æŠ€èƒ½å†…å®¹çš„æ®µè½åˆ—è¡¨
    """
    lines = text.split('\n')
    skill_paragraphs = []
    current_paragraph = []
    in_skill_section = False
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€èƒ½åç§°
        if skill_name in line_stripped:
            in_skill_section = True
            current_paragraph = [line_stripped]
        elif in_skill_section:
            # å¦‚æœé‡åˆ°ç©ºè¡Œæˆ–æ–°æ ‡é¢˜ï¼Œç»“æŸå½“å‰æ®µè½
            if not line_stripped or line_stripped.startswith('#'):
                if current_paragraph:
                    skill_paragraphs.append({
                        'start_line': i - len(current_paragraph) + 1,
                        'end_line': i,
                        'content': '\n'.join(current_paragraph),
                        'lines': current_paragraph
                    })
                    current_paragraph = []
                    in_skill_section = False
            else:
                current_paragraph.append(line_stripped)
    
    # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph:
        skill_paragraphs.append({
            'start_line': len(lines) - len(current_paragraph),
            'end_line': len(lines) - 1,
            'content': '\n'.join(current_paragraph),
            'lines': current_paragraph
        })
    
    return skill_paragraphs

def analyze_chunk_coverage(chunks: List[Dict[str, Any]], skill_name: str) -> Dict[str, Any]:
    """
    åˆ†æåˆ‡ç‰‡å¯¹æŒ‡å®šæŠ€èƒ½çš„è¦†ç›–æƒ…å†µ
    
    Args:
        chunks: åˆ‡ç‰‡åˆ—è¡¨
        skill_name: æŠ€èƒ½åç§°
        
    Returns:
        è¦†ç›–åˆ†æç»“æœ
    """
    coverage_analysis = {
        'skill_name': skill_name,
        'total_chunks': len(chunks),
        'containing_chunks': [],
        'missing_chunks': [],
        'coverage_rate': 0.0
    }
    
    # æ£€æŸ¥æ¯ä¸ªåˆ‡ç‰‡æ˜¯å¦åŒ…å«æŠ€èƒ½åç§°
    for i, chunk in enumerate(chunks):
        chunk_text = chunk.get('text', '')
        if skill_name in chunk_text:
            coverage_analysis['containing_chunks'].append({
                'chunk_index': i,
                'chunk_id': chunk.get('chunk_id', f'chunk_{i}'),
                'section_heading': chunk.get('section_heading', ''),
                'content_preview': chunk_text[:200] + '...' if len(chunk_text) > 200 else chunk_text,
                'full_content': chunk_text
            })
    
    # è®¡ç®—è¦†ç›–ç‡
    if coverage_analysis['containing_chunks']:
        coverage_analysis['coverage_rate'] = len(coverage_analysis['containing_chunks']) / len(chunks) * 100
    
    return coverage_analysis

def test_chunking_logic(file_path: str, skill_name: str = "æ­»äº¡ä¹‹èˆ"):
    """
    æµ‹è¯•åˆ‡ç‰‡é€»è¾‘
    
    Args:
        file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        skill_name: è¦æ£€æŸ¥çš„æŠ€èƒ½åç§°
    """
    print(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {file_path}")
    print(f"ğŸ¯ ç›®æ ‡æŠ€èƒ½: {skill_name}")
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå¤§å°: {len(content)} å­—ç¬¦")
    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return
    
    # 1. åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾æŠ€èƒ½å†…å®¹
    print(f"\nğŸ“– æ­¥éª¤1: åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾ '{skill_name}' å†…å®¹...")
    skill_paragraphs = find_skill_content_in_text(content, skill_name)
    
    if skill_paragraphs:
        print(f"âœ… åœ¨åŸå§‹æ–‡æœ¬ä¸­æ‰¾åˆ° {len(skill_paragraphs)} ä¸ªåŒ…å« '{skill_name}' çš„æ®µè½:")
        for i, para in enumerate(skill_paragraphs, 1):
            print(f"  æ®µè½ {i}:")
            print(f"    è¡Œå·: {para['start_line']}-{para['end_line']}")
            print(f"    å†…å®¹é¢„è§ˆ: {para['content'][:100]}...")
    else:
        print(f"âŒ åœ¨åŸå§‹æ–‡æœ¬ä¸­æœªæ‰¾åˆ° '{skill_name}' ç›¸å…³å†…å®¹")
        return
    
    # 2. ä½¿ç”¨åˆ‡ç‰‡å™¨å¤„ç†æ–‡æ¡£
    print(f"\nğŸ”§ æ­¥éª¤2: ä½¿ç”¨åˆ‡ç‰‡å™¨å¤„ç†æ–‡æ¡£...")
    try:
        chunker = SemanticDocumentChunker()
        chunks = chunker.chunk_text(content, {"source_file": os.path.basename(file_path)})
        print(f"âœ… åˆ‡ç‰‡å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ‡ç‰‡")
        
        # ä¼˜åŒ–åˆ‡ç‰‡
        optimized_chunks = chunker.optimize_chunks(chunks)
        print(f"âœ… ä¼˜åŒ–å®Œæˆï¼Œä¼˜åŒ–å {len(optimized_chunks)} ä¸ªåˆ‡ç‰‡")
        
    except Exception as e:
        print(f"âŒ åˆ‡ç‰‡å¤„ç†å¤±è´¥: {e}")
        return
    
    # 3. åˆ†æåˆ‡ç‰‡è¦†ç›–æƒ…å†µ
    print(f"\nğŸ“Š æ­¥éª¤3: åˆ†æåˆ‡ç‰‡å¯¹ '{skill_name}' çš„è¦†ç›–æƒ…å†µ...")
    coverage_analysis = analyze_chunk_coverage(optimized_chunks, skill_name)
    
    print(f"ğŸ“ˆ è¦†ç›–åˆ†æç»“æœ:")
    print(f"  æ€»åˆ‡ç‰‡æ•°: {coverage_analysis['total_chunks']}")
    print(f"  åŒ…å«æŠ€èƒ½çš„åˆ‡ç‰‡æ•°: {len(coverage_analysis['containing_chunks'])}")
    print(f"  è¦†ç›–ç‡: {coverage_analysis['coverage_rate']:.2f}%")
    
    if coverage_analysis['containing_chunks']:
        print(f"\nğŸ“‹ åŒ…å« '{skill_name}' çš„åˆ‡ç‰‡è¯¦æƒ…:")
        for i, chunk_info in enumerate(coverage_analysis['containing_chunks'], 1):
            print(f"  åˆ‡ç‰‡ {i}:")
            print(f"    ç´¢å¼•: {chunk_info['chunk_index']}")
            print(f"    æ ‡é¢˜: {chunk_info['section_heading']}")
            print(f"    å†…å®¹é¢„è§ˆ: {chunk_info['content_preview']}")
            print()
    else:
        print(f"âŒ åˆ‡ç‰‡ä¸­æœªæ‰¾åˆ° '{skill_name}' ç›¸å…³å†…å®¹")
        
        # 4. æ·±å…¥åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°
        print(f"\nğŸ” æ­¥éª¤4: æ·±å…¥åˆ†æåˆ‡ç‰‡å†…å®¹...")
        print("æ£€æŸ¥å‰10ä¸ªåˆ‡ç‰‡çš„å†…å®¹:")
        for i, chunk in enumerate(optimized_chunks[:10], 1):
            chunk_text = chunk.get('text', '')
            print(f"  åˆ‡ç‰‡ {i}:")
            print(f"    æ ‡é¢˜: {chunk.get('section_heading', 'æ— æ ‡é¢˜')}")
            print(f"    å†…å®¹é•¿åº¦: {len(chunk_text)} å­—ç¬¦")
            print(f"    å†…å®¹é¢„è§ˆ: {chunk_text[:100]}...")
            print()
    
    # 5. ä¿å­˜åˆ†æç»“æœ
    print(f"\nğŸ’¾ æ­¥éª¤5: ä¿å­˜åˆ†æç»“æœ...")
    try:
        # ä¿å­˜åˆ‡ç‰‡ç»“æœ
        timestamp = os.path.basename(file_path).replace('.md', '')
        save_chunks_to_text(optimized_chunks, f"test/testdata/{timestamp}_chunks.txt")
        save_chunks_to_json(optimized_chunks, f"test/testdata/{timestamp}_chunks.json")
        
        # ä¿å­˜è¦†ç›–åˆ†æç»“æœ
        import json
        analysis_file = f"test/testdata/{timestamp}_coverage_analysis.json"
        os.makedirs(os.path.dirname(analysis_file), exist_ok=True)
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(coverage_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ° test/testdata/ ç›®å½•")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python test_chunk_analysis.py <æ–‡æ¡£æ–‡ä»¶è·¯å¾„> [æŠ€èƒ½åç§°]")
        print("ç¤ºä¾‹: python test_chunk_analysis.py DATAUPLOD/aeldari.md æ­»äº¡ä¹‹èˆ")
        return
    
    file_path = sys.argv[1]
    skill_name = sys.argv[2] if len(sys.argv) > 2 else "æ­»äº¡ä¹‹èˆ"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    # è¿è¡Œåˆ†æ
    test_chunking_logic(file_path, skill_name)

if __name__ == "__main__":
    main() 