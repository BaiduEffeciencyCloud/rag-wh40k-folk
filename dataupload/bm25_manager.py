import os
import sys
import json
import pickle
import glob
import logging
import subprocess
from collections import Counter
import jieba
from rank_bm25 import BM25Okapi
from dataupload.wh40k_weight_enhancer import WH40KWeightEnhancer
from config import PINECONE_MAX_SPARSE_VALUES
import config as project_config
from storage.oss_storage import OSSStorage

logger = logging.getLogger(__name__)

class BM25Manager:
    def __init__(self, k1=1.5, b=0.75, min_freq=5, max_vocab_size=10000, custom_dict_path=None, user_dict_path=None, enable_wh40k_enhancement=False):
        self.k1 = k1
        self.b = b
        self.min_freq = min_freq
        self.max_vocab_size = max_vocab_size
        self.bm25_model = None
        self.vocabulary = {}
        self.corpus_tokens = []
        self.raw_texts = []  # åŸå§‹æ–‡æœ¬åˆ—è¡¨
        self.is_fitted = False
        self.whitelist_phrases = set()  # æ–°å¢ï¼šå­˜å‚¨ç™½åå•çŸ­è¯­
        self.enable_wh40k_enhancement = enable_wh40k_enhancement
        
        # åˆå§‹åŒ–æƒé‡å¢å¼ºå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_wh40k_enhancement:
            self.weight_enhancer = WH40KWeightEnhancer()
        
        if custom_dict_path and os.path.exists(custom_dict_path):
            jieba.load_userdict(custom_dict_path)
        if user_dict_path and os.path.exists(user_dict_path):
            logger.info(f"åŠ è½½ä¿æŠ¤æ€§userdict: {user_dict_path}")
            try:
                with open(user_dict_path, 'r', encoding='utf-8') as f:
                    preview_count = 0
                    for i, line in enumerate(f):
                        phrase = line.strip().split()[0] if line.strip() else None
                        if phrase:
                            self.whitelist_phrases.add(phrase)
                            preview_count += 1
                        if i >= 99:
                            break
                logger.info(f"userdict é¢„è§£æçŸ­è¯­æ•°ï¼ˆæœ€å¤šé¢„è§ˆ100è¡Œï¼‰ï¼š{preview_count}")
            except Exception as e:
                logger.warning(f"userdict é¢„è§£æå¤±è´¥ï¼ˆä¸å½±å“åŠ è½½ï¼‰ï¼š{e}")
            jieba.load_userdict(user_dict_path)

    def tokenize_chinese(self, text):
        return list(jieba.cut(text))

    def _build_vocabulary(self, corpus_texts):
        all_tokens = []
        for text in corpus_texts:
            all_tokens.extend(self.tokenize_chinese(text))
        counter = Counter(all_tokens)
        # å‰ªæï¼šåªä¿ç•™é«˜é¢‘è¯
        most_common = [w for w, c in counter.items() if c >= self.min_freq]
        most_common = most_common[:self.max_vocab_size]
        # ====== ç™½åå•å¼ºåˆ¶ä¿æŠ¤é€»è¾‘ ======
        # ä¸šåŠ¡èƒŒæ™¯ï¼šç™½åå•ï¼ˆä¿æŠ¤æ€§è¯å…¸ï¼‰ä¸­çš„çŸ­è¯­ï¼Œå±äºä¸šåŠ¡å…œåº•ã€å¿…é¡»æ•´ä½“å¬å›çš„é«˜ä¼˜å…ˆçº§tokenã€‚
        # å¦‚æœä»…é é¢‘ç‡å‰ªæï¼Œä½é¢‘/æœªå‡ºç°çš„ç™½åå•çŸ­è¯­ä¼šè¢«é—æ¼ï¼Œå¯¼è‡´æ£€ç´¢æ¼å¬å›ï¼Œè¿èƒŒä¿æŠ¤æ€§è®¾è®¡åˆè¡·ã€‚
        # å› æ­¤ï¼šæ— è®ºç™½åå•çŸ­è¯­åœ¨è¯­æ–™ä¸­å‡ºç°é¢‘ç‡å¦‚ä½•ï¼Œéƒ½å¿…é¡»å¼ºåˆ¶çº³å…¥vocabularyã€‚
        for phrase in getattr(self, 'whitelist_phrases', set()):
            if phrase not in most_common:
                most_common.append(phrase)
        self.vocabulary = {w: i for i, w in enumerate(most_common)}

    def fit(self, corpus_texts):
        self.raw_texts = list(corpus_texts)
        self._build_vocabulary(self.raw_texts)
        # ====== ç™½åå•çŸ­è¯­è™šæ‹Ÿæ–‡æœ¬å…œåº• ======
        # ä¸šåŠ¡èƒŒæ™¯ï¼šBM25Okapi åªä¼šä¸º fit è¯­æ–™ä¸­å‡ºç°è¿‡çš„ token è®¡ç®— idfã€‚
        # å¦‚æœç™½åå•çŸ­è¯­ä»…è¢«å¼ºåˆ¶åŠ å…¥ vocabularyï¼Œä½† fit è¯­æ–™æœªå‡ºç°ï¼Œåˆ™ idf ä¸ä¼šåˆ†é…ï¼Œå¯¼è‡´ç¨€ç–å‘é‡é“¾è·¯æ–­è£‚ã€‚
        # å·¥ç¨‹å…œåº•ï¼šå¦‚æœ‰ç™½åå•çŸ­è¯­æœªå‡ºç°åœ¨è¯­æ–™ä¸­ï¼Œåˆ™è¿½åŠ ä¸€æ¡â€œè™šæ‹Ÿæ–‡æœ¬â€ï¼Œå†…å®¹ä¸ºæ‰€æœ‰æœªå‡ºç°çš„ç™½åå•çŸ­è¯­ï¼Œç”¨ç©ºæ ¼æ‹¼æ¥ã€‚
        # è¿™æ · BM25Okapi çš„ idf ç»Ÿè®¡èƒ½è¦†ç›–æ‰€æœ‰ç™½åå•çŸ­è¯­ï¼Œä¸”å¯¹æ­£å¸¸æƒé‡åˆ†å¸ƒå½±å“æå°ã€‚
        missing_phrases = [p for p in getattr(self, 'whitelist_phrases', set()) if all(p not in text for text in self.raw_texts)]
        if missing_phrases:
            self.raw_texts.append(' '.join(missing_phrases))
        self.corpus_tokens = [self.tokenize_chinese(text) for text in self.raw_texts]
        self.bm25_model = BM25Okapi(self.corpus_tokens, k1=self.k1, b=self.b)
        self.is_fitted = True

    def incremental_update(self, new_corpus_texts, storage=None):
        """
        å¢é‡è®­ç»ƒï¼šæŒä¹…åŒ–+å…¨é‡é‡å»º
        storage: RawChunkStorageå®ä¾‹ï¼Œè‹¥ä¸ºNoneåˆ™ä¸åšæœ¬åœ°æŒä¹…åŒ–
        """
        if storage is not None:
            # 1. è¯»å–å†å²åˆ‡ç‰‡
            history_chunks = storage.load()
            # 2. åˆå¹¶æ–°åˆ‡ç‰‡å¹¶å»é‡
            all_chunks = list(dict.fromkeys(history_chunks + new_corpus_texts))
            # 3. ä¿å­˜å…¨é‡åˆ‡ç‰‡
            storage.storage(new_corpus_texts)
        else:
            # å…¼å®¹è€é€»è¾‘
            all_chunks = self.raw_texts + new_corpus_texts
        # 4. ç”¨å…¨é‡åˆ‡ç‰‡é‡å»ºBM25æ¨¡å‹
        self.raw_texts = all_chunks
        self.corpus_tokens = [self.tokenize_chinese(text) for text in self.raw_texts]
        self._build_vocabulary(self.raw_texts)
        self.bm25_model = BM25Okapi(self.corpus_tokens, k1=self.k1, b=self.b)
        self.is_fitted = True

    def get_sparse_vector(self, text):
        if not self.is_fitted:
            raise ValueError("BM25Manageræœªè®­ç»ƒï¼Œè¯·å…ˆfitè¯­æ–™åº“")
        tokens = self.tokenize_chinese(text)
        logger.info("æ‹†åˆ†åçš„queryæ˜¯:" +str(tokens))
        indices, values = [], []
        avgdl = self.bm25_model.avgdl
        k1, b = self.bm25_model.k1, self.bm25_model.b
        dl = len(tokens)
        token_freq = {}
        for token in tokens:
            token_freq[token] = token_freq.get(token, 0) + 1
        for token, freq in token_freq.items():
            if token in self.vocabulary:
                idf = self.bm25_model.idf.get(token, 0)
                tf = freq
                denom = tf + k1 * (1 - b + b * dl / avgdl)
                score = idf * tf * (k1 + 1) / denom
                
                # åº”ç”¨æƒé‡å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.enable_wh40k_enhancement and hasattr(self, 'weight_enhancer'):
                    score = self.weight_enhancer.calculate_enhanced_weight(token, score)
                
                if score > 0:
                    indices.append(self.vocabulary[token])
                    values.append(score)
        logger.info(f"[DEBUG] sparse indices: {indices}, values: {values}")
        # è¿½åŠ å†™å…¥æœ¬åœ° query_sparse.log
        with open("query_sparse.log", "a", encoding="utf-8") as f:
            f.write(f"[DEBUG] sparse indices: {indices}, values: {values}\n")
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šindiceså’Œvalueså¿…é¡»é•¿åº¦ä¸€è‡´ï¼Œè¿™æ˜¯ç¨€ç–å‘é‡çš„åŸºæœ¬è¦æ±‚ã€‚
        # indicesä»£è¡¨è¯å…¸ä¸­tokençš„idï¼Œvaluesä»£è¡¨å¯¹åº”çš„æƒé‡ï¼ŒäºŒè€…å¿…é¡»ä¸€ä¸€å¯¹åº”ã€‚
        # æ³¨æ„ï¼štokensï¼ˆåˆ†è¯ç»“æœï¼‰å’Œindicesé•¿åº¦å¯èƒ½ä¸ä¸€è‡´ï¼Œå› ä¸ºåªæœ‰è¯å…¸å†…çš„tokenæ‰ä¼šåˆ†é…indexã€‚
        if len(indices) != len(values):
            raise Exception(f"indiceså’Œvaluesé•¿åº¦ä¸ä¸€è‡´: indices={len(indices)}, values={len(values)}")
        
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šindicesé•¿åº¦ä¸èƒ½è¶…è¿‡PINECONE_MAX_SPARSE_VALUESï¼Œå¦åˆ™Pineconeä¼šæ‹’ç»ã€‚
        # ä¸€æ—¦è¶…é•¿è¯´æ˜chunkè¿‡å¤§æˆ–è¯å…¸è¿‡å¯†ï¼Œéœ€è¦ä¼˜åŒ–åˆ†å—ç­–ç•¥æˆ–è¯å…¸é…ç½®ã€‚
        if len(indices) > PINECONE_MAX_SPARSE_VALUES:
            raise Exception(f"ç”Ÿæˆçš„ç¨€ç–å‘é‡é•¿åº¦è¶…è¿‡PINECONE_MAX_SPARSE_VALUES ({PINECONE_MAX_SPARSE_VALUES}): {len(indices)}")
        
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šindicesä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œè¿™è¿åäº†ç¨€ç–å‘é‡çš„åŸºæœ¬è¯­ä¹‰ã€‚
        # ä¸€æ—¦å‡ºç°è´Ÿæ•°è¯´æ˜è¯å…¸æ•°æ®æ±¡æŸ“æˆ–ä¸Šæ¸¸bugï¼Œç›´æ¥raiseå¼‚å¸¸ï¼Œé˜²æ­¢è„æ•°æ®æµå…¥ä¸‹æ¸¸ã€‚
        if any(i < 0 for i in indices):
            raise Exception(f"æ£€æµ‹åˆ°éæ³•è´Ÿæ•°index: {indices}")
        
        return {"indices": indices, "values": values}

    def save_model(self, model_path, vocab_path):
        with open(model_path, 'wb') as f:
            pickle.dump(self.bm25_model, f)
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(self.vocabulary, f, ensure_ascii=False)
        # äº‘ç«¯ä¸Šä¼ ï¼ˆä»…æ¨¡å‹ï¼‰ï¼ŒæŒ‰å·²ç¡®è®¤å†³ç­–ï¼šå¤±è´¥å³ä¸­æ–­ï¼ˆraiseï¼‰
        try:
            if getattr(project_config, 'CLOUD_STORAGE', False):
                oss = OSSStorage(
                    access_key_id=project_config.OSS_ACCESS_KEY,
                    access_key_secret=project_config.OSS_ACCESS_KEY_SECRET,
                    endpoint=project_config.OSS_ENDPOINT,
                    bucket_name=project_config.OSS_BUCKET_NAME_MODELS,
                    region=project_config.OSS_REGION,
                )
                key = os.path.basename(model_path)
                with open(model_path, 'rb') as mf:
                    data = mf.read()
                ok = oss.storage(data=data, oss_path=key)
                uri = f"oss://{project_config.OSS_BUCKET_NAME_MODELS}/{key}"
                logger.info(f"[OSS] upload model: bucket={project_config.OSS_BUCKET_NAME_MODELS}, key={key}, uri={uri}, bytes={len(data)}, ok={ok}")
                if not ok:
                    raise Exception(f"æ¨¡å‹ä¸Šä¼ å¤±è´¥: {uri}")
        except Exception as e:
            # å†³ç­– 4Cï¼šå¤±è´¥å³ä¸­æ–­
            logger.error(f"[OSS] upload model failed: {e}")
            raise

    def load_model(self, model_path, vocab_path):
        model_loaded = False
        vocab_loaded = False
        # è‹¥å†…å­˜ä¸­å·²å­˜åœ¨è¯å…¸ï¼Œåˆ™è§†ä¸ºå·²åŠ è½½ï¼Œé¿å…é‡å¤ä»äº‘/æœ¬åœ°è¯»å–
        if isinstance(self.vocabulary, dict) and self.vocabulary:
            vocab_loaded = True
            logger.info("[PRESET] in-memory vocabulary detected, skip external vocab loading")
        # äº‘ç«¯ä¼˜å…ˆ
        try:
            if getattr(project_config, 'CLOUD_STORAGE', False):
                # è¯»å–æ¨¡å‹
                try:
                    oss_models = OSSStorage(
                        access_key_id=project_config.OSS_ACCESS_KEY,
                        access_key_secret=project_config.OSS_ACCESS_KEY_SECRET,
                        endpoint=project_config.OSS_ENDPOINT,
                        bucket_name=getattr(project_config, 'OSS_BUCKET_NAME_MODELS', None),
                        region=getattr(project_config, 'OSS_REGION', None),
                    )
                    model_key = os.path.basename(model_path)
                    model_bytes = oss_models.get_object_bytes(model_key)
                    if model_bytes:
                        self.bm25_model = pickle.loads(model_bytes)
                        # ç±»å‹/å±æ€§æ ¡éªŒ
                        if not hasattr(self.bm25_model, 'idf') or not hasattr(self.bm25_model, 'k1') or not hasattr(self.bm25_model, 'b'):
                            raise RuntimeError('äº‘ç«¯æ¨¡å‹å¯¹è±¡æ ¡éªŒå¤±è´¥')
                        model_loaded = True
                        logger.info(f"ä» OSS è¯»å–æ¨¡å‹æˆåŠŸ: bucket={getattr(project_config,'OSS_BUCKET_NAME_MODELS', None)}, key={model_key}, bytes={len(model_bytes)}")
                except Exception as e:
                    logger.warning(f"[OSS] load model fallback local: {e}")

                # è¯»å–è¯å…¸ï¼ˆè‹¥å†…å­˜æœªé¢„è½½ï¼‰
                if not vocab_loaded:
                    try:
                        oss_dicts = OSSStorage(
                            access_key_id=project_config.OSS_ACCESS_KEY,
                            access_key_secret=project_config.OSS_ACCESS_KEY_SECRET,
                            endpoint=project_config.OSS_ENDPOINT,
                            bucket_name=getattr(project_config, 'OSS_BUCKET_NAME_DICT', None),
                            region=getattr(project_config, 'OSS_REGION', None),
                        )
                        vocab_key = os.path.basename(vocab_path)
                        vocab_bytes = oss_dicts.get_object_bytes(vocab_key)
                        if vocab_bytes:
                            self.vocabulary = json.loads(vocab_bytes.decode('utf-8'))
                            if not isinstance(self.vocabulary, dict):
                                raise RuntimeError('äº‘ç«¯è¯å…¸æ•°æ®ä¸æ˜¯dict')
                            vocab_loaded = True
                            logger.info(f"[OSS] load vocab: bucket={getattr(project_config,'OSS_BUCKET_NAME_DICT', None)}, key={vocab_key}, bytes={len(vocab_bytes)}, used=cloud, ok=True")
                    except Exception as e:
                        logger.warning(f"[OSS] load vocab fallback local: {e}")
        except Exception as e:
            # æ€»ä½“äº‘ç«¯åˆå§‹åŒ–å¼‚å¸¸ï¼Œèµ°æœ¬åœ°å›é€€
            logger.warning(f"[OSS] init/load error, fallback to local: {e}")

        # æœ¬åœ°å›é€€ï¼ˆå¯¹æœªæˆåŠŸçš„éƒ¨åˆ†åˆ†åˆ«å›é€€ï¼‰
        try:
            if not model_loaded:
                with open(model_path, 'rb') as f:
                    self.bm25_model = pickle.load(f)
                if not hasattr(self.bm25_model, 'idf') or not hasattr(self.bm25_model, 'k1') or not hasattr(self.bm25_model, 'b'):
                    raise RuntimeError('æœ¬åœ°æ¨¡å‹å¯¹è±¡æ ¡éªŒå¤±è´¥')
                logger.info(f"[LOCAL] load model: path={model_path}, used=local, ok=True")
                model_loaded = True
            if not vocab_loaded:
                with open(vocab_path, 'r', encoding='utf-8') as f:
                    self.vocabulary = json.load(f)
                if not isinstance(self.vocabulary, dict):
                    raise RuntimeError('æœ¬åœ°è¯å…¸æ•°æ®ä¸æ˜¯dict')
                logger.info(f"[LOCAL] load vocab: path={vocab_path}, used=local, ok=True")
                vocab_loaded = True
        except Exception as e:
            # ä»»ä¸€å›é€€å¤±è´¥ï¼Œç•™å¾…ç»Ÿä¸€åˆ¤å®š
            if not model_loaded or not vocab_loaded:
                logger.error(f"[LOAD] both cloud and local failed: model_ok={model_loaded}, vocab_ok={vocab_loaded}, err={e}")
                raise RuntimeError(f"load_model failed: model_ok={model_loaded}, vocab_ok={vocab_loaded}")

        self.is_fitted = True

    # ===== é‡æ„ç›®æ ‡ï¼šæ–°å¢æ–¹æ³• =====
    def load_dictionary_from_bytes(self, data_bytes: bytes) -> dict:
        """ä»å­—èŠ‚æ•°æ®åŠ è½½è¯å…¸ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1) è¯é¢‘æ ¼å¼ï¼š{"è¯": {"weight": æ•°å€¼}, ...}
        2) ä¼ ç»Ÿæ ¼å¼ï¼š{"vocabulary": {...}} æˆ– ç›´æ¥è¯å…¸ {...}
        """
        try:
            text = data_bytes.decode('utf-8') if isinstance(data_bytes, (bytes, bytearray)) else str(data_bytes)
            dict_data = json.loads(text)
            if isinstance(dict_data, dict):
                # è¯é¢‘æ ¼å¼
                if all(isinstance(v, dict) and 'weight' in v for v in dict_data.values()):
                    self.vocabulary = {word: data['weight'] for word, data in dict_data.items()}
                    logger.info("æ£€æµ‹åˆ°å¸¦è¯é¢‘æ ¼å¼è¯å…¸ï¼ˆbytesï¼‰")
                else:
                    # ä¼ ç»Ÿæ ¼å¼
                    if 'vocabulary' in dict_data and isinstance(dict_data['vocabulary'], dict):
                        self.vocabulary = dict_data['vocabulary']
                    else:
                        self.vocabulary = dict_data
                    logger.info("æ£€æµ‹åˆ°ä¼ ç»Ÿæ ¼å¼è¯å…¸ï¼ˆbytesï¼‰")
                logger.info(f"æˆåŠŸä»bytesåŠ è½½è¯å…¸ï¼Œè¯æ±‡é‡: {len(self.vocabulary)}")
                return dict_data
            else:
                raise Exception("è§£æåçš„è¯å…¸æ•°æ®édictç±»å‹")
        except Exception as e:
            raise Exception(f"ä»bytesåŠ è½½è¯å…¸å¤±è´¥: {e}")

    def load_userdict_from_bytes(self, data_bytes: bytes) -> None:
        """ä»å­—èŠ‚æ•°æ®åŠ è½½userdictï¼ˆç™½åå•çŸ­è¯­ï¼‰ã€‚
        æ¯è¡Œä¸€ä¸ªçŸ­è¯­ï¼Œå¯èƒ½åŒ…å«é¢‘ç‡ã€è¯æ€§ï¼›ä»…å–é¦–åˆ—ä¸ºçŸ­è¯­ã€‚
        åŒæ—¶åŠ è½½åˆ°jiebaçš„ç”¨æˆ·è¯å…¸ä¸­ï¼Œä»¥ä¾¿åˆ†è¯ä¿æŠ¤ã€‚
        """
        try:
            content = data_bytes.decode('utf-8') if isinstance(data_bytes, (bytes, bytearray)) else str(data_bytes)
            phrases = set()
            for line in content.splitlines():
                stripped = line.strip()
                if not stripped:
                    continue
                first = stripped.split()[0]
                if first:
                    phrases.add(first)
            # è®°å½•åˆ°å®ä¾‹çš„ç™½åå•é›†åˆ
            if not hasattr(self, 'whitelist_phrases'):
                self.whitelist_phrases = set()
            self.whitelist_phrases.update(phrases)
            # å°†çŸ­è¯­åŠ¨æ€åŠ å…¥jiebaè¯å…¸ï¼ˆæ— é¢‘ç‡åˆ™ä½¿ç”¨é»˜è®¤ï¼‰
            for p in phrases:
                jieba.add_word(p)
            logger.info(f"ä»bytesåŠ è½½userdictå®Œæˆï¼ŒçŸ­è¯­æ•°: {len(phrases)}")
        except Exception as e:
            raise Exception(f"ä»bytesåŠ è½½userdictå¤±è´¥: {e}")
    
    def get_latest_dict_file(self, dict_dir: str = "dict") -> str:
        """æŸ¥æ‰¾dictç›®å½•ä¸‹æœ€æ–°çš„bm25_vocab_*.jsonæ–‡ä»¶ï¼ˆæ’é™¤å¸¦freqçš„æ–‡ä»¶ï¼‰"""
        if not os.path.exists(dict_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰bm25_vocab_*.jsonæ–‡ä»¶ï¼Œä½†æ’é™¤å¸¦freqçš„æ–‡ä»¶
        vocab_pattern = os.path.join(dict_dir, "bm25_vocab_*.json")
        vocab_files = glob.glob(vocab_pattern)
        
        # è¿‡æ»¤æ‰åŒ…å«freqçš„æ–‡ä»¶
        vocab_files = [f for f in vocab_files if 'freq' not in os.path.basename(f)]
        
        if not vocab_files:
            return None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        vocab_files.sort(reverse=True)
        return vocab_files[0]
    
    def get_latest_freq_dict_file(self, dict_dir: str = "dict") -> str:
        """æŸ¥æ‰¾dictç›®å½•ä¸‹æœ€æ–°çš„bm25_vocab_freq_*.jsonæ–‡ä»¶"""
        if not os.path.exists(dict_dir):
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰bm25_vocab_freq_*.jsonæ–‡ä»¶
        freq_pattern = os.path.join(dict_dir, "bm25_vocab_freq_*.json")
        freq_files = glob.glob(freq_pattern)
        
        if not freq_files:
            return None
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        freq_files.sort(reverse=True)
        return freq_files[0]
    
    def load_dictionary(self, dict_file_path: str) -> dict:
        """åŠ è½½è¯å…¸æ–‡ä»¶ - æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ ¼å¼"""
        try:
            with open(dict_file_path, 'r', encoding='utf-8') as f:
                dict_data = json.load(f)
            
            # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ ¼å¼
            if 'freq' in os.path.basename(dict_file_path):
                # å¸¦è¯é¢‘æ ¼å¼ï¼šæå–weightå€¼
                self.vocabulary = {word: data['weight'] for word, data in dict_data.items()}
                logger.info("æ£€æµ‹åˆ°å¸¦è¯é¢‘æ ¼å¼è¯å…¸ï¼Œæå–weightå€¼")
            else:
                # ä¼ ç»Ÿæ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
                if 'vocabulary' in dict_data:
                    self.vocabulary = dict_data['vocabulary']
                else:
                    self.vocabulary = dict_data
                logger.info("æ£€æµ‹åˆ°ä¼ ç»Ÿæ ¼å¼è¯å…¸")
            
            logger.info(f"æˆåŠŸåŠ è½½è¯å…¸ï¼Œè¯æ±‡é‡: {len(self.vocabulary)}")
            return dict_data
        except Exception as e:
            raise Exception(f"åŠ è½½è¯å…¸å¤±è´¥: {e}")
    
    def generate_sparse_vectors(self, texts: list, config=None) -> list:
        """ä¸ºæ–‡æœ¬åˆ—è¡¨ç”Ÿæˆsparseå‘é‡ - æ”¯æŒå‚æ•°é…ç½®"""
        if not self.vocabulary:
            raise Exception("è¯å…¸æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½è¯å…¸")
        
        # å¦‚æœæä¾›äº†é…ç½®ï¼Œåˆ›å»ºæ–°å®ä¾‹ä»¥æ”¯æŒå‚æ•°è°ƒä¼˜ï¼ˆevaltoolçš„æ–¹å¼ï¼‰
        if config:
            logger.info("ğŸ”„ ä½¿ç”¨é…ç½®å‚æ•°åˆ›å»ºæ–°çš„BM25Managerå®ä¾‹")
            # åˆ›å»ºæ–°çš„BM25Managerå®ä¾‹ï¼Œä½¿ç”¨é…ç½®å‚æ•°
            bm25 = BM25Manager(
                min_freq=config.min_freq,
                max_vocab_size=config.max_vocab_size
            )
            bm25.vocabulary = self.vocabulary
            # é‡æ–°è®­ç»ƒä»¥åº”ç”¨æ–°å‚æ•°
            bm25.fit(texts)
            logger.info("âœ… æ–°BM25Managerå®ä¾‹è®­ç»ƒå®Œæˆ")
        else:
            # ä½¿ç”¨å½“å‰å®ä¾‹ï¼ˆupsert.pyçš„æ–¹å¼ï¼‰
            logger.info("ğŸ“ ä½¿ç”¨å½“å‰BM25Managerå®ä¾‹")
            bm25 = self
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œè¿›è¡Œè®­ç»ƒ
            if not bm25.is_fitted:
                logger.info("ğŸ”„ å¼€å§‹è®­ç»ƒBM25æ¨¡å‹...")
                bm25.fit(texts)
                logger.info("âœ… BM25æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # ç”Ÿæˆç¨€ç–å‘é‡
        sparse_vectors = []
        for text in texts:
            try:
                sparse_vector = bm25.get_sparse_vector(text)
                sparse_vectors.append(sparse_vector)
            except Exception as e:
                logger.warning(f"ç”Ÿæˆsparseå‘é‡å¤±è´¥: {e}")
                sparse_vectors.append({'indices': [], 'values': []})
        return sparse_vectors
    
    def analyze_vocabulary_coverage(self, texts: list) -> dict:
        """åˆ†æè¯æ±‡è¡¨è¦†ç›–ç‡ - å‚è€ƒevaltoolçš„å®ç°"""
        if not texts:
            return {
                'total_tokens': 0,
                'covered_tokens': 0,
                'coverage_rate': 0,
                'missing_tokens': []
            }
        
        all_tokens = []
        for text in texts:
            tokens = list(jieba.cut(text))
            all_tokens.extend(tokens)
        
        total_tokens = len(all_tokens)
        unique_tokens = set(all_tokens)
        covered_tokens = [token for token in unique_tokens if token in self.vocabulary]
        missing_tokens = [token for token in unique_tokens if token not in self.vocabulary]
        coverage_rate = len(covered_tokens) / len(unique_tokens) if unique_tokens else 0
        
        return {
            'total_tokens': total_tokens,
            'unique_tokens': len(unique_tokens),
            'covered_tokens': len(covered_tokens),
            'coverage_rate': coverage_rate,
            'missing_tokens': missing_tokens[:100]
        }
    
    def get_vocabulary_stats(self) -> dict:
        """è·å–è¯æ±‡ç»Ÿè®¡ä¿¡æ¯ - ä¸evaltoolä¿æŒä¸€è‡´"""
        if not self.vocabulary:
            return {
                'vocabulary_size': 0,
                'avg_frequency': 0,
                'max_frequency': 0,
                'min_frequency': 0
            }
        
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾vocabularyçš„å€¼æ˜¯é¢‘ç‡ï¼Œå¦‚æœä¸æ˜¯ï¼Œéœ€è¦è°ƒæ•´é€»è¾‘
        # æˆ‘ä¸çŸ¥é“æ‚¨çš„è¯å…¸æ–‡ä»¶ä¸­vocabularyçš„å€¼å…·ä½“æ˜¯ä»€ä¹ˆå«ä¹‰ï¼Œè¯·ç¡®è®¤
        frequencies = list(self.vocabulary.values())
        
        return {
            'vocabulary_size': len(self.vocabulary),
            'avg_frequency': sum(frequencies) / len(frequencies),
            'max_frequency': max(frequencies),
            'min_frequency': min(frequencies)
        }
    
    # generate_dictionary å‡½æ•°å·²ç§»é™¤ï¼ˆæœªåœ¨ä¸šåŠ¡é€»è¾‘ä¸­ä½¿ç”¨ï¼‰