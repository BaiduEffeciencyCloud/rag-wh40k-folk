import os
import json
import logging
import asyncio
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from openai import OpenAI
from neo4j import GraphDatabase
import sys
import argparse

# Ensure the project root is in the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from config import OPENAI_API_KEY, LLM_MODEL
from dataupload.data_vector_v3 import SemanticDocumentChunker

logger = logging.getLogger(__name__)

@dataclass
class Entity:
    """å®ä½“æ•°æ®ç±»"""
    text: str
    type: str  # unit, skill, weapon, phase, rule
    confidence: float
    aliases: List[str] = None
    properties: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.aliases is None:
            self.aliases = []
        if self.properties is None:
            self.properties = {}

@dataclass
class Relationship:
    """å…³ç³»æ•°æ®ç±»"""
    source_entity: Entity
    target_entity: Entity
    relation_type: str  # HAS_SKILL, ACTIVATES_IN, etc.
    properties: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.properties is None:
            self.properties = {}

class KnowledgeGraphManager:
    """
    çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ (å•ä¸€æ•°æ®åº“ï¼Œé€šè¿‡å‰ç¼€å’Œå±æ€§å®ç°ç¯å¢ƒéš”ç¦»)
    """
    
    def __init__(self, 
                 neo4j_uri: str = None,
                 username: str = None,
                 password: str = None,
                 openai_api_key: str = None,
                 environment: str = "production",
                 patterns_config_path: str = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨
        """
        self.environment = environment.lower()
        
        # æ ¹æ®ç¯å¢ƒå†³å®šè¿æ¥ä¿¡æ¯å’Œå‰ç¼€
        if self.environment == "test":
            self.neo4j_uri = neo4j_uri or os.getenv("NEO4J_TEST_URI", "bolt://localhost:7687")
            self.username = username or os.getenv("NEO4J_TEST_USERNAME", "neo4j")
            self.password = password or os.getenv("NEO4J_TEST_PASSWORD", "19811024")
            self.node_prefix = "test_"
            self.relationship_prefix = "test_"
        else: # production or development
            self.neo4j_uri = neo4j_uri or os.getenv("NEO4J_URI", "bolt://localhost:7687")
            self.username = username or os.getenv("NEO4J_USERNAME", "neo4j")
            self.password = password or os.getenv("NEO4J_PASSWORD", "19811024")
            self.node_prefix = ""
            self.relationship_prefix = ""

        self.openai_client = OpenAI(api_key=openai_api_key or OPENAI_API_KEY)
        
        try:
            self.driver = GraphDatabase.driver(self.neo4j_uri, auth=(self.username, self.password))
            with self.driver.session() as session:
                session.run("RETURN 1")
            logging.info(f"âœ… Neo4jè¿æ¥æˆåŠŸ - ç¯å¢ƒ: {self.environment}")
        except Exception as e:
            logging.error(f"âŒ Neo4jè¿æ¥å¤±è´¥: {e}")
            self.driver = None
        
        # ä»Neo4jæ•°æ®åº“åŠ¨æ€åŠ è½½æ¨¡å¼å’Œåˆ«å
        self._load_patterns_from_db()
        
        logger.info(f"KnowledgeGraphManageråˆå§‹åŒ–å®Œæˆ - ç¯å¢ƒ: {self.environment}")
    
    def _load_patterns_from_db(self):
        """ä»Neo4jæ•°æ®åº“åŠ¨æ€åŠ è½½å®ä½“æ¨¡å¼å’Œåˆ«åæ˜ å°„"""
        if not self.driver:
            logging.error("Neo4jé©±åŠ¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ¨¡å¼ã€‚")
            self.entity_patterns = {}
            self.alias_mapping = {}
            return

        self.entity_patterns = {}
        self.alias_mapping = {}
        
        logging.info("ä»Neo4jæ•°æ®åº“åŠ è½½å®ä½“æ¨¡å¼...")
        with self.driver.session() as session:
            try:
                # 1. æŸ¥è¯¢æ‰€æœ‰å¸¦æœ‰ 'name' å±æ€§çš„èŠ‚ç‚¹æ ‡ç­¾ (é™¤å»äº†_testå‰ç¼€å’Œä¸€äº›é€šç”¨æ ‡ç­¾)
                labels_result = session.run("""
                    CALL db.labels() YIELD label
                    WHERE NOT label STARTS WITH 'test_' AND NOT label IN ['Entity', 'Resource']
                    RETURN label
                """)
                entity_types = [record["label"] for record in labels_result]

                # 2. ä¸ºæ¯ä¸ªæ ‡ç­¾(å®ä½“ç±»å‹)æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹åç§°
                for label in entity_types:
                    # å°†æ ‡ç­¾åè½¬ä¸ºå°å†™ï¼Œä»¥ç”¨ä½œentity_patternsçš„é”® (e.g., 'Faction' -> 'faction')
                    pattern_key = label.lower().replace('_', '') # 'Sub_faction' -> 'subfaction'
                    
                    query = f"MATCH (n:{label}) RETURN n.name AS name"
                    nodes_result = session.run(query)
                    self.entity_patterns[pattern_key] = [record["name"] for record in nodes_result]
                
                logging.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.entity_patterns)} ç§å®ä½“ç±»å‹ï¼Œå…± {sum(len(v) for v in self.entity_patterns.values())} ä¸ªå®ä½“ã€‚")

                # 3. æŸ¥è¯¢æ‰€æœ‰åˆ«åå…³ç³»
                aliases_result = session.run("""
                    MATCH (alias)-[:ALIAS_OF]->(entity)
                    RETURN entity.name AS entity_name, COLLECT(alias.name) AS aliases
                """)
                for record in aliases_result:
                    self.alias_mapping[record["entity_name"]] = record["aliases"]
                
                logging.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.alias_mapping)} ä¸ªå®ä½“çš„åˆ«åæ˜ å°„ã€‚")

            except Exception as e:
                logging.error(f"âŒ ä»Neo4jåŠ è½½æ¨¡å¼æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                self.entity_patterns = {}
                self.alias_mapping = {}

    def extract_entities(self, text: str, heading: str = None, hierarchy: Dict[str, str] = None) -> List[Entity]:
        """
        ä»æ–‡æœ¬ä¸­æå–å®ä½“ï¼ˆæ··åˆæ¨¡å¼ï¼šè§„åˆ™ + LLMå¹¶è¡Œï¼Œè§„åˆ™ä¼˜å…ˆï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            heading: ç« èŠ‚æ ‡é¢˜
            hierarchy: å±‚çº§ä¿¡æ¯
            
        Returns:
            List[Entity]: æå–çš„å®ä½“åˆ—è¡¨
        """
        # 1. é¦–å…ˆï¼Œæ‰§è¡Œè§„åˆ™æå–
        entities_from_rules = self._extract_entities_by_rules(text)
        
        # 2. ç„¶åï¼Œè®©LLMç‹¬ç«‹åœ°è¿›è¡Œæå–
        try:
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†ä¼ é€’ existing_entities
            entities_from_llm = self.extract_entities_with_llm(text)
            logging.info(f"ğŸ¤– LLM ç‹¬ç«‹å‘ç°äº† {len(entities_from_llm)} ä¸ªå®ä½“ã€‚")
        except Exception as e:
            logging.error(f"âŒ LLMå®ä½“æå–å¤±è´¥: {e}")
            entities_from_llm = []

        # 3. åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨ï¼ˆè§„åˆ™æå–çš„ç»“æœåœ¨å‰ï¼Œä¿è¯å…¶ä¼˜å…ˆçº§ï¼‰
        all_entities = entities_from_rules + entities_from_llm
        
        # 4. å»é‡å’Œæœ€ç»ˆå¤„ç†
        # _deduplicate_entities ä¼šä¿ç•™å…ˆå‡ºç°çš„å®ä½“ï¼Œä»è€Œå®ç°"è§„åˆ™ä¼˜å…ˆ"
        final_entities = self._deduplicate_entities(all_entities)
        
        logging.info(f"ğŸ“Š å®ä½“æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(final_entities)} ä¸ªå”¯ä¸€å®ä½“ã€‚")
        return final_entities

    def extract_entities_with_llm(self, text: str) -> List[Entity]:
        """
        ä½¿ç”¨LLMä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰å®ƒèƒ½è¯†åˆ«çš„å®ä½“ã€‚
        é‡‡ç”¨æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰å’Œå°‘é‡ç¤ºä¾‹ï¼ˆFew-shotï¼‰çš„Promptç­–ç•¥ã€‚

        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            List[Entity]: LLMå‘ç°çš„å®ä½“åˆ—è¡¨
        """
        if not self.openai_client:
            logging.warning("OpenAI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡LLMå®ä½“æå–ã€‚")
            return []

        entity_types = list(self.entity_patterns.keys())

        prompt = f"""
ä½ æ˜¯ä¸€ä½ç²¾é€šã€Šæˆ˜é”¤40kã€‹èƒŒæ™¯å’Œè§„åˆ™çš„ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç»™å®šçš„æ–‡æœ¬ä¸­ï¼Œè¯†åˆ«å‡ºæ‰€æœ‰æ½œåœ¨çš„å®ä½“ã€‚
å®ä½“ç±»å‹åŒ…æ‹¬ï¼š{', '.join(entity_types)}ã€‚

ä¸ºäº†å¸®åŠ©ä½ æ›´å¥½åœ°å®Œæˆä»»åŠ¡ï¼Œæˆ‘å°†ç»™ä½ ä¸€äº›ç¤ºä¾‹ï¼Œè¯·ä½ å­¦ä¹ æˆ‘çš„æ€è€ƒæ–¹å¼ï¼š

**ç¤ºä¾‹1ï¼š**
æ–‡æœ¬ï¼š "åœ¨æˆ˜æ–—é˜¶æ®µï¼Œè£…å¤‡äº†åŠ¨åŠ›å‰‘çš„ä¸‘è§’å‰§å›¢å¯ä»¥ä½¿ç”¨æ­»äº¡ä¹‹èˆã€‚"
ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š
1. "æˆ˜æ–—é˜¶æ®µ" æ˜¯ä¸€ä¸ªæ˜ç¡®çš„æ¸¸æˆé˜¶æ®µï¼Œåº”è¯¥æ ‡è®°ä¸º 'phase'ã€‚
2. "åŠ¨åŠ›å‰‘" æ˜¯ä¸€ä¸ªæ­¦å™¨ï¼Œåº”è¯¥æ ‡è®°ä¸º 'weapon'ã€‚
3. "ä¸‘è§’å‰§å›¢" æ˜¯ä¸€ä¸ªå·²çŸ¥çš„å•ä½ï¼Œåº”è¯¥æ ‡è®°ä¸º 'unit'ã€‚
4. "æ­»äº¡ä¹‹èˆ" å¬èµ·æ¥åƒä¸€ä¸ªç‰¹æ®Šèƒ½åŠ›æˆ–æŠ€èƒ½ï¼Œåº”è¯¥æ ‡è®°ä¸º 'skill'ã€‚
è¾“å‡ºJSON:
[
  {{"text": "æˆ˜æ–—é˜¶æ®µ", "type": "phase", "reason": "è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†æ¸¸æˆé˜¶æ®µ"}},
  {{"text": "åŠ¨åŠ›å‰‘", "type": "weapon", "reason": "è¿™æ˜¯ä¸€ä¸ªæ­¦å™¨åç§°"}},
  {{"text": "ä¸‘è§’å‰§å›¢", "type": "unit", "reason": "è¿™æ˜¯ä¸€ä¸ªå•ä½åç§°"}},
  {{"text": "æ­»äº¡ä¹‹èˆ", "type": "skill", "reason": "è¿™æ˜¯ä¸€ä¸ªæŠ€èƒ½åç§°"}}
]

**ç¤ºä¾‹2ï¼š**
æ–‡æœ¬: "ç«é¾™æ­¦å£«æ˜¯é˜¿è‹ç„‰å°¼çš„ä¸€ä¸ªæ”¯æ´¾æ­¦å£«ã€‚"
ä½ çš„æ€è€ƒè¿‡ç¨‹:
1. "ç«ç„°é¾™æ­¦å£«" æ˜¯ä¸€ä¸ªå…·ä½“çš„æˆ˜å£«ç±»å‹ï¼Œå±äºä¸€ä¸ªå•ä½ï¼Œæ ‡è®°ä¸º 'unit'ã€‚
2. "é˜¿è‹ç„‰å°¼" æ˜¯è‰¾è¾¾çµæ—çš„ä¸€ä¸ªå­é˜µè¥ï¼Œæ ‡è®°ä¸º 'sub_faction'ã€‚
è¾“å‡ºJSON:
[
  {{"text": "ç«ç„°é¾™æ­¦å£«", "type": "unit", "reason": "è¿™æ˜¯ä¸€ä¸ªå…·ä½“çš„å•ä½åç§°"}},
  {{"text": "é˜¿è‹ç„‰å°¼", "type": "sub_faction", "reason": "è¿™æ˜¯ä¸€ä¸ªå·²çŸ¥çš„å­é˜µè¥"}}
]

---
ç°åœ¨ï¼Œè¯·ä½ æŒ‰ç…§ä¸Šé¢çš„æ€è€ƒæ–¹å¼ï¼Œåˆ†æä¸‹é¢çš„æ–‡æœ¬ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚å¦‚æœæ²¡æœ‰ä»»ä½•å¯è¯†åˆ«çš„å®ä½“ï¼Œåˆ™è¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨ `[]`ã€‚

è¿™æ˜¯éœ€è¦ä½ åˆ†æçš„æ–‡æœ¬ï¼š
---
{text}
---
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"}, # ä½¿ç”¨JSONæ¨¡å¼ç¡®ä¿è¾“å‡ºæ ¼å¼
            )
            
            response_content = response.choices[0].message.content
            # LLMå¯èƒ½è¿”å›ä¸€ä¸ªåŒ…å«æ ¹é”®çš„JSONå¯¹è±¡ï¼Œæˆ‘ä»¬éœ€è¦æå–å‡ºåˆ—è¡¨
            response_data = json.loads(response_content)
            
            # å‡è®¾LLMå¯èƒ½è¿”å›å¦‚ {"new_entities": [...]} çš„æ ¼å¼ï¼Œå°è¯•æå–
            if isinstance(response_data, dict):
                # å¯»æ‰¾å­—å…¸ä¸­çš„ç¬¬ä¸€ä¸ªå€¼ä¸ºåˆ—è¡¨çš„é”®
                for key, value in response_data.items():
                    if isinstance(value, list):
                        llm_results = value
                        break
                else:
                    llm_results = []
            elif isinstance(response_data, list):
                llm_results = response_data
            else:
                llm_results = []

            new_entities = []
            for item in llm_results:
                # éªŒè¯è¿”å›çš„æ•°æ®
                if "text" in item and "type" in item and item["type"] in entity_types:
                    new_entities.append(Entity(
                        text=item["text"],
                        type=item["type"],
                        confidence=0.85, # æ¥è‡ªLLMçš„å®ä½“ç»™ä¸€ä¸ªè¾ƒé«˜çš„é»˜è®¤ç½®ä¿¡åº¦
                        aliases=[], # LLMç›®å‰ä¸è´Ÿè´£æå–åˆ«å
                        properties={"source": "llm", "reason": item.get("reason", "")}
                    ))
            
            return new_entities

        except Exception as e:
            logging.error(f"è°ƒç”¨LLM APIæˆ–è§£æå…¶å“åº”æ—¶å‡ºé”™: {e}")
            return []

    def _deduplicate_entities(self, entities: List[Entity]) -> List[Entity]:
        """å¯¹å®ä½“åˆ—è¡¨è¿›è¡Œå»é‡"""
        unique_entities = {}
        for entity in entities:
            # ä½¿ç”¨å®ä½“æ–‡æœ¬ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
            if entity.text not in unique_entities:
                unique_entities[entity.text] = entity
        return list(unique_entities.values())

    def _extract_entities_by_rules(self, text: str) -> List[Entity]:
        """åŸºäºè§„åˆ™æå–å®ä½“"""
        import re
        entities = []
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                # å¯¹æ¨¡å¼è¿›è¡Œè½¬ä¹‰ï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼ç‰¹æ®Šå­—ç¬¦å¯¼è‡´çš„é”™è¯¯
                escaped_pattern = re.escape(pattern)
                matches = re.finditer(escaped_pattern, text, re.IGNORECASE)
                for match in matches:
                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence = min(len(match.group()) / len(text) * 2, 1.0)
                    
                    # æå–åˆ«å
                    aliases = self._extract_aliases(match.group(), entity_type)
                    
                    # æå–å±æ€§
                    properties = self._extract_properties(text, match.group(), entity_type)
                    
                    entities.append(Entity(
                        text=match.group(),
                        type=entity_type,
                        confidence=confidence,
                        aliases=aliases,
                        properties=properties
                    ))
        
        return entities
    
    def _extract_aliases(self, entity_text: str, entity_type: str) -> List[str]:
        """ä»åŠ è½½çš„é…ç½®ä¸­æå–å®ä½“åˆ«å"""
        # ç›´æ¥ä½¿ç”¨åŠ è½½çš„ alias_mapping
        return self.alias_mapping.get(entity_text, [])
    
    def _extract_properties(self, text: str, entity_text: str, entity_type: str) -> Dict[str, Any]:
        """æå–å®ä½“å±æ€§"""
        import re
        properties = {}
        
        # æ ¹æ®å®ä½“ç±»å‹æå–ä¸åŒå±æ€§
        if entity_type == 'weapon':
            # æå–æ­¦å™¨å±æ€§
            range_match = re.search(r'(\d+)å¯¸', text)
            if range_match:
                properties['range'] = int(range_match.group(1))
            
            attacks_match = re.search(r'æ”»å‡»åŠ›[ï¼š:]\s*(\d+)', text)
            if attacks_match:
                properties['attacks'] = int(attacks_match.group(1))
            
            strength_match = re.search(r'åŠ›é‡[ï¼š:]\s*(\d+)', text)
            if strength_match:
                properties['strength'] = int(strength_match.group(1))
        
        elif entity_type == 'unit':
            # æå–å•ä½å±æ€§
            points_match = re.search(r'(\d+)åˆ†', text)
            if points_match:
                properties['points'] = int(points_match.group(1))
            
            type_match = re.search(r'(æ­¥å…µ|éª‘å…µ|è½½å…·)', text)
            if type_match:
                properties['unit_type'] = type_match.group(1)
        
        elif entity_type == 'skill':
            # æå–æŠ€èƒ½å±æ€§
            phase_match = re.search(r'(è¿‘æˆ˜|å°„å‡»|ç§»åŠ¨|å†²é”‹)é˜¶æ®µ', text)
            if phase_match:
                properties['phase'] = phase_match.group(1) + 'é˜¶æ®µ'
        
        return properties
    
    def extract_relationships(self, text: str, entities: List[Entity], content_type: str = None) -> List[Relationship]:
        """
        æå–å®ä½“é—´å…³ç³»
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            entities: å®ä½“åˆ—è¡¨
            content_type: å†…å®¹ç±»å‹
            
        Returns:
            List[Relationship]: å…³ç³»åˆ—è¡¨
        """
        try:
            relationships = []
            
            # åŸºäºè§„åˆ™çš„å…³ç³»æå–
            for i, entity1 in enumerate(entities):
                for j, entity2 in enumerate(entities):
                    if i != j:
                        relation_type = self._determine_relationship_type(text, entity1, entity2)
                        if relation_type:
                            source, target = self._determine_relationship_direction(text, entity1, entity2, relation_type)
                            properties = self._extract_relationship_properties(text, entity1, entity2, relation_type)
                            
                            relationships.append(Relationship(
                                source_entity=source,
                                target_entity=target,
                                relation_type=relation_type,
                                properties=properties
                            ))
            
            logger.info(f"æå–åˆ° {len(relationships)} ä¸ªå…³ç³»")
            return relationships
            
        except Exception as e:
            logger.error(f"å…³ç³»æå–å¤±è´¥: {e}")
            return []
    
    def _determine_relationship_type(self, text: str, entity1: Entity, entity2: Entity) -> Optional[str]:
        """ç¡®å®šå…³ç³»ç±»å‹"""
        import re
        
        # æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„æ–‡æœ¬
        entity1_pos = text.find(entity1.text)
        entity2_pos = text.find(entity2.text)
        
        if entity1_pos == -1 or entity2_pos == -1:
            return None
        
        # è·å–ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„æ–‡æœ¬ç‰‡æ®µ
        start = min(entity1_pos, entity2_pos)
        end = max(entity1_pos + len(entity1.text), entity2_pos + len(entity2.text))
        context = text[start:end]
        
        # æ ¹æ®å®ä½“ç±»å‹å’Œä¸Šä¸‹æ–‡åˆ¤æ–­å…³ç³»
        if entity1.type == 'unit' and entity2.type == 'skill':
            if re.search(r'æ‹¥æœ‰|è·å¾—|ä½¿ç”¨|has|gain|use', context):
                return 'HAS_SKILL'
        
        elif entity1.type == 'skill' and entity2.type == 'phase':
            if re.search(r'åœ¨.*é˜¶æ®µ|in.*phase|å½“.*æ—¶|when', context):
                return 'ACTIVATES_IN'
        
        elif entity1.type == 'unit' and entity2.type == 'weapon':
            if re.search(r'è£…å¤‡|æºå¸¦|equipped|carry', context):
                return 'EQUIPPED_WITH'
        
        elif entity1.type == 'skill' and entity2.type in ['unit', 'weapon']:
            if re.search(r'æä¾›|ç»™äºˆ|é€ æˆ|provides|gives|deals', context):
                return 'PROVIDES_EFFECT'
        
        elif entity1.type == 'rule' and entity2.type == 'phase':
            if re.search(r'é€‚ç”¨äº|åœ¨.*ä¸­|applies in|during', context):
                return 'APPLIES_IN'
        
        return None
    
    def _determine_relationship_direction(self, text: str, entity1: Entity, entity2: Entity, relation_type: str) -> tuple:
        """ç¡®å®šå…³ç³»çš„æ–¹å‘"""
        # æ ¹æ®å…³ç³»ç±»å‹ç¡®å®šæºå®ä½“å’Œç›®æ ‡å®ä½“
        if relation_type == 'HAS_SKILL':
            return (entity1, entity2) if entity1.type == 'unit' else (entity2, entity1)
        elif relation_type == 'ACTIVATES_IN':
            return (entity1, entity2) if entity1.type == 'skill' else (entity2, entity1)
        elif relation_type == 'EQUIPPED_WITH':
            return (entity1, entity2) if entity1.type == 'unit' else (entity2, entity1)
        elif relation_type == 'PROVIDES_EFFECT':
            return (entity1, entity2) if entity1.type == 'skill' else (entity2, entity1)
        elif relation_type == 'APPLIES_IN':
            return (entity1, entity2) if entity1.type == 'rule' else (entity2, entity1)
        else:
            return (entity1, entity2)
    
    def _extract_relationship_properties(self, text: str, entity1: Entity, entity2: Entity, relation_type: str) -> Dict[str, Any]:
        """æå–å…³ç³»å±æ€§"""
        import re
        properties = {}
        
        # æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„æ–‡æœ¬
        entity1_pos = text.find(entity1.text)
        entity2_pos = text.find(entity2.text)
        
        if entity1_pos != -1 and entity2_pos != -1:
            start = min(entity1_pos, entity2_pos)
            end = max(entity1_pos + len(entity1.text), entity2_pos + len(entity2.text))
            context = text[start:end]
            
            # æ ¹æ®å…³ç³»ç±»å‹æå–å±æ€§
            if relation_type == 'HAS_SKILL':
                # æå–æŠ€èƒ½ä½¿ç”¨æ¡ä»¶
                condition_match = re.search(r'å½“.*æ—¶|if.*then|when', context)
                if condition_match:
                    properties['condition'] = condition_match.group()
            
            elif relation_type == 'EQUIPPED_WITH':
                # æå–è£…å¤‡æ•°é‡
                quantity_match = re.search(r'(\d+)ä¸ª|(\d+)æŠŠ|(\d+)ä»¶', context)
                if quantity_match:
                    properties['quantity'] = int(quantity_match.group(1) or quantity_match.group(2) or quantity_match.group(3))
        
        return properties
    
    def update_knowledge_graph(self, entities: List[Entity], relationships: List[Relationship], metadata: Dict[str, Any] = None):
        """
        æ›´æ–°çŸ¥è¯†å›¾è°±
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            relationships: å…³ç³»åˆ—è¡¨
            metadata: å…ƒæ•°æ®
        """
        if not self.driver:
            logging.error("Neo4jè¿æ¥ä¸å¯ç”¨")
            return
        
        try:
            with self.driver.session() as session:
                for entity in entities:
                    self._create_entity_node(session, entity, metadata)
                for relationship in relationships:
                    self._create_relationship(session, relationship, metadata)
                self._create_alias_relationships(session, entities)
        except Exception as e:
            logging.error(f"æ›´æ–°çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
    
    def _create_entity_node(self, session, entity: Entity, metadata: Dict[str, Any] = None):
        """åˆ›å»ºå®ä½“èŠ‚ç‚¹"""
        node_label = f"{self.node_prefix}{entity.type.capitalize()}"
        properties = {
            'name': entity.text, 'type': entity.type, 'confidence': entity.confidence,
            'aliases': entity.aliases, 'environment': self.environment, **entity.properties
        }
        if metadata:
            properties.update({k: metadata.get(k, '') for k in ['source_file', 'faction', 'content_type', 'created_at', 'document_name', 'faction_name']})
        
        cypher = f"""
        MERGE (e:{node_label} {{name: $name, environment: $environment}})
        SET e += $properties
        """
        session.run(cypher, name=entity.text, environment=self.environment, properties=properties)
    
    def _create_relationship(self, session, relationship: Relationship, metadata: Dict[str, Any] = None):
        """åˆ›å»ºå…³ç³»"""
        relationship_type = f"{self.relationship_prefix}{relationship.relation_type}"
        properties = {'environment': self.environment, **relationship.properties}
        if metadata:
            properties.update({k: metadata.get(k, '') for k in ['source_file', 'faction', 'created_at', 'document_name', 'faction_name']})

        source_node_label = f"{self.node_prefix}{relationship.source_entity.type.capitalize()}"
        target_node_label = f"{self.node_prefix}{relationship.target_entity.type.capitalize()}"

        cypher = f"""
        MATCH (a:{source_node_label} {{name: $source_name, environment: $environment}})
        MATCH (b:{target_node_label} {{name: $target_name, environment: $environment}})
        MERGE (a)-[r:{relationship_type}]->(b)
        SET r += $properties
        """
        session.run(cypher, 
                   source_name=relationship.source_entity.text,
                   target_name=relationship.target_entity.text,
                   environment=self.environment,
                   properties=properties)
    
    def _create_alias_relationships(self, session, entities: List[Entity]):
        """åˆ›å»ºåˆ«åå…³ç³»"""
        alias_relationship_type = f"{self.relationship_prefix}ALIAS_OF"
        for entity in entities:
            if entity.aliases:
                for alias in entity.aliases:
                    cypher = f"""
                    MATCH (a {{name: $name1, environment: $environment}})
                    MATCH (b {{name: $alias, environment: $environment}})
                    WHERE id(a) <> id(b)
                    MERGE (a)-[r:{alias_relationship_type}]->(b)
                    SET r.environment = $environment
                    """
                    session.run(cypher, name1=entity.text, alias=alias, environment=self.environment)
    
    def query_entities(self, query: str) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[Dict[str, Any]]: æŸ¥è¯¢ç»“æœ
        """
        if not self.driver: return []
        try:
            with self.driver.session() as session:
                cypher = """
                MATCH (e)
                WHERE (e.name CONTAINS $query OR $query IN e.aliases) AND e.environment = $environment
                RETURN e LIMIT 10
                """
                result = session.run(cypher, query=query, environment=self.environment)
                return [record['e'] for record in result]
        except Exception as e:
            logging.error(f"æŸ¥è¯¢å®ä½“å¤±è´¥: {e}")
            return []
    
    def expand_query_with_kg(self, query: str) -> List[str]:
        """
        åŸºäºçŸ¥è¯†å›¾è°±æ‰©å±•æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            List[str]: æ‰©å±•åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            # 1. æå–æŸ¥è¯¢ä¸­çš„å®ä½“
            entities = self.query_entities(query)
            
            # 2. åŸºäºå®ä½“å…³ç³»ç”Ÿæˆæ‰©å±•æŸ¥è¯¢
            expanded_queries = [query]  # ä¿ç•™åŸå§‹æŸ¥è¯¢
            
            for entity in entities:
                entity_name = entity['name']
                
                # è·å–å®ä½“çš„å…³ç³»
                relationships = self.query_relationships(entity_name)
                
                for rel in relationships:
                    target_name = rel['target']['name']
                    relationship_type = rel['relationship']['type']
                    
                    # ç§»é™¤ç¯å¢ƒå‰ç¼€ä»¥è·å–çœŸå®çš„å…³ç³»ç±»å‹
                    if self.relationship_prefix and relationship_type.startswith(self.relationship_prefix):
                        relationship_type = relationship_type[len(self.relationship_prefix):]
                    
                    # ç”Ÿæˆç›¸å…³æŸ¥è¯¢
                    if relationship_type == 'HAS_SKILL':
                        expanded_queries.append(f"{entity_name}çš„{target_name}æŠ€èƒ½")
                        expanded_queries.append(f"{target_name}æŠ€èƒ½å¦‚ä½•ä½¿ç”¨")
                    
                    elif relationship_type == 'EQUIPPED_WITH':
                        expanded_queries.append(f"{entity_name}è£…å¤‡çš„{target_name}")
                        expanded_queries.append(f"{target_name}æ­¦å™¨å±æ€§")
                    
                    elif relationship_type == 'ACTIVATES_IN':
                        expanded_queries.append(f"{entity_name}åœ¨{target_name}ä¸­çš„æ•ˆæœ")
                
                # æ·»åŠ åˆ«åæŸ¥è¯¢
                aliases = self.get_entity_aliases(entity_name)
                for alias in aliases:
                    if alias != entity_name:
                        expanded_queries.append(query.replace(entity_name, alias))
            
            # å»é‡
            unique_queries = list(set(expanded_queries))
            
            logger.info(f"æŸ¥è¯¢æ‰©å±•ï¼š{query} -> {len(unique_queries)} ä¸ªæ‰©å±•æŸ¥è¯¢")
            return unique_queries
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [query]
    
    def query_relationships(self, entity_name: str, relation_type: str = None) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢å®ä½“çš„å…³ç³»
        
        Args:
            entity_name: å®ä½“åç§°
            relation_type: å…³ç³»ç±»å‹ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            List[Dict[str, Any]]: å…³ç³»æŸ¥è¯¢ç»“æœ
        """
        if not self.driver:
            logger.error("Neo4jè¿æ¥ä¸å¯ç”¨")
            return []
        
        try:
            with self.driver.session() as session:
                # æ„å»ºå…³ç³»ç±»å‹æŸ¥è¯¢æ¡ä»¶
                if relation_type:
                    # æ·»åŠ ç¯å¢ƒå‰ç¼€
                    prefixed_relation_type = f"{self.relationship_prefix}{relation_type}"
                    relationship_condition = f"r:{prefixed_relation_type}"
                else:
                    # æŸ¥è¯¢æ‰€æœ‰å…³ç³»ï¼Œä½†åªé™äºå½“å‰ç¯å¢ƒ
                    relationship_condition = "r"
                
                cypher = f"""
                MATCH (a)-[{relationship_condition}]->(b)
                WHERE (a.name = $entity_name OR $entity_name IN a.aliases)
                AND a.environment = $environment AND b.environment = $environment
                RETURN a, r, b
                """
                
                result = session.run(cypher, entity_name=entity_name, environment=self.environment)
                relationships = []
                
                for record in result:
                    relationships.append({
                        'source': record['a'],
                        'relationship': record['r'],
                        'target': record['b']
                    })
                
                return relationships
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å…³ç³»å¤±è´¥: {e}")
            return []
    
    def get_entity_aliases(self, entity_name: str) -> List[str]:
        """
        è·å–å®ä½“çš„æ‰€æœ‰åˆ«å
        
        Args:
            entity_name: å®ä½“åç§°
            
        Returns:
            List[str]: åˆ«ååˆ—è¡¨
        """
        if not self.driver:
            logger.error("Neo4jè¿æ¥ä¸å¯ç”¨")
            return []
        
        try:
            with self.driver.session() as session:
                cypher = """
                MATCH (e)
                WHERE (e.name = $entity_name OR $entity_name IN e.aliases)
                AND e.environment = $environment
                RETURN e.aliases
                """
                
                result = session.run(cypher, entity_name=entity_name, environment=self.environment)
                record = result.single()
                
                if record:
                    return record['e.aliases'] or []
                return []
                
        except Exception as e:
            logger.error(f"è·å–å®ä½“åˆ«åå¤±è´¥: {e}")
            return []
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.driver:
            self.driver.close()
            logger.info("Neo4jè¿æ¥å·²å…³é—­")

    def analyze_document(self, file_path: str, limit: Optional[int] = None):
        """
        åˆ†æå•ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼Œé€šè¿‡åˆ†å—æå–å¹¶æ‰“å°æ‰€æœ‰è¯†åˆ«å‡ºçš„å®ä½“ã€‚
        è¿™æ˜¯ä¸€ä¸ªç”¨äºè°ƒè¯•å’Œä¼˜åŒ–çš„è¾…åŠ©æ–¹æ³•ã€‚

        Args:
            file_path: è¦åˆ†æçš„æ–‡æ¡£çš„è·¯å¾„ã€‚
            limit: (å¯é€‰) é™åˆ¶åˆ†æçš„å—æ•°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚
        """
        logging.info(f"ğŸš€ å¼€å§‹åˆ†ææ–‡æ¡£: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except FileNotFoundError:
            logging.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            return
        except Exception as e:
            logging.error(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return

        # 1. ä½¿ç”¨ SemanticDocumentChunker è¿›è¡Œåˆ†å—
        document_name = os.path.basename(file_path).split('.')[0]
        chunker = SemanticDocumentChunker(document_name=document_name)
        chunks = chunker.chunk_text(content)
        
        if limit:
            chunks = chunks[:limit]
            logging.info(f"ğŸ“„ åˆ†æè¢«é™åˆ¶ä¸ºå‰ {limit} ä¸ªå—ã€‚")
        else:
            logging.info(f"ğŸ“„ æ–‡æ¡£è¢«æ‹†åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œåˆ†æã€‚")

        # 2. éå†æ¯ä¸ªå—ï¼Œæå–å®ä½“
        all_entities = []
        for i, chunk in enumerate(chunks):
            chunk_text = chunk.get("text", "")
            if not chunk_text:
                continue
            
            logging.info(f"--- æ­£åœ¨åˆ†æå— {i+1}/{len(chunks)} ---")
            entities_in_chunk = self.extract_entities(chunk_text)
            all_entities.extend(entities_in_chunk)
            # å¯é€‰ï¼šå¦‚æœæƒ³çœ‹æ¯ä¸ªå—çš„ç»“æœï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
            # if entities_in_chunk:
            #     print(f"  åœ¨å— {i+1} ä¸­æ‰¾åˆ°: {[e.text for e in entities_in_chunk]}")

        # 3. å¯¹æ‰€æœ‰æ‰¾åˆ°çš„å®ä½“è¿›è¡Œå»é‡å’Œæ•´ç†
        final_entities = self._deduplicate_entities(all_entities)

        if not final_entities:
            logging.warning("åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“ã€‚")
            return

        print("\n" + "="*50)
        print(f"ğŸ“„ åœ¨ã€Š{os.path.basename(file_path)}ã€‹ä¸­æ‰¾åˆ°çš„å®ä½“åˆ†æç»“æœ (å…± {len(final_entities)} ä¸ªå”¯ä¸€å®ä½“)")
        print("="*50)
        
        entities_by_type = {}
        for entity in final_entities:
            if entity.type not in entities_by_type:
                entities_by_type[entity.type] = []
            entities_by_type[entity.type].append(entity)
            
        for entity_type, entity_list in sorted(entities_by_type.items()):
            print(f"\n--- {entity_type.capitalize()} ({len(entity_list)}ä¸ª) ---")
            entity_texts = [f"{e.text} ({e.properties.get('source', 'rule')})" for e in entity_list]
            print(", ".join(sorted(list(set(entity_texts))))) # å†æ¬¡å»é‡ä»¥é˜²ä¸‡ä¸€

        print("\n" + "="*50)
        logging.info(f"âœ… åˆ†æå®Œæˆã€‚")


# å½“è¯¥è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œæ—¶
if __name__ == "__main__":
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="ä»æ–‡æ¡£ä¸­æå–å®ä½“å¹¶è¿›è¡Œåˆ†æã€‚")
    parser.add_argument("file_path", help="è¦åˆ†æçš„æ–‡æ¡£çš„è·¯å¾„ã€‚")
    parser.add_argument("--limit", type=int, default=None, help="ï¼ˆå¯é€‰ï¼‰é™åˆ¶åˆ†æå‰Nä¸ªæ–‡æœ¬å—ï¼Œç”¨äºå¿«é€Ÿè°ƒè¯•ã€‚")
    
    args = parser.parse_args()

    # åˆå§‹åŒ–ç®¡ç†å™¨
    kg_manager = KnowledgeGraphManager()

    # ä½¿ç”¨ limit å‚æ•°è°ƒç”¨åˆ†ææ–¹æ³•
    kg_manager.analyze_document(args.file_path, limit=args.limit)

    # å…³é—­è¿æ¥
    kg_manager.close() 